{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Course info # Fall 2020 // TR 4:30-6pm // Virtual Course. All course meetings will be conducted in Zoom. The course website is a living document. It will change regularly to reflect the needs of the course. Contact # Mackenzie Brooks // Associate Professor & Digital Humanities Librarian // (she/her/hers) brooksm@wlu.edu // 540-458-8659 // Leyburn 321 Office hours: TR 1:30-4:30pm Course description # This course introduces students to the creation, use, and visualization of data in humanities-based research. The course is predicated on the fact that the digital turn of the last several decades has drastically changed the nature of knowledge production and distribution. In the 21st century, we must develop fluencies in media beyond the printed word such as text mining, network analysis, data visualization, and spatial analysis. Readings and discussions of theory from the field and set of practices that is Digital Humanities (DH) will complement hands-on application of digital methods and computational thinking. While the objects of our study will primarily come from the humanities, the methods of analysis are widely applicable to the social and natural sciences. Course objectives # Recognize and assess data-driven digital projects. Integrate research goals and digital methods with discipline-specific inquiry. Develop skills necessary to create, structure, clean, manipulate, and visualize data. Engage in collaborative, interdisciplinary, project-based learning. Create professional website to document and present unit assignments. DCI Learning Outcomes # Develop critical capacities for analyzing the role of technology and digital media in contemporary culture, and apply those capacities to a range of disciplinary and interdisciplinary inquiries. Develop the technical skills necessary for academic and professional success, with an emphasis on online communication and information. Demonstrate the ability to communicate across different media and to both academic and general audiences. Engage in collaborative, interdisciplinary, project-based learning. Develop digital projects that contribute to the scholarly conversation in the student's field of study and demonstrate an awareness of the technological and critical needs of the discipline. Develop an online professional identity and a portfolio of work in the minor. Course protocols # If you have ever labeled yourself as \u201cbad at technology,\u201d you are welcome in this course. There is no assumed technical skill. \"Technology\" is a lot of things. Chances are you will know something about it that your classmates do not. Be generous about teaching and learning from each other. See also: https://xkcd.com/1053/ Your laptop will be a major player in this course, even more than usual. Please make sure it is good working order by updating software (operating system, browers, etc) and visiting the IT help desk as necessary. That being said, using your laptop in class is not an invitation to distraction. Look up how to disable notifications since you will have to share your screen in class. This course will mix theory and hands-on work. Be prepared to think critically about technology AND to get your hands dirty in a little bit of code. The DH community values experimentation and productive failure, often in public. We will learn from how and why things don\u2019t work. We are not just going to talk about what we\u2019re learning but how we\u2019re learning it. Give yourself a break from perfectionism! Pandemic protocols # This semester will be a lot. Go easy on yourself, your classmates, and your instructor. We are all trying to make the best of a situation that no one anticipated or asked for. I thought carefully about how to conduct this class. The digital work we will do in this class lends itself to virtual instruction perhaps more than other courses. We will be sharing screens, using breakout rooms, and working collaboratively online. These are skills that you can take with you later in life. But please let me know what is working or not working for you! That being said, you will feel overwhelmed by technology and grow weary of screens. Give yourself a break! Go outside, run up and down the stairs a few times, look at some art, the mountains, pictures of your pet. Walk around the block without your phone. Hydrate. The virtual environment does make communication harder. You might not feel comfortable speaking up or be able to stay after class. I might not be able to \"read the room\" as effectively as usual. Please, please reach out whenever you have questions, be it via Zoom, chat, email, or a call. I am happy to send you a Zoom link to chat. I will be on campus sometimes and can meet you with in person. Required texts # All readings are available freely online or through Leyburn Library\u2019s subscriptions. We will rely heavily on the following texts: The Historian's Macroscope Programming Historian You will be provided with a domain from Reclaim Hosting in the format http://username.wludci.info . You can purchase a custom domain, such as http://www.myname.com for $15 a year. If you are interested in keeping your domain after graduation, the W&L library can transfer ownership to you. Policies # Accommodations # Washington and Lee University makes reasonable academic accommodations for qualified students with disabilities. All undergraduate accommodations must be approved through the Title IX Coordinator and Director of Disability Resources, Elrod Commons 212 (540) 458-4055. Students requesting accommodations for this course should present an official accommodation letter within the first two weeks of the term and schedule a meeting outside of class time to discuss accommodations. It is the student\u2019s responsibility to present this paperwork in a timely fashion and to follow up about accommodation arrangements. Accommodations for test-taking must be arranged with the professor at least a week before the date of the test or exam, including finals. Attendance # You may notice that I do not factor in attendance and participation into your grade. This is on purpose. I try to design my classes so that you can't not participate. Class sessions are structured to help you advance your learning, not just for you to receive content. If you miss a lot of class, it will affect your grade because you aren't learning what you need to learn to fulfill assignments, not because I've docked a point. But life happens and sometimes you need a day or two. Or a pandemic happens and we all need a break. I expect that you'll talk to me about these instances and take responsibility for catching up on what you've missed. I will not plan to record our classes unless you let me know in advance that you'll be missing class and would like a recording. Plagiarism # All writing should be your own or should be cited properly. The writing assignments in this course are different than what is required in other courses, so we will discuss proper citation procedures for writing for the Web, writing in a group, and writing technical documentation. For more info: http://libguides.wlu.edu/plagiarism License # This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License . Acknowledgments # Thanks to Paula Kiser, Digital Scholarship Librarian at W&L, for her feedback and contributions to this coursebook. This coursebook was inspired by the Text Analysis Coursebook created by Brandon Walsh and Sarah Horowitz . Both should be acknowledged beyond the coursebook for their generous and ongoing conversations about pedagogy and Digital Humanities. Manatee icon credit: Freepik from www.flaticon.com .","title":"DCI 102 Data in the Humanities"},{"location":"#course-info","text":"Fall 2020 // TR 4:30-6pm // Virtual Course. All course meetings will be conducted in Zoom. The course website is a living document. It will change regularly to reflect the needs of the course.","title":"Course info"},{"location":"#contact","text":"Mackenzie Brooks // Associate Professor & Digital Humanities Librarian // (she/her/hers) brooksm@wlu.edu // 540-458-8659 // Leyburn 321 Office hours: TR 1:30-4:30pm","title":"Contact"},{"location":"#course-description","text":"This course introduces students to the creation, use, and visualization of data in humanities-based research. The course is predicated on the fact that the digital turn of the last several decades has drastically changed the nature of knowledge production and distribution. In the 21st century, we must develop fluencies in media beyond the printed word such as text mining, network analysis, data visualization, and spatial analysis. Readings and discussions of theory from the field and set of practices that is Digital Humanities (DH) will complement hands-on application of digital methods and computational thinking. While the objects of our study will primarily come from the humanities, the methods of analysis are widely applicable to the social and natural sciences.","title":"Course description"},{"location":"#course-objectives","text":"Recognize and assess data-driven digital projects. Integrate research goals and digital methods with discipline-specific inquiry. Develop skills necessary to create, structure, clean, manipulate, and visualize data. Engage in collaborative, interdisciplinary, project-based learning. Create professional website to document and present unit assignments.","title":"Course objectives"},{"location":"#dci-learning-outcomes","text":"Develop critical capacities for analyzing the role of technology and digital media in contemporary culture, and apply those capacities to a range of disciplinary and interdisciplinary inquiries. Develop the technical skills necessary for academic and professional success, with an emphasis on online communication and information. Demonstrate the ability to communicate across different media and to both academic and general audiences. Engage in collaborative, interdisciplinary, project-based learning. Develop digital projects that contribute to the scholarly conversation in the student's field of study and demonstrate an awareness of the technological and critical needs of the discipline. Develop an online professional identity and a portfolio of work in the minor.","title":"DCI Learning Outcomes"},{"location":"#course-protocols","text":"If you have ever labeled yourself as \u201cbad at technology,\u201d you are welcome in this course. There is no assumed technical skill. \"Technology\" is a lot of things. Chances are you will know something about it that your classmates do not. Be generous about teaching and learning from each other. See also: https://xkcd.com/1053/ Your laptop will be a major player in this course, even more than usual. Please make sure it is good working order by updating software (operating system, browers, etc) and visiting the IT help desk as necessary. That being said, using your laptop in class is not an invitation to distraction. Look up how to disable notifications since you will have to share your screen in class. This course will mix theory and hands-on work. Be prepared to think critically about technology AND to get your hands dirty in a little bit of code. The DH community values experimentation and productive failure, often in public. We will learn from how and why things don\u2019t work. We are not just going to talk about what we\u2019re learning but how we\u2019re learning it. Give yourself a break from perfectionism!","title":"Course protocols"},{"location":"#pandemic-protocols","text":"This semester will be a lot. Go easy on yourself, your classmates, and your instructor. We are all trying to make the best of a situation that no one anticipated or asked for. I thought carefully about how to conduct this class. The digital work we will do in this class lends itself to virtual instruction perhaps more than other courses. We will be sharing screens, using breakout rooms, and working collaboratively online. These are skills that you can take with you later in life. But please let me know what is working or not working for you! That being said, you will feel overwhelmed by technology and grow weary of screens. Give yourself a break! Go outside, run up and down the stairs a few times, look at some art, the mountains, pictures of your pet. Walk around the block without your phone. Hydrate. The virtual environment does make communication harder. You might not feel comfortable speaking up or be able to stay after class. I might not be able to \"read the room\" as effectively as usual. Please, please reach out whenever you have questions, be it via Zoom, chat, email, or a call. I am happy to send you a Zoom link to chat. I will be on campus sometimes and can meet you with in person.","title":"Pandemic protocols"},{"location":"#required-texts","text":"All readings are available freely online or through Leyburn Library\u2019s subscriptions. We will rely heavily on the following texts: The Historian's Macroscope Programming Historian You will be provided with a domain from Reclaim Hosting in the format http://username.wludci.info . You can purchase a custom domain, such as http://www.myname.com for $15 a year. If you are interested in keeping your domain after graduation, the W&L library can transfer ownership to you.","title":"Required texts"},{"location":"#policies","text":"","title":"Policies"},{"location":"#accommodations","text":"Washington and Lee University makes reasonable academic accommodations for qualified students with disabilities. All undergraduate accommodations must be approved through the Title IX Coordinator and Director of Disability Resources, Elrod Commons 212 (540) 458-4055. Students requesting accommodations for this course should present an official accommodation letter within the first two weeks of the term and schedule a meeting outside of class time to discuss accommodations. It is the student\u2019s responsibility to present this paperwork in a timely fashion and to follow up about accommodation arrangements. Accommodations for test-taking must be arranged with the professor at least a week before the date of the test or exam, including finals.","title":"Accommodations"},{"location":"#attendance","text":"You may notice that I do not factor in attendance and participation into your grade. This is on purpose. I try to design my classes so that you can't not participate. Class sessions are structured to help you advance your learning, not just for you to receive content. If you miss a lot of class, it will affect your grade because you aren't learning what you need to learn to fulfill assignments, not because I've docked a point. But life happens and sometimes you need a day or two. Or a pandemic happens and we all need a break. I expect that you'll talk to me about these instances and take responsibility for catching up on what you've missed. I will not plan to record our classes unless you let me know in advance that you'll be missing class and would like a recording.","title":"Attendance"},{"location":"#plagiarism","text":"All writing should be your own or should be cited properly. The writing assignments in this course are different than what is required in other courses, so we will discuss proper citation procedures for writing for the Web, writing in a group, and writing technical documentation. For more info: http://libguides.wlu.edu/plagiarism","title":"Plagiarism"},{"location":"#license","text":"This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License .","title":"License"},{"location":"#acknowledgments","text":"Thanks to Paula Kiser, Digital Scholarship Librarian at W&L, for her feedback and contributions to this coursebook. This coursebook was inspired by the Text Analysis Coursebook created by Brandon Walsh and Sarah Horowitz . Both should be acknowledged beyond the coursebook for their generous and ongoing conversations about pedagogy and Digital Humanities. Manatee icon credit: Freepik from www.flaticon.com .","title":"Acknowledgments"},{"location":"assignments/","text":"Specifications Grading # The assignment and grading structure for this course might be a little different than what you're used to. In this course, we'll use something called Specifications Grading . The goals of the system are to reduce the stress and mystery of grades while also raising academic standards. I chose this system to complement the technologies and digital methods we will be learning this term, as well as to combat grade anxiety. It is more important to me that you explore and experiment with these methods than it is that you get the \"right answer.\" It's hard to feel comfortable experimenting and making mistakes if you're worried about every point. So rather than assign points or grades, I will mark each assignment as Complete/Incomplete according to a set of specifications. You must complete a certain number of assignments in each category to receive an A, B, C, etc. as listed below. You will receive three tokens to use in the case that you cannot turn in work on time or to complete an incomplete assignment. You will use Canvas to turn in assignments and to receive feedback. I will mark your assignments as complete/incomplete, however! Canvas will not be able to calculate your current grade with this system. You should rely on this page or this handout to help calculate your grade. This is a lot to get used to at first, so please ask any questions you have early in the semester. We will do a midterm check-in so you have a good understanding of where you are. Grading Scale # To earn a... Complete the following A 7 weekly activity logs 10-12 blog posts 5 project pieces B 6 weekly activity logs 8 or 9 blog posts 4 project pieces C 5 weekly activity logs 6 or 7 blog posts 3 project pieces D less than 5 weekly activity logs less than 5 blog posts less than 3 project pieces Tokens # You're not expected to be perfect at every assignment! To help you recover from any incomplete assignments, you will be assigned three tokens . Using a token will give you one week to revise and resubmit an assignment to receive credit. To use a token, you must email me with your intention to use a token, the assignment you wish resubmit, and the expected time frame. Using a token does not guarantee that you will receive a complete on the resubmission, but hopefully with my feedback you can get there. Weekly Activity Logs # Each week you'll complete several activities designed to increase your familiarity with your computer, digital methods, and digital tools. Many of these activities will be started or completed together in our class sessions, but some you'll be expected to do on your own. Activities will be about learning a process or method, rather than delivering the right answer. You will share your results on your website and instructions for what to share will be posted on the schedule with the due date. You'll turn in this work on Canvas and receive feedback there. Blog posts # Each week you'll write a 300-500 word blog post on your course website. Prompts will be provided with each week's activity log, posted on the schedule. These posts will ask you to do one or more of the following: 1) reflect on what you've learned this week 2) make connections between new skills and readings or 3) get you thinking and prepared for the project. Humanities Data Project # In the second half of the term, you will design and conduct an independent data-driven project. This project asks you adopt one or more of the methods we've learned so far and apply it to a humanities-based topic of your choosing. You will select a text or body of material, identify potential research questions, create a data model and data set, then visualize and analyze your data in a way that attempts to address your research questions. You can read more about how this will work in the Process section. The project will consist of five pieces: Project proposal Data documentation Data visualization Results Reflection Project Proposal # First things first, you should have an idea of what you want to do and how you're going to do it. During the first half the term, we'll set time aside for brainstorming and researching your project. In the proposal, you will identify the topic of your project, some potential research questions, show evidence of research, indicate the methods you intend to pursue, and designate your own standards for success. We'll meet one-on-one during Week 8 to discuss your plans. Specifications: Due Monday, October 19th, at 9am. 750-1000 words. Posted to your website in a designated project section. Turn in the link to this page on Canvas. Address the following: What is the topic of your project? What led you to this topic? What is the source of your data? Tell me about it. What are some potential research questions (at least two)? What discipline are you situating yourself in? What is your proposed method of analysis? What research have you done on your topic? Cite at least 2-3 scholarly sources and any less-than-scholarly sources that are relevant. What will success look like for you? Is there a skill you want to develop? Do you want to advance your knowledge on a particular method or topic? Include a proposed schedule and list of tasks to complete your project. You can adjust this as you go, but it's good to give it some thought now. Data Documentation # Once you've proposed your project, it's time to get working with your data! This part of the project is going to look different for everyone, so the important part is that you document what you're doing and how you're doing it. Specifications: Due Monday, October 26th, at 9am. No word count requirement, but it should be at least 2-3 pages, if not more. The more detail the better! Posted to your website in a designated project section. Turn in the link to this page on Canvas. Contains 4 pieces: Part 1: Data Model - In this document, you will identify the structure of your data, list all the fields you will be using in your dataset, the format and type of those fields, and the source of the data for those fields. If your project involves a text corpus, you should list the scope and rationale of your corpus, as well as the source of the data. A network analysis project should include an edge list and an attribute table, with details about each attribute. Part 2: Data Plan - Describe your process for creating this data set according to your model. Will you be transcribing from a book? Copying from multiple sources? Is there data to be cleaned or transformed? OCR to be done? List the steps in detail. Identify any potential challenges to your plan. Part 3: Data License - Under what license will you release this data? How are you crediting your sources? How should others cite your work? Consult Open Data Commons or Creative Commons for licenses. Feel free to go back through the projects we've looked at already to see how they license their data. Part 4: The actual data itself. This should be uploaded to your Wordpress site and linked from your project page, near the data license. It is okay if your data is not complete yet, but you should have an initial draft available. Data Visualization # Your humanities data project should include at least one (really awesome) visualization or more than one (really good) visualizations. The visualizations should reflect your chosen method of analysis - a network graph, map, etc. Your visualization should adhere to the design principles we covered in week 4. Specifications: Final visualizations due Monday, November 9th, at 9am. Draft visualizations due Monday, November 2, at 9am. Posted to your website in a designated project section. Turn in the link to this page on Canvas. If your project involves an interactive visualization, it is okay to only one visualization. If you're working with static visualizations, like a graph, chart, or even some maps, create 2-5 visualizations. Visualizations should be clearly labled, with an accompanying short caption. The caption should explain the significance of this visualization and its connection to your research questions. Visualization should follow the design principles we've covered in class. Results # Time to share your results! In this section, you'll describe the findings of your data analysis and visualization. You should attempt to answer your research question. If you find you cannot answers those questions, talk about why and speculate as to what new research questions you might pursue instead. Your results should be grounded in the discipline that you identified in the proposal. You should use your data visualizations to illustrate your points. Think of this as a data-driven digital essay. Do not be afraid to address the limitations, set-backs, or future directions for this project. Specifications: Due Monday, November 9th, at 9am. Posted to your website in a designated project section. Turn in the link to this page on Canvas. 750-1000 words. Address the following: What have you learned about your topic or how have you addressed your research questions by putting together your data set? What have you learned through the process of visualizing your data? What disciplinary angle did you take? How do your results advance a conversation in that discipline? Use a source or two to demonstrate this. What are future directions of this project? What work needs to be done (by you or someone else)? What would the project look like if with more time/data/analysis/visualizations? What were the problems or challenges with your method? Reflection # No project is complete without time taken to reflect on its successes and lessons learned. Talk about your feelings! What have you learned about humanities data? What have you learned about yourself and the way you learn new things? Specifications: Due Friday, November 13th, at 11:59pm. Submit as a document in Canvas. 500-1000 words. Address the following: What have you learned? What has been challenging? Think back to the start of class. How has your relationship with technology changed? What about your conception of data? If you could do it all again, what would you do differently? How can you apply the skills of this course in future courses or non-academic endeavors?","title":"Assignments"},{"location":"assignments/#specifications-grading","text":"The assignment and grading structure for this course might be a little different than what you're used to. In this course, we'll use something called Specifications Grading . The goals of the system are to reduce the stress and mystery of grades while also raising academic standards. I chose this system to complement the technologies and digital methods we will be learning this term, as well as to combat grade anxiety. It is more important to me that you explore and experiment with these methods than it is that you get the \"right answer.\" It's hard to feel comfortable experimenting and making mistakes if you're worried about every point. So rather than assign points or grades, I will mark each assignment as Complete/Incomplete according to a set of specifications. You must complete a certain number of assignments in each category to receive an A, B, C, etc. as listed below. You will receive three tokens to use in the case that you cannot turn in work on time or to complete an incomplete assignment. You will use Canvas to turn in assignments and to receive feedback. I will mark your assignments as complete/incomplete, however! Canvas will not be able to calculate your current grade with this system. You should rely on this page or this handout to help calculate your grade. This is a lot to get used to at first, so please ask any questions you have early in the semester. We will do a midterm check-in so you have a good understanding of where you are.","title":"Specifications Grading"},{"location":"assignments/#grading-scale","text":"To earn a... Complete the following A 7 weekly activity logs 10-12 blog posts 5 project pieces B 6 weekly activity logs 8 or 9 blog posts 4 project pieces C 5 weekly activity logs 6 or 7 blog posts 3 project pieces D less than 5 weekly activity logs less than 5 blog posts less than 3 project pieces","title":"Grading Scale"},{"location":"assignments/#tokens","text":"You're not expected to be perfect at every assignment! To help you recover from any incomplete assignments, you will be assigned three tokens . Using a token will give you one week to revise and resubmit an assignment to receive credit. To use a token, you must email me with your intention to use a token, the assignment you wish resubmit, and the expected time frame. Using a token does not guarantee that you will receive a complete on the resubmission, but hopefully with my feedback you can get there.","title":"Tokens"},{"location":"assignments/#weekly-activity-logs","text":"Each week you'll complete several activities designed to increase your familiarity with your computer, digital methods, and digital tools. Many of these activities will be started or completed together in our class sessions, but some you'll be expected to do on your own. Activities will be about learning a process or method, rather than delivering the right answer. You will share your results on your website and instructions for what to share will be posted on the schedule with the due date. You'll turn in this work on Canvas and receive feedback there.","title":"Weekly Activity Logs"},{"location":"assignments/#blog-posts","text":"Each week you'll write a 300-500 word blog post on your course website. Prompts will be provided with each week's activity log, posted on the schedule. These posts will ask you to do one or more of the following: 1) reflect on what you've learned this week 2) make connections between new skills and readings or 3) get you thinking and prepared for the project.","title":"Blog posts"},{"location":"assignments/#humanities-data-project","text":"In the second half of the term, you will design and conduct an independent data-driven project. This project asks you adopt one or more of the methods we've learned so far and apply it to a humanities-based topic of your choosing. You will select a text or body of material, identify potential research questions, create a data model and data set, then visualize and analyze your data in a way that attempts to address your research questions. You can read more about how this will work in the Process section. The project will consist of five pieces: Project proposal Data documentation Data visualization Results Reflection","title":"Humanities Data Project"},{"location":"assignments/#project-proposal","text":"First things first, you should have an idea of what you want to do and how you're going to do it. During the first half the term, we'll set time aside for brainstorming and researching your project. In the proposal, you will identify the topic of your project, some potential research questions, show evidence of research, indicate the methods you intend to pursue, and designate your own standards for success. We'll meet one-on-one during Week 8 to discuss your plans. Specifications: Due Monday, October 19th, at 9am. 750-1000 words. Posted to your website in a designated project section. Turn in the link to this page on Canvas. Address the following: What is the topic of your project? What led you to this topic? What is the source of your data? Tell me about it. What are some potential research questions (at least two)? What discipline are you situating yourself in? What is your proposed method of analysis? What research have you done on your topic? Cite at least 2-3 scholarly sources and any less-than-scholarly sources that are relevant. What will success look like for you? Is there a skill you want to develop? Do you want to advance your knowledge on a particular method or topic? Include a proposed schedule and list of tasks to complete your project. You can adjust this as you go, but it's good to give it some thought now.","title":"Project Proposal"},{"location":"assignments/#data-documentation","text":"Once you've proposed your project, it's time to get working with your data! This part of the project is going to look different for everyone, so the important part is that you document what you're doing and how you're doing it. Specifications: Due Monday, October 26th, at 9am. No word count requirement, but it should be at least 2-3 pages, if not more. The more detail the better! Posted to your website in a designated project section. Turn in the link to this page on Canvas. Contains 4 pieces: Part 1: Data Model - In this document, you will identify the structure of your data, list all the fields you will be using in your dataset, the format and type of those fields, and the source of the data for those fields. If your project involves a text corpus, you should list the scope and rationale of your corpus, as well as the source of the data. A network analysis project should include an edge list and an attribute table, with details about each attribute. Part 2: Data Plan - Describe your process for creating this data set according to your model. Will you be transcribing from a book? Copying from multiple sources? Is there data to be cleaned or transformed? OCR to be done? List the steps in detail. Identify any potential challenges to your plan. Part 3: Data License - Under what license will you release this data? How are you crediting your sources? How should others cite your work? Consult Open Data Commons or Creative Commons for licenses. Feel free to go back through the projects we've looked at already to see how they license their data. Part 4: The actual data itself. This should be uploaded to your Wordpress site and linked from your project page, near the data license. It is okay if your data is not complete yet, but you should have an initial draft available.","title":"Data Documentation"},{"location":"assignments/#data-visualization","text":"Your humanities data project should include at least one (really awesome) visualization or more than one (really good) visualizations. The visualizations should reflect your chosen method of analysis - a network graph, map, etc. Your visualization should adhere to the design principles we covered in week 4. Specifications: Final visualizations due Monday, November 9th, at 9am. Draft visualizations due Monday, November 2, at 9am. Posted to your website in a designated project section. Turn in the link to this page on Canvas. If your project involves an interactive visualization, it is okay to only one visualization. If you're working with static visualizations, like a graph, chart, or even some maps, create 2-5 visualizations. Visualizations should be clearly labled, with an accompanying short caption. The caption should explain the significance of this visualization and its connection to your research questions. Visualization should follow the design principles we've covered in class.","title":"Data Visualization"},{"location":"assignments/#results","text":"Time to share your results! In this section, you'll describe the findings of your data analysis and visualization. You should attempt to answer your research question. If you find you cannot answers those questions, talk about why and speculate as to what new research questions you might pursue instead. Your results should be grounded in the discipline that you identified in the proposal. You should use your data visualizations to illustrate your points. Think of this as a data-driven digital essay. Do not be afraid to address the limitations, set-backs, or future directions for this project. Specifications: Due Monday, November 9th, at 9am. Posted to your website in a designated project section. Turn in the link to this page on Canvas. 750-1000 words. Address the following: What have you learned about your topic or how have you addressed your research questions by putting together your data set? What have you learned through the process of visualizing your data? What disciplinary angle did you take? How do your results advance a conversation in that discipline? Use a source or two to demonstrate this. What are future directions of this project? What work needs to be done (by you or someone else)? What would the project look like if with more time/data/analysis/visualizations? What were the problems or challenges with your method?","title":"Results"},{"location":"assignments/#reflection","text":"No project is complete without time taken to reflect on its successes and lessons learned. Talk about your feelings! What have you learned about humanities data? What have you learned about yourself and the way you learn new things? Specifications: Due Friday, November 13th, at 11:59pm. Submit as a document in Canvas. 500-1000 words. Address the following: What have you learned? What has been challenging? Think back to the start of class. How has your relationship with technology changed? What about your conception of data? If you could do it all again, what would you do differently? How can you apply the skills of this course in future courses or non-academic endeavors?","title":"Reflection"},{"location":"data/","text":"In this section, we'll dig deeper into the fundamentals of working with data by concentrating on data structures and methods for cleaning messy data. What is data again? Data structures Tabular data Relational data Data serialization Textual data Data types Clean, tidy, and normal data Data cleaning tools and methods Open Refine Excel Regular Expressions Data modeling Extending your data Linked data Activities Activity 3.1 Activity 3.2 Activity 3.3 Activity 3.4 Readings What is data again? # In the last section, we got as far as defining data as \"a value assigned to a thing.\" While that remains true, let's dig a little deeper. If we put on our humanities hat, we start to see data little more complexly. In her article, \" Humanities Approaches to Graphical Display ,\" Johanna Drucker posits the following about data: \"To overturn the assumptions that structure conventions acquired from other domains requires that we re-examine the intellectual foundations of digital humanities, putting techniques of graphical display on a foundation that is humanistic at its base. This requires first and foremost that we reconceive all data as capta. Differences in the etymological roots of the terms data and capta make the distinction between constructivist and realist approaches clear. Capta is \u201ctaken\u201d actively while data is assumed to be a \u201cgiven\u201d able to be recorded and observed. From this distinction, a world of differences arises. Humanistic inquiry acknowledges the situated, partial, and constitutive character of knowledge production, the recognition that knowledge is constructed, taken, not simply given as a natural representation of pre-existing fact.\" This is a long quote, but the concept of \"capta\" is a useful one. Even if we don't go around calling everything \"capta\" instead of \"data,\" it is a helpful reminder that data is constructed, especially but not only in the humanities. Every single piece of data represents a decision that someone made. The number of rows and columns, the labels, the format, the extent or granularity of the values and the things - these are all choices. They are choices you should consider when creating a data set from scratch or when you use someone else's data. In the rest of this section, we will dig into the details of putting together a data set. Data structures # Data can exist in many different forms. You might imagine a \"spreadsheet\" when thinking about data, but in truth, there are a number of other ways to express and structure your data. Tabular data # This is our common spreadsheet data structure. Data fits into a table, in which each row represents a thing. Each column is a different field or modifier of that thing. The values reside in each individual cell. In the following table, each row represents one pet. PetID Name Animal Breed Pet1 Huxley Cat Tabby Pet2 Harper Dog Flat Coat Retriever Pet3 Hobbes Dog Hound If you're new to making spreadsheets, it can be tempting to swap the rows and columns so that each column represents a thing. It can also be tempting to include empty cells as spaces or use visual cues like color or bold/italics to set apart text. These methods will certainly help you as a human, but it's important to keep in mind that most of the software that might process your data will not be able to understand these visual cues. Most of the software that will process your data, whether it's for visualization or analysis, will want to see your data in a format known as CSV or Comma Separated Value. This format strips away any visual formatting and stores each cell in plain text, separated by a comma. So the above table about pets, would look like this: PetID,Name,Animal,Breed, Pet1,Huxley,Cat,Tabby, Pet2,Harper,Dog,Flat Coat Retriever, Pet3,Hobbes,Dog,Hound The CSV format reduces file size, simplifies the content, and keeps the file from being dependent on proprietary software like Excel. Fortunately, you don't have to construct your data with commas. Applications like Excel or Google Sheets will let you save or export your file with the file extension .csv . It's a good habit to get into when working with data. You might also see .tsv for tab separated values, in which the spaces we see when we hit the tab button function as a single character. Relational data # For some projects, a single table quickly becomes insufficient. Perhaps your project requires collecting data about a group of people, the houses they live in, the land they occupy, etc. It might be time to look into using a database to store all that data. A database works on a relational model - you might have multiple tables, but they are connected through shared values or \"keys.\" Using a database allows you to gather data about many different things, things that may require different fields to describe them. So my table of people may contain a column to note which house they live in, but I can rely on a totally separate table to list all the details about the house itself. This keeps me from cluttering my people table with information that isn't relevant to the main thing my row is about. Often, this is done with an ID number rather than the full title. A second advantage of a database is the ability to ask questions of your data. To continue the example from above, I can now ask questions like: How many people live in houses that are yellow? Of the houses on this block, which ones are occupied by people with children? These questions require that I filter based on certain values within each distinct data table and combine them into a new set. Typically, questions like this are asked with a language called SQL or Structure Query Language (yes, another language with its own syntax!). The basic syntax of SQL looks like this: SELECT column_name,column_name FROM table_name WHERE column_name operator value; Here's a more specific example. In this one, the asterisk means \"give me everything.\" So I'm asking for all the fields that exist about the cats. SELECT * FROM pets WHERE animal='cat'; Asking these types of questions of data is often what humanities scholars are after when they decide to build a database. However, designing a database and the interface that goes along with it is a big endeavor, and often requires the help of a professional database designer. Databases can be built in many different types of software (you might encounter names like MySQL or PostgreSQL), but web-based databases will need to be installed on a server, not just your own computer. If you're undertaking a big project, it's important to assess your resources for creating a database. Fortunately, there are an increasing number of options for storing, visualizing, and analyzing data without database software. Creating well-structured data ensures that your data can be imported into and out of these tools with ease. Data serialization # Data serialization is the process by which data is transformed into another format that can be transferred or stored, then reconstituted. Often, this is necessary so that a programming language or software application can more easily and quickly do something with the data. With serialization comes a few different types of data structures that are important to know. JSON , or JavaScript Objection Notation, is a way to structure data based on attribute-value pairs and curly brackets. Just like CSV, if you're working with a JSON file, it will have the file extension .json . You probably won't end up writing JSON from scratch, but you might have to convert your data to JSON for a certain application. And FYI, JSON is not only used with JavaScript. Here's an example. Notice the level of structure introduced with the Address line. There are four fields and values (you might see this called an array) that modify one field, Address. { \"PetID\": \"Pet1\", \"Name\": \"Huxley\", \"Animal\": true, \"Breed\": \"Tabby\", \"Address\": { \"streetAddress\": \"555 5th St.\", \"city\": \"The Couch\", \"state\": \"Living Room\", \"postalCode\": \"55555\" }, } XML or Extensible Markup Language is more than just a serialized data format, it's also a markup language (like HTML). XML has been around a long time, so you will see it referenced in many Digital Humanities projects. XML is highly structured, but allows the user to define the tags that are used. A common application in humanities projects is the Text Encoding Initiative or TEI. TEI is used to markup humanities texts, just like you would markup a website, but instead you use tags like <l> for line and <epigraph> for epigraph. TEI allows you to do things like build a website for testing your ability to read rhyme and meter in poetry. It is a foundational practice in DH - the process by which a human interprets a text and encodes the meaning in a way that the computer can understand. Example: <lg rhyme=\"ABCCBBA\"> <l>The sunlight on the <rhyme label=\"A\">garden</rhyme> </l> <l> <rhyme label=\"A\">Harden</rhyme>s and grows <rhyme label=\"B\">cold</rhyme>,</l> <l>We cannot cage the <rhyme label=\"C\">minute</rhyme> </l> <l>Wi<rhyme label=\"C\">thin it</rhyme>s nets of <rhyme label=\"B\">gold</rhyme> </l> <l>When all is <rhyme label=\"B\">told</rhyme> </l> <l>We cannot beg for <rhyme label=\"A\">pardon</rhyme>.</l> </lg> Textual data # Not all data has needs to exist in a spreadsheet. In the humanities, we read a lot! We need a way to store textual data, such as a whole book, set of documents, or a century of newspaper articles. A later section will cover the ins and outs of text analysis, but if you're looking to analyze a corpus (another word for collection of texts) of some kind, you need to structure it in some way. The common method for storing textual data is in a plain text format or with the file extension .txt . Plain text files are just that - documents without the formatting that comes from MS Word or other word processing applications. Just like our HTML documents or our CSV files, a text document simplifies the content for the computer. When we're counting the number of times Jane Austen used the word \"honor\" or looking for the use of the word \"evolution\" over time , we are concerned with the words in the text, not the way the text looks. Plain text format reduces file size to improve processing time and transfer of data. You'll probably want to use a text editor for viewing these files, just like we did when writing HTML. Depending on what you're doing, you will also want to pay attention to the name and organization of your text files. If you noticed in the Voyant instance with Jane Austen, the title of each text was preceded by the year of publication. This file naming strategy makes it easy to sort the texts by their publication date. It's just something to pay attention to when constructing your corpus. If you do care about what the text looks like, you might want to look for ways to analyze TEI documents, not just plain text. Text Encoding allows you to add details about the appearance of the text, perhaps there's another hand writing in the margins or use of italics or white space in poetry. Data types # Once you have settled on the right structure for your data, you need to consider the data types you will use in each field. Most programming languages accept the following: Data Type Example Strings or characters, often surrounded by quote marks \"String!\" Integer 4 Decimal 4.10 Boolean, another way to say true/false or yes/no TRUE Date, usually formatted as YYYY-MM-DD 2019-10-20 Time, usually formatted as HH:MM:SS 10:17:26 There are a few others data types, but these will be most relevant in humanities projects. You will find that Excel loves to chew up and spit out dates unless you standardize formatting in this way. Clean, tidy, and normal data # As you start to work with data, you'll likely encounter these terms \"clean, tidy, normal,\" used as adjectives and verbs. What do they all mean? How do you know when your data is clean and tidy? Maybe your data is just the black sheep of the family! Cleaning data usually refers to the process of correcting the contents of your dataset. Typos, mistakes, new information can all be cleaned up after the data entry process is complete. You don't have to read every cell one by one, there are a number of tools and methods that will help automate this process. Often, visualization your data is a great way to find issues you didn't know you had. Tidy data comes from the community around R, the programming language. Tidy data is well-structured, or as Hadley Wickham puts it, \"Tidy datasets are easy to manipulate, model and visualise, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.\" Sounds familiar, right? Normalized data is a database term. It refers to the process of structuring your tables to reduce redundancy and complexity. Normalized data should be easier for both humans and computers to understand. Before we get to tools and methods for straightening up your data, let's take a moment for a pep talk. Data is messy! Whether you created it yourself or are using someone else's data, chances are, something will be wrong with it! The data entry process can be long, and it's easy to change your habits or come upon a better way to record things. This is normal. To set yourself up for success, document your decisions as you make them. It is easy to absorbed in your work, make a decision, then put your work down and forget about the decision a week later. Keeping a log of changes will also help anyone else who might be contributing. Be consistent! If you want to change a \"T\" to \"True,\" it's easy to find and replace. It's not as easy to go through a mix of values. Finally, remember what the computer is for. It can be tempting to embark on a cleanup project by hand, instead of looking for automated ways to correct your data. Knowing when to cleanup by hand or when to spend the time figuring out how to automate a task is a mark of experience. You won't get it right every time. But it is worthwhile to consider if something can be done in an automated way. Find and replace, when coupled with regular expressions, can be pretty powerful. It can feel like a waste to spend time automating a task when you could just do it, but the skills you learn in the process are valuable and can be applied on future tasks. Data cleaning tools and methods # Open Refine # I try to avoid calling technology magic, but Open Refine is magic. It takes some getting used to, but it is indeed a \"powerful tool for working with messy data.\" When you install and run Open Refine, it launches a local server on computer so you interact with the application in your browser. However, just because it's in your browser does not mean it's connected to the internet. Once you've loaded your data, you use the drop down arrows to access various tools for modifying your data. This is not a tool for scrolling through your data, it's used for targeted approaches to specific issues. As an example, you can use the \"cluster and edit\" feature to find similar cells and standardize their contents. The \"text facet\" feature shows how many cells share the same value. When you sort by alphabetical order, it is really easy to see if you have entered the same name in two different ways. There are a number of tutorials on the documentation page . Try this one, \" Cleaning Data with OpenRefine \" from the Programming Historian. Excel # Before you get to specialized tools like Open Refine, you can always use Excel or Google Sheets to check your work. Filter - use the filter feature to assess the contents of a single column. You can easily see if a cell is empty or has an incorrect entry. Data Validation - allows you to control what is entered into a cell. You can identify a list of options or select a specific data type. Formulas - there are plenty of spreadsheet formulas designed to work with strings of text. Use the help pages and a search engine to look for examples. There are multiple methods for splitting columns if you need to separate first and last names, or copy over the first half of a string. Regular Expressions # Regular expressions are a method for matching patterns in text. It's a set of special characters that when used together in certain contexts helps you find and replace in powerful ways. For example, you could find all the phone numbers in a text by looking for the pattern (###) ###-#### or all forms of \"women\" by using the pattern \"wom.n\" to replace the fourth character. Regular expressions are awesome, but they can get frustrating quickly. Don't be overwhelmed by the complex patterns you encounter in tutorials. Even the basics can come in handy when cleaning data. Plus, there are a lot of resources out there for helping you use them effectively. Regular Expressions 101 Beginning with Regular Expressions Cleaning OCR'd text with regular expressions Data modeling # Whether you are aware of it or not, data modeling is part of the data creation process. All of the decisions that you might make about your data, the format, the data types, the fields, help to form a model of your data. Data modeling is a process that has been part of humanities work for a long time, just not by that name. When we attempt to organize the world through means like time periods, genres, or lists of names (prosopographies) or places (gazeteers), we are creating models that may or may not line up with reality. A model is an abstraction of an object. It can be a window into our disciplinary or personal values. The tools you choose to use will also contribute to the shape of your data model, as they can limit what and how you record your data. There is a lot more to be said about data models and their role in humanities data work. There are different types of models and various ways to approach their creation and use. Data modeling has precise meanings within other disciplines, such as computer science. If you want to learn more, start with \" Data Modeling in a Digital Humanities Context \" by Julia Flanders and Fotis Jannidis. But for now, let's return to that Miriam Posner article, \" Humanities Data: A Necessary Contradiction .\" \"When you call something data, you imply that it exists in discrete, fungible units; that it is computationally tractable; that its meaningful qualities can be enumerated in a finite list; that someone else performing the same operations on the same data will come up with the same results. This is not how humanists think of the material they work with.\" Data modeling is important precisely because it forces humanities scholars to clearly identify their goals and parameters. If you start a project without an initial idea of your data model, you could lose years sifting through archival material or transcribing handwritten notes. Or as Scott Weingart writes , \"Humanistic data are almost by definition uncertain, open to interpretation, flexible, and not easily definable.\" To many humanities scholars, articulating a detailed data model can feel like a reduction of their subject. Or perhaps they're eager to dive into material, and then end up with a mess of a spreadsheet, unsure how to turn it into anything usable. It's okay if your data model grows and changes over time, but it should be recognized as a formal step in the process. What does a data model look like? It depends on your project, but at its most basic, it could be a list of the fields in your spreadsheet, with a description of the values and their format. A data model for the Pets example above could be: PetID: a generic ID for each pet, max 6 alphanumeric characters Name: the given name of the pet. Animal: specify the type of animal from a controlled list: Dog, Cat, Bird, Gecko Breed: specify the breed of pet from a controlled vocabulary of Official Animal Breeds. You might also want to indicate the extent of this list of pets. Who owns them? What the connection between them? Where did this information come from? Obviously, this is not an ambitious data project as it stands, but we can imagine some other categories to include: age, location, weight, health, favorite toy, favorite nap spot, etc. When I decide to add another field or change how I enter a value, I must update my data model to reflect these changes. Perhaps I realize halfway through my data collection that I should be using the scientific classification for animals, rather than a generic \"dog\" and \"cat.\" If I had thought about my data model, I might have come to this decision at the beginning of my project. Think of data modeling as a favor to yourself and to anyone else who might encounter your data and wonder about your decisions. Extending your data # Once you have a good sense of how your data is going to look, it is worth considering options for extending your data. Do not assume you have to start from scratch. There are many other organizations and projects out there dedicated to creating data that can be used by others. Pelagios can help you describe historical places. PeriodO can help you describe time. Getty Art and Architecture Thesaurus contains thousands of terms for describing art and material culture. And of course, libraries have long been in the business of organizing and sharing their data. Linked data # One way of extending your data is through the use of something called \"linked data.\" Linked data is a method for structuring data so that it can be connected and linked to other data sets. You might also see this called the \"Semantic Web.\" One way to create linked data is through the use of URIs or Uniform Resource Identifiers. The idea is that every thing in your data should be given a unique identifier. When you or anyone else references that thing, you use the identifier to indicate that we are talking about the same thing. The value of URIs becomes clear in any project with people. It's common for people to share the same names - giving them a unique ID number eliminates confusion and unites variant forms of the same name. But URIs are not just ID numbers, the should be HTTP URIs, meaning they look like a URL. Here's one for Audre Lorde: https://viaf.org/viaf/61565157/. The other principle of linked data is the not only are the things given unique identifiers, so are the relationships between those things. We can standardize those connections and use URIs to express them. When we connect these URIs together, we start to see our data in a new way: the graph. Source: RDF Primer We haven't talked about RDF yet, but we can define it using terms we have just learned. RDF , or Resource Description Framework, is a data model for describing resources. It structures data through the use of triples or subject-predicate-object expressions, such as the ones we see in the image above. RDF can be serialized in several ways, through formats like Turtle, JSON-LD, RDFa, and RDF-XML. Okay, that was a lot of acronyms, but you should recognize some. Here's some RDF about Audre Lorde: <rdf:RDF> <rdf:Description rdf:about=\"http://viaf.org/viaf/sourceID/DBC%7C87097946621565#skos:Concept\"> <foaf:focus rdf:resource=\"http://viaf.org/viaf/61565157\"/> <skos:prefLabel>Lorde, Audre f. 1934</skos:prefLabel> <skos:inScheme rdf:resource=\"http://viaf.org/authorityScheme/DBC\"/> <rdf:type rdf:resource=\"http://www.w3.org/2004/02/skos/core#Concept\"/> </rdf:Description> <rdf:Description rdf:about=\"http://viaf.org/viaf/8101155708706522580005\"> <rdfs:label>Sister outsider</rdfs:label> </rdf:Description> <rdf:Description rdf:about=\"http://viaf.org/viaf/sourceID/NII%7CDA04877615#skos:Concept\"> <foaf:focus rdf:resource=\"http://viaf.org/viaf/61565157\"/> <skos:altLabel>\u30ed\u30fc\u30c9, \u30aa\u30fc\u30c9\u30ea</skos:altLabel> <skos:altLabel>Rollins, Audre Lorde</skos:altLabel> <skos:altLabel>Lorde, Audre Geraldine</skos:altLabel> <skos:altLabel>Lorde, Audre</skos:altLabel> <skos:prefLabel>Lorde, Audre</skos:prefLabel> <skos:inScheme rdf:resource=\"http://viaf.org/authorityScheme/NII\"/> <rdf:type rdf:resource=\"http://www.w3.org/2004/02/skos/core#Concept\"/> </rdf:Description> </rdf:RDF> It's not that much fun to read, but you're a human, not a computer. In this example, you can pick out terms like \"SKOS\" and \"FOAF.\" If you have been following along and nerding about about data modeling, go ahead and look those up or check out this tutorial: \" Introduction to the Principles of Linked Open Data \". If you're overwhelmed, tuck them in your pocket for later. This is a window into advanced work with humanities data. It's good to know it exists, but it's okay if you're not ready for it yet. Activities # Activity 3.1 # In this activity, we'll assess a dataset according to the criteria we've learned about in this section. Download the LexingtonCemetery.csv file. Open in Excel. Since this dataset is a CSV file, you may need to follow these instructions. The process may differ depending on what version of Excel you're using. Navigate to the Data tab in Excel. Select \"From Text\" then find your file. A wizard should pop up to walk you through the import process. It should automatically detect the commas, but if it doesn't, you will need to check the box to indicate that this file is delimited by commas. The data preview should separate each distinct column of data, rather than everything running together. What do you see? Take some time to explore this data by scrolling around. Get a sense of what is here and what might be missing. It might help to take a look at the cemetery website or to read this press release . In a separate document, start to draft a data model for this spreadsheet as you encounter it. Make note of the data types and formats, as well as suggestions for improvement. You'll turn in this data model with activity 3.3. Activity 3.2 # Let's take Open Refine for a spin in order to clean up this data set. Download Open Refine and run the program. Follow the installation instruction if you get stuck. It should open a new tab in your browser with the address http://127.0.0.1:3333/ . This address is local to your computer, not something you can actually visit on the internet. Import the LexingtonCemetery.csv file. You will see a page of import options, you should be able to accept the defaults. Spend some time clicking on all the options in Open Refine to see what happens. Set a timer and spend 10-15 minutes playing around with the software before you turn to documentation or instructions. What can you learn in this time? If things get too messy, you can always exit out and start over. Now answer these questions: What do the \"Text Facet\" or \"Numeric Facet\" features reveal about this data? Are there outliers that need investigation? Are there columns that should be split or joined? How would you do this in Open Refine (or Excel?) What columns would be best served by the \"cluster and edit\" feature? Why? Which columns need to be modified for consistency? Activity 3.3 # Time to take a little step back from this data set and think about its potential. In the same document that you started in Activity 3.1, answer the following questions: What cleanup work does this data need to be more usable? Do you need to adjust your data model? How might you extend this data set? What research questions does it inspire? Do you need to conduct more research to learn about the people in this data set? How will you go about it? What (or who?) might be missing from this data set? We have yet to learn about the ins and outs of methods of analyzing data, but what would you want to do with this data? Update your data model to reflect your thorough thinking about this data set. It should include a list of each field, its data type, and any notes about the content of this field. Bonus: visit the cemetery! How does experiencing it in person change your view of the data? Does everything match up? Do you observe other things that should be in this data? Activity 3.4 # Try your hand at creating your own dataset in a small group. Start with the Alumni Coeducation Correspondence, May 10, 1984 (report) from the W&L Digital Archive . Take some time to read through the report, making note of the content and how it is presented. How are the first few pages different than the rest? Create your data model in a Box note for your group in this folder . Look over all the types of information included in the report. Choose what fields you think you should include. Why are you choosing those fields? How would these fields be useful in exploring/visualizing the data? Will your data collection enable certain research questions? Is there anything you can leave out? Once your model is set up, go ahead and start transcribing the data. Each team will be assigned a few pages to transcribe into this Google spreadsheet . Try to avoid looking at other team's sheets. We want to be able to compare the differences at the end of this activity. After we come back from the small groups, compare your data model to that of other groups and answer the following questions. How are the models different? What did your group include that others didn't and visa versa? How do your fields change the questions you can ask the data? How could you use Open Refine to clean up the data you created? Readings # Tidy Data for the Humanities by Matt Lincoln Big? Smart? Clean? Messy? Data in the Humanities by Christof Sch\u00f6ch Designing Databases for Historical Research Against Cleaning - W&L login required","title":"Data"},{"location":"data/#what-is-data-again","text":"In the last section, we got as far as defining data as \"a value assigned to a thing.\" While that remains true, let's dig a little deeper. If we put on our humanities hat, we start to see data little more complexly. In her article, \" Humanities Approaches to Graphical Display ,\" Johanna Drucker posits the following about data: \"To overturn the assumptions that structure conventions acquired from other domains requires that we re-examine the intellectual foundations of digital humanities, putting techniques of graphical display on a foundation that is humanistic at its base. This requires first and foremost that we reconceive all data as capta. Differences in the etymological roots of the terms data and capta make the distinction between constructivist and realist approaches clear. Capta is \u201ctaken\u201d actively while data is assumed to be a \u201cgiven\u201d able to be recorded and observed. From this distinction, a world of differences arises. Humanistic inquiry acknowledges the situated, partial, and constitutive character of knowledge production, the recognition that knowledge is constructed, taken, not simply given as a natural representation of pre-existing fact.\" This is a long quote, but the concept of \"capta\" is a useful one. Even if we don't go around calling everything \"capta\" instead of \"data,\" it is a helpful reminder that data is constructed, especially but not only in the humanities. Every single piece of data represents a decision that someone made. The number of rows and columns, the labels, the format, the extent or granularity of the values and the things - these are all choices. They are choices you should consider when creating a data set from scratch or when you use someone else's data. In the rest of this section, we will dig into the details of putting together a data set.","title":"What is data again?"},{"location":"data/#data-structures","text":"Data can exist in many different forms. You might imagine a \"spreadsheet\" when thinking about data, but in truth, there are a number of other ways to express and structure your data.","title":"Data structures"},{"location":"data/#tabular-data","text":"This is our common spreadsheet data structure. Data fits into a table, in which each row represents a thing. Each column is a different field or modifier of that thing. The values reside in each individual cell. In the following table, each row represents one pet. PetID Name Animal Breed Pet1 Huxley Cat Tabby Pet2 Harper Dog Flat Coat Retriever Pet3 Hobbes Dog Hound If you're new to making spreadsheets, it can be tempting to swap the rows and columns so that each column represents a thing. It can also be tempting to include empty cells as spaces or use visual cues like color or bold/italics to set apart text. These methods will certainly help you as a human, but it's important to keep in mind that most of the software that might process your data will not be able to understand these visual cues. Most of the software that will process your data, whether it's for visualization or analysis, will want to see your data in a format known as CSV or Comma Separated Value. This format strips away any visual formatting and stores each cell in plain text, separated by a comma. So the above table about pets, would look like this: PetID,Name,Animal,Breed, Pet1,Huxley,Cat,Tabby, Pet2,Harper,Dog,Flat Coat Retriever, Pet3,Hobbes,Dog,Hound The CSV format reduces file size, simplifies the content, and keeps the file from being dependent on proprietary software like Excel. Fortunately, you don't have to construct your data with commas. Applications like Excel or Google Sheets will let you save or export your file with the file extension .csv . It's a good habit to get into when working with data. You might also see .tsv for tab separated values, in which the spaces we see when we hit the tab button function as a single character.","title":"Tabular data"},{"location":"data/#relational-data","text":"For some projects, a single table quickly becomes insufficient. Perhaps your project requires collecting data about a group of people, the houses they live in, the land they occupy, etc. It might be time to look into using a database to store all that data. A database works on a relational model - you might have multiple tables, but they are connected through shared values or \"keys.\" Using a database allows you to gather data about many different things, things that may require different fields to describe them. So my table of people may contain a column to note which house they live in, but I can rely on a totally separate table to list all the details about the house itself. This keeps me from cluttering my people table with information that isn't relevant to the main thing my row is about. Often, this is done with an ID number rather than the full title. A second advantage of a database is the ability to ask questions of your data. To continue the example from above, I can now ask questions like: How many people live in houses that are yellow? Of the houses on this block, which ones are occupied by people with children? These questions require that I filter based on certain values within each distinct data table and combine them into a new set. Typically, questions like this are asked with a language called SQL or Structure Query Language (yes, another language with its own syntax!). The basic syntax of SQL looks like this: SELECT column_name,column_name FROM table_name WHERE column_name operator value; Here's a more specific example. In this one, the asterisk means \"give me everything.\" So I'm asking for all the fields that exist about the cats. SELECT * FROM pets WHERE animal='cat'; Asking these types of questions of data is often what humanities scholars are after when they decide to build a database. However, designing a database and the interface that goes along with it is a big endeavor, and often requires the help of a professional database designer. Databases can be built in many different types of software (you might encounter names like MySQL or PostgreSQL), but web-based databases will need to be installed on a server, not just your own computer. If you're undertaking a big project, it's important to assess your resources for creating a database. Fortunately, there are an increasing number of options for storing, visualizing, and analyzing data without database software. Creating well-structured data ensures that your data can be imported into and out of these tools with ease.","title":"Relational data"},{"location":"data/#data-serialization","text":"Data serialization is the process by which data is transformed into another format that can be transferred or stored, then reconstituted. Often, this is necessary so that a programming language or software application can more easily and quickly do something with the data. With serialization comes a few different types of data structures that are important to know. JSON , or JavaScript Objection Notation, is a way to structure data based on attribute-value pairs and curly brackets. Just like CSV, if you're working with a JSON file, it will have the file extension .json . You probably won't end up writing JSON from scratch, but you might have to convert your data to JSON for a certain application. And FYI, JSON is not only used with JavaScript. Here's an example. Notice the level of structure introduced with the Address line. There are four fields and values (you might see this called an array) that modify one field, Address. { \"PetID\": \"Pet1\", \"Name\": \"Huxley\", \"Animal\": true, \"Breed\": \"Tabby\", \"Address\": { \"streetAddress\": \"555 5th St.\", \"city\": \"The Couch\", \"state\": \"Living Room\", \"postalCode\": \"55555\" }, } XML or Extensible Markup Language is more than just a serialized data format, it's also a markup language (like HTML). XML has been around a long time, so you will see it referenced in many Digital Humanities projects. XML is highly structured, but allows the user to define the tags that are used. A common application in humanities projects is the Text Encoding Initiative or TEI. TEI is used to markup humanities texts, just like you would markup a website, but instead you use tags like <l> for line and <epigraph> for epigraph. TEI allows you to do things like build a website for testing your ability to read rhyme and meter in poetry. It is a foundational practice in DH - the process by which a human interprets a text and encodes the meaning in a way that the computer can understand. Example: <lg rhyme=\"ABCCBBA\"> <l>The sunlight on the <rhyme label=\"A\">garden</rhyme> </l> <l> <rhyme label=\"A\">Harden</rhyme>s and grows <rhyme label=\"B\">cold</rhyme>,</l> <l>We cannot cage the <rhyme label=\"C\">minute</rhyme> </l> <l>Wi<rhyme label=\"C\">thin it</rhyme>s nets of <rhyme label=\"B\">gold</rhyme> </l> <l>When all is <rhyme label=\"B\">told</rhyme> </l> <l>We cannot beg for <rhyme label=\"A\">pardon</rhyme>.</l> </lg>","title":"Data serialization"},{"location":"data/#textual-data","text":"Not all data has needs to exist in a spreadsheet. In the humanities, we read a lot! We need a way to store textual data, such as a whole book, set of documents, or a century of newspaper articles. A later section will cover the ins and outs of text analysis, but if you're looking to analyze a corpus (another word for collection of texts) of some kind, you need to structure it in some way. The common method for storing textual data is in a plain text format or with the file extension .txt . Plain text files are just that - documents without the formatting that comes from MS Word or other word processing applications. Just like our HTML documents or our CSV files, a text document simplifies the content for the computer. When we're counting the number of times Jane Austen used the word \"honor\" or looking for the use of the word \"evolution\" over time , we are concerned with the words in the text, not the way the text looks. Plain text format reduces file size to improve processing time and transfer of data. You'll probably want to use a text editor for viewing these files, just like we did when writing HTML. Depending on what you're doing, you will also want to pay attention to the name and organization of your text files. If you noticed in the Voyant instance with Jane Austen, the title of each text was preceded by the year of publication. This file naming strategy makes it easy to sort the texts by their publication date. It's just something to pay attention to when constructing your corpus. If you do care about what the text looks like, you might want to look for ways to analyze TEI documents, not just plain text. Text Encoding allows you to add details about the appearance of the text, perhaps there's another hand writing in the margins or use of italics or white space in poetry.","title":"Textual data"},{"location":"data/#data-types","text":"Once you have settled on the right structure for your data, you need to consider the data types you will use in each field. Most programming languages accept the following: Data Type Example Strings or characters, often surrounded by quote marks \"String!\" Integer 4 Decimal 4.10 Boolean, another way to say true/false or yes/no TRUE Date, usually formatted as YYYY-MM-DD 2019-10-20 Time, usually formatted as HH:MM:SS 10:17:26 There are a few others data types, but these will be most relevant in humanities projects. You will find that Excel loves to chew up and spit out dates unless you standardize formatting in this way.","title":"Data types"},{"location":"data/#clean-tidy-and-normal-data","text":"As you start to work with data, you'll likely encounter these terms \"clean, tidy, normal,\" used as adjectives and verbs. What do they all mean? How do you know when your data is clean and tidy? Maybe your data is just the black sheep of the family! Cleaning data usually refers to the process of correcting the contents of your dataset. Typos, mistakes, new information can all be cleaned up after the data entry process is complete. You don't have to read every cell one by one, there are a number of tools and methods that will help automate this process. Often, visualization your data is a great way to find issues you didn't know you had. Tidy data comes from the community around R, the programming language. Tidy data is well-structured, or as Hadley Wickham puts it, \"Tidy datasets are easy to manipulate, model and visualise, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.\" Sounds familiar, right? Normalized data is a database term. It refers to the process of structuring your tables to reduce redundancy and complexity. Normalized data should be easier for both humans and computers to understand. Before we get to tools and methods for straightening up your data, let's take a moment for a pep talk. Data is messy! Whether you created it yourself or are using someone else's data, chances are, something will be wrong with it! The data entry process can be long, and it's easy to change your habits or come upon a better way to record things. This is normal. To set yourself up for success, document your decisions as you make them. It is easy to absorbed in your work, make a decision, then put your work down and forget about the decision a week later. Keeping a log of changes will also help anyone else who might be contributing. Be consistent! If you want to change a \"T\" to \"True,\" it's easy to find and replace. It's not as easy to go through a mix of values. Finally, remember what the computer is for. It can be tempting to embark on a cleanup project by hand, instead of looking for automated ways to correct your data. Knowing when to cleanup by hand or when to spend the time figuring out how to automate a task is a mark of experience. You won't get it right every time. But it is worthwhile to consider if something can be done in an automated way. Find and replace, when coupled with regular expressions, can be pretty powerful. It can feel like a waste to spend time automating a task when you could just do it, but the skills you learn in the process are valuable and can be applied on future tasks.","title":"Clean, tidy, and normal data"},{"location":"data/#data-cleaning-tools-and-methods","text":"","title":"Data cleaning tools and methods"},{"location":"data/#open-refine","text":"I try to avoid calling technology magic, but Open Refine is magic. It takes some getting used to, but it is indeed a \"powerful tool for working with messy data.\" When you install and run Open Refine, it launches a local server on computer so you interact with the application in your browser. However, just because it's in your browser does not mean it's connected to the internet. Once you've loaded your data, you use the drop down arrows to access various tools for modifying your data. This is not a tool for scrolling through your data, it's used for targeted approaches to specific issues. As an example, you can use the \"cluster and edit\" feature to find similar cells and standardize their contents. The \"text facet\" feature shows how many cells share the same value. When you sort by alphabetical order, it is really easy to see if you have entered the same name in two different ways. There are a number of tutorials on the documentation page . Try this one, \" Cleaning Data with OpenRefine \" from the Programming Historian.","title":"Open Refine"},{"location":"data/#excel","text":"Before you get to specialized tools like Open Refine, you can always use Excel or Google Sheets to check your work. Filter - use the filter feature to assess the contents of a single column. You can easily see if a cell is empty or has an incorrect entry. Data Validation - allows you to control what is entered into a cell. You can identify a list of options or select a specific data type. Formulas - there are plenty of spreadsheet formulas designed to work with strings of text. Use the help pages and a search engine to look for examples. There are multiple methods for splitting columns if you need to separate first and last names, or copy over the first half of a string.","title":"Excel"},{"location":"data/#regular-expressions","text":"Regular expressions are a method for matching patterns in text. It's a set of special characters that when used together in certain contexts helps you find and replace in powerful ways. For example, you could find all the phone numbers in a text by looking for the pattern (###) ###-#### or all forms of \"women\" by using the pattern \"wom.n\" to replace the fourth character. Regular expressions are awesome, but they can get frustrating quickly. Don't be overwhelmed by the complex patterns you encounter in tutorials. Even the basics can come in handy when cleaning data. Plus, there are a lot of resources out there for helping you use them effectively. Regular Expressions 101 Beginning with Regular Expressions Cleaning OCR'd text with regular expressions","title":"Regular Expressions"},{"location":"data/#data-modeling","text":"Whether you are aware of it or not, data modeling is part of the data creation process. All of the decisions that you might make about your data, the format, the data types, the fields, help to form a model of your data. Data modeling is a process that has been part of humanities work for a long time, just not by that name. When we attempt to organize the world through means like time periods, genres, or lists of names (prosopographies) or places (gazeteers), we are creating models that may or may not line up with reality. A model is an abstraction of an object. It can be a window into our disciplinary or personal values. The tools you choose to use will also contribute to the shape of your data model, as they can limit what and how you record your data. There is a lot more to be said about data models and their role in humanities data work. There are different types of models and various ways to approach their creation and use. Data modeling has precise meanings within other disciplines, such as computer science. If you want to learn more, start with \" Data Modeling in a Digital Humanities Context \" by Julia Flanders and Fotis Jannidis. But for now, let's return to that Miriam Posner article, \" Humanities Data: A Necessary Contradiction .\" \"When you call something data, you imply that it exists in discrete, fungible units; that it is computationally tractable; that its meaningful qualities can be enumerated in a finite list; that someone else performing the same operations on the same data will come up with the same results. This is not how humanists think of the material they work with.\" Data modeling is important precisely because it forces humanities scholars to clearly identify their goals and parameters. If you start a project without an initial idea of your data model, you could lose years sifting through archival material or transcribing handwritten notes. Or as Scott Weingart writes , \"Humanistic data are almost by definition uncertain, open to interpretation, flexible, and not easily definable.\" To many humanities scholars, articulating a detailed data model can feel like a reduction of their subject. Or perhaps they're eager to dive into material, and then end up with a mess of a spreadsheet, unsure how to turn it into anything usable. It's okay if your data model grows and changes over time, but it should be recognized as a formal step in the process. What does a data model look like? It depends on your project, but at its most basic, it could be a list of the fields in your spreadsheet, with a description of the values and their format. A data model for the Pets example above could be: PetID: a generic ID for each pet, max 6 alphanumeric characters Name: the given name of the pet. Animal: specify the type of animal from a controlled list: Dog, Cat, Bird, Gecko Breed: specify the breed of pet from a controlled vocabulary of Official Animal Breeds. You might also want to indicate the extent of this list of pets. Who owns them? What the connection between them? Where did this information come from? Obviously, this is not an ambitious data project as it stands, but we can imagine some other categories to include: age, location, weight, health, favorite toy, favorite nap spot, etc. When I decide to add another field or change how I enter a value, I must update my data model to reflect these changes. Perhaps I realize halfway through my data collection that I should be using the scientific classification for animals, rather than a generic \"dog\" and \"cat.\" If I had thought about my data model, I might have come to this decision at the beginning of my project. Think of data modeling as a favor to yourself and to anyone else who might encounter your data and wonder about your decisions.","title":"Data modeling"},{"location":"data/#extending-your-data","text":"Once you have a good sense of how your data is going to look, it is worth considering options for extending your data. Do not assume you have to start from scratch. There are many other organizations and projects out there dedicated to creating data that can be used by others. Pelagios can help you describe historical places. PeriodO can help you describe time. Getty Art and Architecture Thesaurus contains thousands of terms for describing art and material culture. And of course, libraries have long been in the business of organizing and sharing their data.","title":"Extending your data"},{"location":"data/#linked-data","text":"One way of extending your data is through the use of something called \"linked data.\" Linked data is a method for structuring data so that it can be connected and linked to other data sets. You might also see this called the \"Semantic Web.\" One way to create linked data is through the use of URIs or Uniform Resource Identifiers. The idea is that every thing in your data should be given a unique identifier. When you or anyone else references that thing, you use the identifier to indicate that we are talking about the same thing. The value of URIs becomes clear in any project with people. It's common for people to share the same names - giving them a unique ID number eliminates confusion and unites variant forms of the same name. But URIs are not just ID numbers, the should be HTTP URIs, meaning they look like a URL. Here's one for Audre Lorde: https://viaf.org/viaf/61565157/. The other principle of linked data is the not only are the things given unique identifiers, so are the relationships between those things. We can standardize those connections and use URIs to express them. When we connect these URIs together, we start to see our data in a new way: the graph. Source: RDF Primer We haven't talked about RDF yet, but we can define it using terms we have just learned. RDF , or Resource Description Framework, is a data model for describing resources. It structures data through the use of triples or subject-predicate-object expressions, such as the ones we see in the image above. RDF can be serialized in several ways, through formats like Turtle, JSON-LD, RDFa, and RDF-XML. Okay, that was a lot of acronyms, but you should recognize some. Here's some RDF about Audre Lorde: <rdf:RDF> <rdf:Description rdf:about=\"http://viaf.org/viaf/sourceID/DBC%7C87097946621565#skos:Concept\"> <foaf:focus rdf:resource=\"http://viaf.org/viaf/61565157\"/> <skos:prefLabel>Lorde, Audre f. 1934</skos:prefLabel> <skos:inScheme rdf:resource=\"http://viaf.org/authorityScheme/DBC\"/> <rdf:type rdf:resource=\"http://www.w3.org/2004/02/skos/core#Concept\"/> </rdf:Description> <rdf:Description rdf:about=\"http://viaf.org/viaf/8101155708706522580005\"> <rdfs:label>Sister outsider</rdfs:label> </rdf:Description> <rdf:Description rdf:about=\"http://viaf.org/viaf/sourceID/NII%7CDA04877615#skos:Concept\"> <foaf:focus rdf:resource=\"http://viaf.org/viaf/61565157\"/> <skos:altLabel>\u30ed\u30fc\u30c9, \u30aa\u30fc\u30c9\u30ea</skos:altLabel> <skos:altLabel>Rollins, Audre Lorde</skos:altLabel> <skos:altLabel>Lorde, Audre Geraldine</skos:altLabel> <skos:altLabel>Lorde, Audre</skos:altLabel> <skos:prefLabel>Lorde, Audre</skos:prefLabel> <skos:inScheme rdf:resource=\"http://viaf.org/authorityScheme/NII\"/> <rdf:type rdf:resource=\"http://www.w3.org/2004/02/skos/core#Concept\"/> </rdf:Description> </rdf:RDF> It's not that much fun to read, but you're a human, not a computer. In this example, you can pick out terms like \"SKOS\" and \"FOAF.\" If you have been following along and nerding about about data modeling, go ahead and look those up or check out this tutorial: \" Introduction to the Principles of Linked Open Data \". If you're overwhelmed, tuck them in your pocket for later. This is a window into advanced work with humanities data. It's good to know it exists, but it's okay if you're not ready for it yet.","title":"Linked data"},{"location":"data/#activities","text":"","title":"Activities"},{"location":"data/#activity-31","text":"In this activity, we'll assess a dataset according to the criteria we've learned about in this section. Download the LexingtonCemetery.csv file. Open in Excel. Since this dataset is a CSV file, you may need to follow these instructions. The process may differ depending on what version of Excel you're using. Navigate to the Data tab in Excel. Select \"From Text\" then find your file. A wizard should pop up to walk you through the import process. It should automatically detect the commas, but if it doesn't, you will need to check the box to indicate that this file is delimited by commas. The data preview should separate each distinct column of data, rather than everything running together. What do you see? Take some time to explore this data by scrolling around. Get a sense of what is here and what might be missing. It might help to take a look at the cemetery website or to read this press release . In a separate document, start to draft a data model for this spreadsheet as you encounter it. Make note of the data types and formats, as well as suggestions for improvement. You'll turn in this data model with activity 3.3.","title":"Activity 3.1"},{"location":"data/#activity-32","text":"Let's take Open Refine for a spin in order to clean up this data set. Download Open Refine and run the program. Follow the installation instruction if you get stuck. It should open a new tab in your browser with the address http://127.0.0.1:3333/ . This address is local to your computer, not something you can actually visit on the internet. Import the LexingtonCemetery.csv file. You will see a page of import options, you should be able to accept the defaults. Spend some time clicking on all the options in Open Refine to see what happens. Set a timer and spend 10-15 minutes playing around with the software before you turn to documentation or instructions. What can you learn in this time? If things get too messy, you can always exit out and start over. Now answer these questions: What do the \"Text Facet\" or \"Numeric Facet\" features reveal about this data? Are there outliers that need investigation? Are there columns that should be split or joined? How would you do this in Open Refine (or Excel?) What columns would be best served by the \"cluster and edit\" feature? Why? Which columns need to be modified for consistency?","title":"Activity 3.2"},{"location":"data/#activity-33","text":"Time to take a little step back from this data set and think about its potential. In the same document that you started in Activity 3.1, answer the following questions: What cleanup work does this data need to be more usable? Do you need to adjust your data model? How might you extend this data set? What research questions does it inspire? Do you need to conduct more research to learn about the people in this data set? How will you go about it? What (or who?) might be missing from this data set? We have yet to learn about the ins and outs of methods of analyzing data, but what would you want to do with this data? Update your data model to reflect your thorough thinking about this data set. It should include a list of each field, its data type, and any notes about the content of this field. Bonus: visit the cemetery! How does experiencing it in person change your view of the data? Does everything match up? Do you observe other things that should be in this data?","title":"Activity 3.3"},{"location":"data/#activity-34","text":"Try your hand at creating your own dataset in a small group. Start with the Alumni Coeducation Correspondence, May 10, 1984 (report) from the W&L Digital Archive . Take some time to read through the report, making note of the content and how it is presented. How are the first few pages different than the rest? Create your data model in a Box note for your group in this folder . Look over all the types of information included in the report. Choose what fields you think you should include. Why are you choosing those fields? How would these fields be useful in exploring/visualizing the data? Will your data collection enable certain research questions? Is there anything you can leave out? Once your model is set up, go ahead and start transcribing the data. Each team will be assigned a few pages to transcribe into this Google spreadsheet . Try to avoid looking at other team's sheets. We want to be able to compare the differences at the end of this activity. After we come back from the small groups, compare your data model to that of other groups and answer the following questions. How are the models different? What did your group include that others didn't and visa versa? How do your fields change the questions you can ask the data? How could you use Open Refine to clean up the data you created?","title":"Activity 3.4"},{"location":"data/#readings","text":"Tidy Data for the Humanities by Matt Lincoln Big? Smart? Clean? Messy? Data in the Humanities by Christof Sch\u00f6ch Designing Databases for Historical Research Against Cleaning - W&L login required","title":"Readings"},{"location":"how-the-web-works/","text":"In this section, we'll explore how Internet works, learn the basics of HTML and CSS, and setup your own website on WordPress. Before we can start doing research with humanities data, we need to understand a little bit about the context of our digital world. Table of Contents: What even is the Internet? HTML Text Editors CSS Your domain Privacy Reclaim Hosting WordPress Activities Activity 1.2: HTML Activity 1.3: CSS Activity 1.4: Make it live Activity 1.5: Install WordPress Activity 1.6: Customize WordPress Resources What even is the Internet? # The Internet is magic! Just kidding, it's not, but it can certainly seem that way. Maybe you're reading this on your phone in the middle of the lawn, or curled up in bed with your laptop. It's more than likely that you're using Wi-Fi or a cellular network and therefore have no physical connection to the Internet. It's understandable that it feels like magic sometimes! But in reality, the Internet is an immensely physical thing. Put simply, it is computers connected to other computers. The information (text or media and all forms in between) you send and receive travels through wires and fiber-optic cables in the ground and even under the ocean . These connected or \"networked\" computers talk to each other using their own languages or more precisely, specific protocols. For instance, every computer has an address, known as an IP (Internet Protocol) address, to help direct traffic to the right place. Another protocol that you use, even if you don't realize it, is the Hypertext Transfer Protocol, or HTTP. Your browser uses HTTP to render websites. When you type in http://www.wlu.edu your browser sends out a request to the computer that can serve up all the files at that particular address. The \"server\" returns the files and your browser (Safari, Chrome, Firefox) renders them into a website. At its simplest, a website is just a folder full of files and images. Those files are full of code, which we'll learn about in the next section. Make sense? Here are a few videos that might help you visualize this great network of computers: What is the Internet? The Internet: Wires, Cables, and Wi-fi HTML # HTML, or Hypertext Markup Language, is one of the basic building blocks of the web. Every website you see is created using this language, from the most basic to most interactive. To prove it, go to your browser open up a new tab right now. Visit any website you like. Right click, or ctrl + click to open up an options menu. Select View Page Source . If you're using Safari, try pressing Command + Option + i . You should see a new window full of text surrounded by angle brackets. That's HTML. HTML stands for HyperText Markup Language. It is a set of tags, or elements , that adds structure to a document or page. When you write a document, you rely on style to indicate something about the text. You might put the title in a bigger font or break up paragraphs with tabs or new lines. Markup languages do this by adding tags around the content you wish to set apart. For example: <h1>This is a top level heading</h1> is HTML that tells the browser to 1) increase the size of the text, but more importantly 2) that this piece of text represents a major section of the document. Which pieces of text on this page are an <h1> ? Can you use the View Page Source trick to confirm? What are other tags that you might expect to exist? If you were to analyze a website, what are the major components? What are the familiar conventions of websites, regardless of the type? Some things that come to mind: menus or navigation <nav> , images <img src=\"kittens.jpg\"> , or even just your basic paragraph <p> . To give you a taste, a basic HTML document might look like this: <html> <head> <title>My Awesome Website</title> </head> <body> <h1>Welcome/h1> <p>This is a long paragraph about my cat.</p> </body> </html> As you can see, angle brackets surround each tag. The tags themselves around pieces of content. There's an opening tag and a closing tag. You can tell the closing tag by the slash after the angle bracket </title> . You should also notice that the tags are nested. The <head> and <body> tag are both children of <html> , and each of those tags has their own children. We'll learn more HTML down in Activity 1.2 . For now, the thing to remember is that HTML exists to tell the computer, specifically the browser, what to do with each piece of content. Another way to say this is semantic markup . Attaching structural or semantic meaning to content is especially important because not everyone (people or computers) uses their eyes to read the Web. Using valid HTML ensures that the Web is accessible to everyone, regardless of their ability. For instance, using heading tags like <h1> and <h2> tell a person about the organization of the page more than making text bold with the <strong> tag. Altering the visual apprearance of the text without the structural headings won't convey the same meaning to a person who uses a screen reader. Even if you don't go on to build websites for a living, something as simple as adding a caption to any image you publish can improve accessibility. An image without a caption or description is meaningless to someone who can't see it. Can you find the caption or \"alt text\" for the image on this page? Text Editors # Before we move on, there's one other important thing to understand about writing HTML. HTML documents are plain text documents. You don't write HTML in a Word doc or a Google doc, instead it's best to use a piece of software called a text editor. There are many text editors out there and it's personal preference which one you use. I like one called Sublime , but I know others use Atom or Visual Studio . Try a few out to see which one suits you. Even though the text is \"plain\", text editors have a feature called syntax highlighting , which means that the text changes color to help you write good code. When it's time to see what your markup or code can do, you'll need to open it in your browser. CSS # Our next building block of the web is CSS, or Cascading Style Sheets. We just learned that HTML will add structure to your document, but you need something else to add the pretty colors and images, also known as style . A style sheet is a plain text document that lists all the styles that you want to apply to your page. The cascading part means that you can be selective about how elements appear. If you analyze this page, what styles would you guess are in play? Are there some elements that look different than other elements? Cascading style sheets allow me to indicate that I want my headings to use a different font than my body text, for instance. CSS uses a different syntax than HTML. It can be a little confusing because it relies on HTML tags, but the structure is a little different. Think about HTML as a document, and CSS as a separate list. Here's an example: body { background-color: aquamarine; font-family: Helvetica, sans-serif; } h1 { font-size: 150%; } The first step is to identify the HTML tag you wish to style. Here, we're styling everything that lives within the <body> tag. In CSS language, this is a selector. Once we identify our selector, we list, or declare the things we want to style within a pair of curly brackets. This can be called the \"declaration block.\" We identify the thing, or \"property\" we wish to style, followed by a colon, then list the \"value\" we want to use, ending with a semi-colon. In CSS terms, it looks like this: selector { property: value; otherProperty: anotherValue; } The syntax takes getting used to, but remember, you do not have to memorize every property or selector. Even experienced Web designers have to look things up. I recommend using the W3 Schools to find reliable code to copy. There's lots to love about CSS. In a relatively simple document, you can control the style of hundreds of HTML documents and ensure consistency. But you can also pinpoint one specific margin or border and change its color or add some padding. One final note: to ensure that your CSS actually appears on your page, you need to add a piece of code to your HTML document that references the CSS document. We'll do this in Activity 1.3 . Your domain # There are many reasons you might want your own website. Whether it's for work, school, volunteering, organizing, art, or just for fun, there are plenty of ways to carve out your own corner of the Web. Social media platforms like LinkedIn or Facebook are ubiquitous now, but it used to be that if you wanted to appear in search results, you had to create your own website. If you are a student or navigating the professional world, chances are that 1) someone out there might Google you and 2) you want them to see accurate information and work that you're proud of. A professional website can be a simple single page with limited information, or it can contain your whole life. Chances are, at some point you've relied on information from a personal website. Maybe you found an article you need for a bibliography, a snippet of code that had been giving you trouble, or even an email address for someone who has lost their wallet. If you're conducting research in a field, there are others who might be interested in the work you're doing. Check out the sites of others in your intended field of study to see what their websites look like. Some scholars share articles they've written, code libraries, data they've gathered, or new methods they're experimenting with. In the Digital Humanities, a field where many humanists work with data, open and public scholarship is highly valued. Many of the links in this coursebook go to the websites of DH-ers who have been generous to share a tutorial or write up their thoughts on a particular method. Whether you create a professional portfolio or not, there is a lot to be said for the form of a Website. Most of the scholarship we read, not to mention everything else, has been designed and formatted to be consumed on the Web. Presenting your own work in that format gives you experience with seemingly silly things like getting your images to float nicely with your text or finding a place to reliably store your data. Creating a clean, well-organized website is harder than it looks! Just like it's easier to read a paragraph than to write one, creating a website takes practice. I recommend posting the exercises and projects of this coursebook to your own website. Privacy # All that being said, you do not have to have a strong presence on the Web. You may have personal and legitimate reasons for not wanting to be found in search results. If you're a student, you may not want your homework following you for decades to come. You have the right to privacy and to completing your coursework in a secure way. Reclaim Hosting # There are many places to find space on the Web, but many individuals and academic institutions work with Reclaim Hosting , an education-focused web hosting company. Reclaim offers reasonable rates, good support, and an awareness of the needs of academic projects. Some institutions offer a \"domain of one's\" program in which affiliated individuals can obtain a domain. WordPress # Activities # Activity 1.2: HTML # Using Sublime Text Editor, start a new document. Open the W3 Schools tutorial in a new tab. Let's write some HTML! First, save the file as index.html on your Desktop. Save your file regularly. Using the HTML Basic as a guide, add the following tags to your document: <html> <head> <title> <body> . To view your page in the browser, open index.html in your browser, usually with the key commands Ctrl + o Explore the tutorial on W3 and add seven more types of tags to the body of your HTML document, including a table, a link, and an image. Create a second HTML page and link the two pages. Activity 1.3: CSS # Create a separate CSS document and save it as style.css in the same place as Activity 1.2. Link the style.css file to your HTML document via the external stylesheet method. Consult the W3 Schools to figure out how to do this. Add a background color. Change the border on your table. Add style to your links when you hover over them. Activity 1.4: Make it live # When you opened your HTML files in your browser, you were viewing your files locally. Only you could see them on your computer. Now it's time to upload them to your domain so other people can view them. Navigate to http://yourdomain.wludci.info/cpanel and login. In CPanel, open up the File Manager. Navigate to the public_html folder. Create a new folder titled activities or something similar. Upload all your HTML and CSS files. Navigate to your equivalent of www.yourdomain.com/activities . What do you see? Activity 1.5: Install WordPress # Login to the Cpanel for your domain by adding /cpanel to the end of your URL. Use the username and password sent to you by Reclaim Hosting/Jason Mickel. You should see a Wordpress icon near the top of the page. Click on it to begin installing Wordpress. Find the \"install this application\" button. Work through the installation. You can leave the default settings except for the following: Directory - since you might want to use this domain for other things, let's put course materials in a subdirectory, such as http://www.mydomain.info/dci102 . Change the administrator username and password to something you are likely to remember. Customize the website title and tagline. Press install to finish up. You should now be able to access your wordpress admin interface by appending /wp-admin to your selected URL. Activity 1.6: Customize WordPress # Change your theme by going to Appearance > Themes . Select an installed theme, or find a new one with the Add New button. Activate multiple themes to try them out! Check your comment settings in Settings > Discussion to make sure your classmates will be able to comment on your posts. Set up your menu (remember we'll have 3 units with similar assignments) by going to Appearance > Menus . You will need to create a menu, add pages to it, and select a location for that menu. This is usually the most confusing part of Wordpress! Before you start adding content to your site, play around with as many of the settings and features as possible. What do all the bells and whistles do? Add your own images or colors. This is your site! Resources # Domain of One's Own Documentation HTML & CSS is Hard Reclaim Hosting Support W3 Schools HTML W3 Schools CSS","title":"How the Web Works"},{"location":"how-the-web-works/#what-even-is-the-internet","text":"The Internet is magic! Just kidding, it's not, but it can certainly seem that way. Maybe you're reading this on your phone in the middle of the lawn, or curled up in bed with your laptop. It's more than likely that you're using Wi-Fi or a cellular network and therefore have no physical connection to the Internet. It's understandable that it feels like magic sometimes! But in reality, the Internet is an immensely physical thing. Put simply, it is computers connected to other computers. The information (text or media and all forms in between) you send and receive travels through wires and fiber-optic cables in the ground and even under the ocean . These connected or \"networked\" computers talk to each other using their own languages or more precisely, specific protocols. For instance, every computer has an address, known as an IP (Internet Protocol) address, to help direct traffic to the right place. Another protocol that you use, even if you don't realize it, is the Hypertext Transfer Protocol, or HTTP. Your browser uses HTTP to render websites. When you type in http://www.wlu.edu your browser sends out a request to the computer that can serve up all the files at that particular address. The \"server\" returns the files and your browser (Safari, Chrome, Firefox) renders them into a website. At its simplest, a website is just a folder full of files and images. Those files are full of code, which we'll learn about in the next section. Make sense? Here are a few videos that might help you visualize this great network of computers: What is the Internet? The Internet: Wires, Cables, and Wi-fi","title":"What even is the Internet?"},{"location":"how-the-web-works/#html","text":"HTML, or Hypertext Markup Language, is one of the basic building blocks of the web. Every website you see is created using this language, from the most basic to most interactive. To prove it, go to your browser open up a new tab right now. Visit any website you like. Right click, or ctrl + click to open up an options menu. Select View Page Source . If you're using Safari, try pressing Command + Option + i . You should see a new window full of text surrounded by angle brackets. That's HTML. HTML stands for HyperText Markup Language. It is a set of tags, or elements , that adds structure to a document or page. When you write a document, you rely on style to indicate something about the text. You might put the title in a bigger font or break up paragraphs with tabs or new lines. Markup languages do this by adding tags around the content you wish to set apart. For example: <h1>This is a top level heading</h1> is HTML that tells the browser to 1) increase the size of the text, but more importantly 2) that this piece of text represents a major section of the document. Which pieces of text on this page are an <h1> ? Can you use the View Page Source trick to confirm? What are other tags that you might expect to exist? If you were to analyze a website, what are the major components? What are the familiar conventions of websites, regardless of the type? Some things that come to mind: menus or navigation <nav> , images <img src=\"kittens.jpg\"> , or even just your basic paragraph <p> . To give you a taste, a basic HTML document might look like this: <html> <head> <title>My Awesome Website</title> </head> <body> <h1>Welcome/h1> <p>This is a long paragraph about my cat.</p> </body> </html> As you can see, angle brackets surround each tag. The tags themselves around pieces of content. There's an opening tag and a closing tag. You can tell the closing tag by the slash after the angle bracket </title> . You should also notice that the tags are nested. The <head> and <body> tag are both children of <html> , and each of those tags has their own children. We'll learn more HTML down in Activity 1.2 . For now, the thing to remember is that HTML exists to tell the computer, specifically the browser, what to do with each piece of content. Another way to say this is semantic markup . Attaching structural or semantic meaning to content is especially important because not everyone (people or computers) uses their eyes to read the Web. Using valid HTML ensures that the Web is accessible to everyone, regardless of their ability. For instance, using heading tags like <h1> and <h2> tell a person about the organization of the page more than making text bold with the <strong> tag. Altering the visual apprearance of the text without the structural headings won't convey the same meaning to a person who uses a screen reader. Even if you don't go on to build websites for a living, something as simple as adding a caption to any image you publish can improve accessibility. An image without a caption or description is meaningless to someone who can't see it. Can you find the caption or \"alt text\" for the image on this page?","title":"HTML"},{"location":"how-the-web-works/#text-editors","text":"Before we move on, there's one other important thing to understand about writing HTML. HTML documents are plain text documents. You don't write HTML in a Word doc or a Google doc, instead it's best to use a piece of software called a text editor. There are many text editors out there and it's personal preference which one you use. I like one called Sublime , but I know others use Atom or Visual Studio . Try a few out to see which one suits you. Even though the text is \"plain\", text editors have a feature called syntax highlighting , which means that the text changes color to help you write good code. When it's time to see what your markup or code can do, you'll need to open it in your browser.","title":"Text Editors"},{"location":"how-the-web-works/#css","text":"Our next building block of the web is CSS, or Cascading Style Sheets. We just learned that HTML will add structure to your document, but you need something else to add the pretty colors and images, also known as style . A style sheet is a plain text document that lists all the styles that you want to apply to your page. The cascading part means that you can be selective about how elements appear. If you analyze this page, what styles would you guess are in play? Are there some elements that look different than other elements? Cascading style sheets allow me to indicate that I want my headings to use a different font than my body text, for instance. CSS uses a different syntax than HTML. It can be a little confusing because it relies on HTML tags, but the structure is a little different. Think about HTML as a document, and CSS as a separate list. Here's an example: body { background-color: aquamarine; font-family: Helvetica, sans-serif; } h1 { font-size: 150%; } The first step is to identify the HTML tag you wish to style. Here, we're styling everything that lives within the <body> tag. In CSS language, this is a selector. Once we identify our selector, we list, or declare the things we want to style within a pair of curly brackets. This can be called the \"declaration block.\" We identify the thing, or \"property\" we wish to style, followed by a colon, then list the \"value\" we want to use, ending with a semi-colon. In CSS terms, it looks like this: selector { property: value; otherProperty: anotherValue; } The syntax takes getting used to, but remember, you do not have to memorize every property or selector. Even experienced Web designers have to look things up. I recommend using the W3 Schools to find reliable code to copy. There's lots to love about CSS. In a relatively simple document, you can control the style of hundreds of HTML documents and ensure consistency. But you can also pinpoint one specific margin or border and change its color or add some padding. One final note: to ensure that your CSS actually appears on your page, you need to add a piece of code to your HTML document that references the CSS document. We'll do this in Activity 1.3 .","title":"CSS"},{"location":"how-the-web-works/#your-domain","text":"There are many reasons you might want your own website. Whether it's for work, school, volunteering, organizing, art, or just for fun, there are plenty of ways to carve out your own corner of the Web. Social media platforms like LinkedIn or Facebook are ubiquitous now, but it used to be that if you wanted to appear in search results, you had to create your own website. If you are a student or navigating the professional world, chances are that 1) someone out there might Google you and 2) you want them to see accurate information and work that you're proud of. A professional website can be a simple single page with limited information, or it can contain your whole life. Chances are, at some point you've relied on information from a personal website. Maybe you found an article you need for a bibliography, a snippet of code that had been giving you trouble, or even an email address for someone who has lost their wallet. If you're conducting research in a field, there are others who might be interested in the work you're doing. Check out the sites of others in your intended field of study to see what their websites look like. Some scholars share articles they've written, code libraries, data they've gathered, or new methods they're experimenting with. In the Digital Humanities, a field where many humanists work with data, open and public scholarship is highly valued. Many of the links in this coursebook go to the websites of DH-ers who have been generous to share a tutorial or write up their thoughts on a particular method. Whether you create a professional portfolio or not, there is a lot to be said for the form of a Website. Most of the scholarship we read, not to mention everything else, has been designed and formatted to be consumed on the Web. Presenting your own work in that format gives you experience with seemingly silly things like getting your images to float nicely with your text or finding a place to reliably store your data. Creating a clean, well-organized website is harder than it looks! Just like it's easier to read a paragraph than to write one, creating a website takes practice. I recommend posting the exercises and projects of this coursebook to your own website.","title":"Your domain"},{"location":"how-the-web-works/#privacy","text":"All that being said, you do not have to have a strong presence on the Web. You may have personal and legitimate reasons for not wanting to be found in search results. If you're a student, you may not want your homework following you for decades to come. You have the right to privacy and to completing your coursework in a secure way.","title":"Privacy"},{"location":"how-the-web-works/#reclaim-hosting","text":"There are many places to find space on the Web, but many individuals and academic institutions work with Reclaim Hosting , an education-focused web hosting company. Reclaim offers reasonable rates, good support, and an awareness of the needs of academic projects. Some institutions offer a \"domain of one's\" program in which affiliated individuals can obtain a domain.","title":"Reclaim Hosting"},{"location":"how-the-web-works/#wordpress","text":"","title":"WordPress"},{"location":"how-the-web-works/#activities","text":"","title":"Activities"},{"location":"how-the-web-works/#activity-12-html","text":"Using Sublime Text Editor, start a new document. Open the W3 Schools tutorial in a new tab. Let's write some HTML! First, save the file as index.html on your Desktop. Save your file regularly. Using the HTML Basic as a guide, add the following tags to your document: <html> <head> <title> <body> . To view your page in the browser, open index.html in your browser, usually with the key commands Ctrl + o Explore the tutorial on W3 and add seven more types of tags to the body of your HTML document, including a table, a link, and an image. Create a second HTML page and link the two pages.","title":"Activity 1.2: HTML"},{"location":"how-the-web-works/#activity-13-css","text":"Create a separate CSS document and save it as style.css in the same place as Activity 1.2. Link the style.css file to your HTML document via the external stylesheet method. Consult the W3 Schools to figure out how to do this. Add a background color. Change the border on your table. Add style to your links when you hover over them.","title":"Activity 1.3: CSS"},{"location":"how-the-web-works/#activity-14-make-it-live","text":"When you opened your HTML files in your browser, you were viewing your files locally. Only you could see them on your computer. Now it's time to upload them to your domain so other people can view them. Navigate to http://yourdomain.wludci.info/cpanel and login. In CPanel, open up the File Manager. Navigate to the public_html folder. Create a new folder titled activities or something similar. Upload all your HTML and CSS files. Navigate to your equivalent of www.yourdomain.com/activities . What do you see?","title":"Activity 1.4: Make it live"},{"location":"how-the-web-works/#activity-15-install-wordpress","text":"Login to the Cpanel for your domain by adding /cpanel to the end of your URL. Use the username and password sent to you by Reclaim Hosting/Jason Mickel. You should see a Wordpress icon near the top of the page. Click on it to begin installing Wordpress. Find the \"install this application\" button. Work through the installation. You can leave the default settings except for the following: Directory - since you might want to use this domain for other things, let's put course materials in a subdirectory, such as http://www.mydomain.info/dci102 . Change the administrator username and password to something you are likely to remember. Customize the website title and tagline. Press install to finish up. You should now be able to access your wordpress admin interface by appending /wp-admin to your selected URL.","title":"Activity 1.5: Install WordPress"},{"location":"how-the-web-works/#activity-16-customize-wordpress","text":"Change your theme by going to Appearance > Themes . Select an installed theme, or find a new one with the Add New button. Activate multiple themes to try them out! Check your comment settings in Settings > Discussion to make sure your classmates will be able to comment on your posts. Set up your menu (remember we'll have 3 units with similar assignments) by going to Appearance > Menus . You will need to create a menu, add pages to it, and select a location for that menu. This is usually the most confusing part of Wordpress! Before you start adding content to your site, play around with as many of the settings and features as possible. What do all the bells and whistles do? Add your own images or colors. This is your site!","title":"Activity 1.6: Customize WordPress"},{"location":"how-the-web-works/#resources","text":"Domain of One's Own Documentation HTML & CSS is Hard Reclaim Hosting Support W3 Schools HTML W3 Schools CSS","title":"Resources"},{"location":"network-analysis/","text":"In this section, we'll explore network analysis as a tool for understanding relationships between people and things. What is network analysis? Nodes, edges, ties, what? Network data Tools Activities Activity 5.1 Activity 5.2 Activity 5.3 Activity 5.4 Resources Readings What is network analysis? # Networks are everywhere. Once you start, it's hard to stop seeing them. The internet is a network, Facebook is a network, Kevin Bacon is the central node in the Hollywood network. Studying the connections between people and things is a major activity in the humanities, so \"network analysis\" feels like a natural method for working with humanities data. Network analysis varies by discipline, but originates from graph theory in mathematics. This is a simplification, but the idea is that one can measure significant people in a network by how many connections they have to other people and by the weight or significance of those connections. You can also learn who might serve as a bridge between social groups or whether you have a dense network of highly connected people or a more disperse group. We can visualize these networks through a network graph - points connected by lines. You don't necessarily have to just study people with network analysis, but it's a common approach. The specific name for this methodology is SNA or Social Network Analysis. SNA is used in many disciplines, with a heavy presence in the social sciences. It's important to recognize that each discipline will have its own conventions and expectations for SNA. You should tread carefully until you have a firm foundation. As Scott Weingart reminds us in \" Networks Demystified :\" \"Networks can be used on any project. Networks should be used on far fewer.\" This is not to scare you! But learning about network analysis is the perfect moment to pause and remember that you are not working in a vacuum. The methods you use have a historical and modern context and you are responsible for learning that context. That being said, network visualizations can be a great place to start. They can help you learn more about your data or illuminate a new direction for your research agenda. Plus, they're fun. Learning the basics will help you be better judge of whether it is, in fact, a good fit for your project. Nodes, edges, ties, what? # Let's take a moment to define some of the terms you'll encounter in network analysis. Nodes/points/agents/vertices - All of these words refer to the points on your network graph. If you're doing social network analysis, the points are the people in your graph. Edges/ties - Refers to the connections between nodes, usually represented by lines between points. Edges can be weighted (stronger or weaker edges) or directional (some relationships are mutual, some are not). The line could be drawn thin or thick based on the weight, or with an arrow to signify direction. The connection between you and your roommate might have a strong weight for example. If you follow a celebrity on Twitter, but they don't follow you back, we can imagine that as a relationship with a single direction. Centrality - the more connections a node has, the more central they are to the network. You can measure centrality in different ways, such as betweenness (the shortest distance between nodes) or eigenvector (way to measure influence in a network). Think about your friends. The person who seems to know everyone might not be the same as the person who can spread information effectively, or to far off parts of the network. Attributes - Information about your nodes. If your network contains people, attributes might be their age, birth dates, affiliation, occupation, etc. Bimodal or multi-modal - a network in which more than one type of thing is being connected. While a unimodal network might connect people to people, a bimodal network looks at authors and their books, or people to places. Scott Weingart has more to say about bimodal networks . Network data # To visualize your network, you need to set up your data is particular way. This might vary depending on the software you use, but it's a good place to start: An edge list is a two-column spreadsheet that forms the the network by listing the nodes and their connections. It's in a deceptively simple format: Source Target Kevin Bacon John Lithgow Kevin Bacon Sarah Jessica Parker Sarah Jessica Parker Matthew Broderick Matthew Broderick Jennifer Grey Jennifer Grey Patrick Swayze Patrick Swayze Demi Moore Demi Moore Kevin Bacon Jennifer Grey Laurence Fishburne Laurence Fishburne Keanu Reeves Kevin Bacon Laurence Fishburne You'll notice that some names can appear more than once, and not necessarily in the same column. Each of these rows represents a single connection, in this case, a movie. Take a minute to try to draw this extremely limited 80s-90s movie network on paper. What do you expect to see? Who has the most connections? Who seems central and who seems like an outlier? How flawed is this network? Add to it if you can. An edge list might build the network, but we need an attribute table to add context to our network. An attribute table lists each node only once, then displays information about that node in subsequent columns. Name Gender Is known for a dance scene Kevin Bacon M Y Matthew Broderick M Y Laurence Fishburne M N Jennifer Grey F Y John Lithgow M N Demi Moore F Y Sarah Jessica Parker F N Keanu Reeves M N Patrick Swawyze M Y Now I can use this information to filter or slice the view of my network. I can also use it to refine my questions or my data set. It's easy for me to see that I have more men in my network than women. If you want to see this data as a visualization, try loading it into Palladio . Copy and paste the following text into the white box in Palladio, then press Load : Source,Target, Kevin Bacon,John Lithgow, Kevin Bacon,Sarah Jessica Parker, Sarah Jessica Parker,Matthew Broderick, Matthew Broderick,Jennifer Grey, Jennifer Grey,Patrick Swayze, Patrick Swayze,Demi Moore, Demi Moore,Kevin Bacon, Jennifer Grey, Laurence Fishburne, Laurence Fishburne, Keanu Reeves, Kevin Bacon, Laurence Fishburne, To generate a network, visit the Graph tab. Use the menu on the right of the screen to select the \"source\" column in Source and the \"target\" column in Target . You should see a network appear! Does it look like what you expected? Tools # Palladio is a data visualization tool created by Stanford's Humanities + Design Research Lab. It's a browser-based tool that accepts structured data and creates network, geospatial, and gallery visualizations. It's easy to use, but can crash under too much data. It's a great way to create first draft visualizations of your data. Gephi is a robust network/graph visualization tool. It runs from your computer and can accept large data sets (though you will need some patience). Gephi has a lot of options for changing the appearance of your network via filters and layout. You can also generate statistics about graph density etc. There are a number of tutorials available. Cytoscape was designed for the sciences, but is now being used for more general network analysis and visualization purposes. Nodegoat is a data modeling and visualization platform with options for network analysis. igraph is a network analysis package for use with R, Python, and C++ UCINET is available on Windows only. It is not free software, but 90 day trials are available. Tutorials are available on the UCINET site. Hanneman and Riddle created this textbook for use with UCINET. Activities # Activity 5.1 # In order to practice using the terminology and methods of network analysis, let's design a network from scratch. I recommend using the whiteboard feature in Zoom, a Google doc/drawing, or a Box note for this exercise. In your group, select a topic for your network. It should be approachable for all members of your group. Game of Thrones? W&L students? Sports? A novel or TV show? List all the nodes in your network. Do they have types? Do they have attributes? Is this a bi-modal or multi-modal network? Start making connections or edges in your network. What type of edges do you need? Do the edges have a weight? Think about centrality. Do you have an ego network? How might you start calculating centrality? Activity 5.2 # Let's continue practicing our skills reviewing established projects. Instead of a blog post, be prepared to talk about this project with your classmates. Select one of the following projects to explore. Use the review criteria from Reviews in DH to analyze the project. Make some notes in if you need to. Kindred Britain Belfast Group Poetry Six Degrees of Francis Bacon Linked Jazz Viral Texts During class, we'll divide into groups to discuss the project you chose. What is this project about? What are the goals? Where/what is the data? How is this a humanities project? How effective are the network visualizations? Why do you say that? What about the design, layout, and organization of the project? What works? What doesn't? How does it contribute to your understanding of the project? Activity 5.3 # Let's practice putting together a network visualization with sample data. Download the results of our super duper quick survey. To create an edge list, open the results in Open Refine. Follow these instructions carefully! Remove the timestamp column. From the first column after the names, select Transpose > Transpose cells across columns into rows. Select the one column option and title your new column Value. Check the box for Fill down in other columns. Press Transpose. You should end up with two columns where each person's name is listed multiple times, corresponding with their answer to each survey question. Extract your new edge list as .csv to download back to your computer. Open Palladio and drag your new edge list into the white box. In Palladio, navigate to the Graph menu item. In the Settings box, you'll want to use the drop down menu to select a column from the data (name or target). You should now see a network viz! Test out the checkboxes to see how that changes your visualization. What do the Facets do at the bottom of the screen? How do you download this image? What are the limits of Palladio? Activity 5.4 # Now try putting together a network visualization from our trusty cemetery dataset. First, what are our questions? Who are the people in our data set? What networks should try to explore? Next, we need an edge list. We know that this is a two column spreadsheet, what belongs in each column? Go ahead and put some data together. This is a big data set, so we'll have to start somewhere. Upload your data into Palladio and create your network. What worked? What didn't? Resources # Creating a Network Graph with Gephi - Miriam Posner From Hermeneutics to Data to Networks: Data Extraction and Network Visualization of Historical Sources - Programming Historian tutorial Introduction to Network Analysis - Thomas Padilla and Brandon Locke Readings # An Entry of One\u2019s Own, or Why Are There So Few Women In the Early Modern Social Network? Demystifying Networks Network Analysis Fundamentals","title":"Network Analysis"},{"location":"network-analysis/#what-is-network-analysis","text":"Networks are everywhere. Once you start, it's hard to stop seeing them. The internet is a network, Facebook is a network, Kevin Bacon is the central node in the Hollywood network. Studying the connections between people and things is a major activity in the humanities, so \"network analysis\" feels like a natural method for working with humanities data. Network analysis varies by discipline, but originates from graph theory in mathematics. This is a simplification, but the idea is that one can measure significant people in a network by how many connections they have to other people and by the weight or significance of those connections. You can also learn who might serve as a bridge between social groups or whether you have a dense network of highly connected people or a more disperse group. We can visualize these networks through a network graph - points connected by lines. You don't necessarily have to just study people with network analysis, but it's a common approach. The specific name for this methodology is SNA or Social Network Analysis. SNA is used in many disciplines, with a heavy presence in the social sciences. It's important to recognize that each discipline will have its own conventions and expectations for SNA. You should tread carefully until you have a firm foundation. As Scott Weingart reminds us in \" Networks Demystified :\" \"Networks can be used on any project. Networks should be used on far fewer.\" This is not to scare you! But learning about network analysis is the perfect moment to pause and remember that you are not working in a vacuum. The methods you use have a historical and modern context and you are responsible for learning that context. That being said, network visualizations can be a great place to start. They can help you learn more about your data or illuminate a new direction for your research agenda. Plus, they're fun. Learning the basics will help you be better judge of whether it is, in fact, a good fit for your project.","title":"What is network analysis?"},{"location":"network-analysis/#nodes-edges-ties-what","text":"Let's take a moment to define some of the terms you'll encounter in network analysis. Nodes/points/agents/vertices - All of these words refer to the points on your network graph. If you're doing social network analysis, the points are the people in your graph. Edges/ties - Refers to the connections between nodes, usually represented by lines between points. Edges can be weighted (stronger or weaker edges) or directional (some relationships are mutual, some are not). The line could be drawn thin or thick based on the weight, or with an arrow to signify direction. The connection between you and your roommate might have a strong weight for example. If you follow a celebrity on Twitter, but they don't follow you back, we can imagine that as a relationship with a single direction. Centrality - the more connections a node has, the more central they are to the network. You can measure centrality in different ways, such as betweenness (the shortest distance between nodes) or eigenvector (way to measure influence in a network). Think about your friends. The person who seems to know everyone might not be the same as the person who can spread information effectively, or to far off parts of the network. Attributes - Information about your nodes. If your network contains people, attributes might be their age, birth dates, affiliation, occupation, etc. Bimodal or multi-modal - a network in which more than one type of thing is being connected. While a unimodal network might connect people to people, a bimodal network looks at authors and their books, or people to places. Scott Weingart has more to say about bimodal networks .","title":"Nodes, edges, ties, what?"},{"location":"network-analysis/#network-data","text":"To visualize your network, you need to set up your data is particular way. This might vary depending on the software you use, but it's a good place to start: An edge list is a two-column spreadsheet that forms the the network by listing the nodes and their connections. It's in a deceptively simple format: Source Target Kevin Bacon John Lithgow Kevin Bacon Sarah Jessica Parker Sarah Jessica Parker Matthew Broderick Matthew Broderick Jennifer Grey Jennifer Grey Patrick Swayze Patrick Swayze Demi Moore Demi Moore Kevin Bacon Jennifer Grey Laurence Fishburne Laurence Fishburne Keanu Reeves Kevin Bacon Laurence Fishburne You'll notice that some names can appear more than once, and not necessarily in the same column. Each of these rows represents a single connection, in this case, a movie. Take a minute to try to draw this extremely limited 80s-90s movie network on paper. What do you expect to see? Who has the most connections? Who seems central and who seems like an outlier? How flawed is this network? Add to it if you can. An edge list might build the network, but we need an attribute table to add context to our network. An attribute table lists each node only once, then displays information about that node in subsequent columns. Name Gender Is known for a dance scene Kevin Bacon M Y Matthew Broderick M Y Laurence Fishburne M N Jennifer Grey F Y John Lithgow M N Demi Moore F Y Sarah Jessica Parker F N Keanu Reeves M N Patrick Swawyze M Y Now I can use this information to filter or slice the view of my network. I can also use it to refine my questions or my data set. It's easy for me to see that I have more men in my network than women. If you want to see this data as a visualization, try loading it into Palladio . Copy and paste the following text into the white box in Palladio, then press Load : Source,Target, Kevin Bacon,John Lithgow, Kevin Bacon,Sarah Jessica Parker, Sarah Jessica Parker,Matthew Broderick, Matthew Broderick,Jennifer Grey, Jennifer Grey,Patrick Swayze, Patrick Swayze,Demi Moore, Demi Moore,Kevin Bacon, Jennifer Grey, Laurence Fishburne, Laurence Fishburne, Keanu Reeves, Kevin Bacon, Laurence Fishburne, To generate a network, visit the Graph tab. Use the menu on the right of the screen to select the \"source\" column in Source and the \"target\" column in Target . You should see a network appear! Does it look like what you expected?","title":"Network data"},{"location":"network-analysis/#tools","text":"Palladio is a data visualization tool created by Stanford's Humanities + Design Research Lab. It's a browser-based tool that accepts structured data and creates network, geospatial, and gallery visualizations. It's easy to use, but can crash under too much data. It's a great way to create first draft visualizations of your data. Gephi is a robust network/graph visualization tool. It runs from your computer and can accept large data sets (though you will need some patience). Gephi has a lot of options for changing the appearance of your network via filters and layout. You can also generate statistics about graph density etc. There are a number of tutorials available. Cytoscape was designed for the sciences, but is now being used for more general network analysis and visualization purposes. Nodegoat is a data modeling and visualization platform with options for network analysis. igraph is a network analysis package for use with R, Python, and C++ UCINET is available on Windows only. It is not free software, but 90 day trials are available. Tutorials are available on the UCINET site. Hanneman and Riddle created this textbook for use with UCINET.","title":"Tools"},{"location":"network-analysis/#activities","text":"","title":"Activities"},{"location":"network-analysis/#activity-51","text":"In order to practice using the terminology and methods of network analysis, let's design a network from scratch. I recommend using the whiteboard feature in Zoom, a Google doc/drawing, or a Box note for this exercise. In your group, select a topic for your network. It should be approachable for all members of your group. Game of Thrones? W&L students? Sports? A novel or TV show? List all the nodes in your network. Do they have types? Do they have attributes? Is this a bi-modal or multi-modal network? Start making connections or edges in your network. What type of edges do you need? Do the edges have a weight? Think about centrality. Do you have an ego network? How might you start calculating centrality?","title":"Activity 5.1"},{"location":"network-analysis/#activity-52","text":"Let's continue practicing our skills reviewing established projects. Instead of a blog post, be prepared to talk about this project with your classmates. Select one of the following projects to explore. Use the review criteria from Reviews in DH to analyze the project. Make some notes in if you need to. Kindred Britain Belfast Group Poetry Six Degrees of Francis Bacon Linked Jazz Viral Texts During class, we'll divide into groups to discuss the project you chose. What is this project about? What are the goals? Where/what is the data? How is this a humanities project? How effective are the network visualizations? Why do you say that? What about the design, layout, and organization of the project? What works? What doesn't? How does it contribute to your understanding of the project?","title":"Activity 5.2"},{"location":"network-analysis/#activity-53","text":"Let's practice putting together a network visualization with sample data. Download the results of our super duper quick survey. To create an edge list, open the results in Open Refine. Follow these instructions carefully! Remove the timestamp column. From the first column after the names, select Transpose > Transpose cells across columns into rows. Select the one column option and title your new column Value. Check the box for Fill down in other columns. Press Transpose. You should end up with two columns where each person's name is listed multiple times, corresponding with their answer to each survey question. Extract your new edge list as .csv to download back to your computer. Open Palladio and drag your new edge list into the white box. In Palladio, navigate to the Graph menu item. In the Settings box, you'll want to use the drop down menu to select a column from the data (name or target). You should now see a network viz! Test out the checkboxes to see how that changes your visualization. What do the Facets do at the bottom of the screen? How do you download this image? What are the limits of Palladio?","title":"Activity 5.3"},{"location":"network-analysis/#activity-54","text":"Now try putting together a network visualization from our trusty cemetery dataset. First, what are our questions? Who are the people in our data set? What networks should try to explore? Next, we need an edge list. We know that this is a two column spreadsheet, what belongs in each column? Go ahead and put some data together. This is a big data set, so we'll have to start somewhere. Upload your data into Palladio and create your network. What worked? What didn't?","title":"Activity 5.4"},{"location":"network-analysis/#resources","text":"Creating a Network Graph with Gephi - Miriam Posner From Hermeneutics to Data to Networks: Data Extraction and Network Visualization of Historical Sources - Programming Historian tutorial Introduction to Network Analysis - Thomas Padilla and Brandon Locke","title":"Resources"},{"location":"network-analysis/#readings","text":"An Entry of One\u2019s Own, or Why Are There So Few Women In the Early Modern Social Network? Demystifying Networks Network Analysis Fundamentals","title":"Readings"},{"location":"process/","text":"In this section, we'll walk through the process of putting together a project with humanities data. Picking a topic Research! What's my project? Research Questions and Data Modeling Creating data Finding data Analyzing your data Case study: Spanish Gallery Resources Picking a topic # First things first, what do you even want to do? Big question, right? Don't worry, it's normal to feel lost at the start. You're probably at the beginning of your academic career and haven't gone through the years of training like most scholars have. You might not feel committed to one discipline or subject area yet. If that's the case, this is your chance to experiment! You can try on a topic like you try on a pair of shoes, spending a little time in them to see if they're still comfortable after walking a mile to the grocery store. For most of you, this is one assignment for one class, not something that has to follow you the rest of your life, unless you want it to. Embrace the ability to travel in your own direction. Some of you may be \"idea people\" and feel like you're brimming with potential topics already. If so, that's great! There will be a section about \"scope\" for you later on. If you need some help generating ideas, it can be helpful to consider these categories: Objects - What thing do you want to learn more about? Is there a book, a moment in history, or an archival collection that has caught your interest? Are you interested in fanfiction about anime, 19th century detective novels, the 1918 pandemic, or perhaps some beautiful illustrated maps you saw in a museum exhibit? Some people are drawn to objects and collections, and work from there. There are lots of ways to source or generate data about objects. You might want to spend your time crafting a beautiful, if small, data set about a set of objects. Questions - What questions keep you up at night? What do you ponder while you run or while you're driving? What have you wondered about while reading for your philosophy class? You might be more of an abstract thinker. How can you approach these questions in a data-driven way? You might be more interested in big approaches to data - what can learn from hundreds or thousands of texts? What patterns can you find through time and space? You should consider working with existing data sets and playing with the kinds of questions and answers you can find. Skills - What do you want to get better at? Some folks are more motivated by improving their own abilities, rather than doing a deep dive into a topic. That's okay too. What skills do you want to work on? How might they transfer to another project or method? Do you want to get really good at data visualization, or maybe play with a new programming language? Or perhaps build confidence with web scraping? Go for it! You can let the software drive your decision making. We need the builders, just as much as we need the analyzers. Research! # Once you have an inkling of what your topic might be, it's time to get researching. You want to get a sense of what is out there already about this topic. You want to find resources that might help with the context, but you also want to see what kind of work has already been done. Save yourself the frustration of getting halfway through a data collection project before you realize that someone has already done this work before. Getting a sense of existing approaches might also help steer you toward your approach. What do similar projects lack? How can you fill in the gaps? Some of you may already be flying through the internet, opening up tab after tab, making notes. But you might also find this part of the process to be paralyzing. It's a wide open world out there. Here are some tips to help you structure this important piece: Structure your research process into phases. Gathering, analyzing, refining, etc. Give yourself time to explore, without deciding whether or not an article, a tutorial, a website is useful or not. You're just trying familiarize yourself with what is out there. You can use a document or something like ZOtero to help. Once you've done your gathering, then spend some time actually reading and considering what you've found. It is useful? Does it help you think about your topic in a new way? Did the title seem great but the content was lacking? Does an article give you a new path to explore? Try to sit with what you've found and think about it before moving on. Once you have a solid sense of what is out there about your topic, it's time to refine and narrow your scope. This stage has its own challenges. It can be hard to make a decision at this stage. You feel pressure to make the right one. Don't worry, you can still pivot if you need to, but remember that there are benefits to making decisions and moving ahead now. You're probably on a deadline and need to get moving onto the next phase of the project. Consider creating a mental or physical \"parking lot\" for ideas or resources that you are intrigued by, but don't have time for right now. You may notice that these tips are conflating the idea of your topic and the research about your topic. That's because at this stage, things are still rather fluid. You may discover that the object, question, or skill that was going to be your topic is just not tenable at this time. Or you may be researching the historical background for one thing and come across an amazing data set for another thing that changes your direction. Enough pep talk! Give me some real tips: Start at the library. Whether it's your library's website or the physical location, browse the resources that your institution has provided for you. Talk to a librarian you know, they love to help. Start local. Every town has its stories. Archives, museums, historical societies, and special collections can hold original or unique materials about the people and places nearby. In this coursebook, we've worked with data about the inhabitants of the local cemetery and the school newspaper. These local stories may not have been analyzed or shared before, but they still have relevance to larger issues. Not all small institutions have the resources to make their holdings available online, so you may have to talk to someone or get creative. Consider the discipline. Don't forget that scholarly work segments itself by discipline, so you may have to navigate those boundaries. Is your topic fundamentally about literature? About history? About philosophy? What other disciplines does it touch? Check out the scholarly organization for that discipline, its major journals, or research centers. What's my project? # Hopefully, if you've read other sections of this coursebook, you have an idea of what humanities data could look like. But what will your data look like? How do you take a topic idea and turn that into data? During the research process, identify and inventory any existing data sets related to your topic. What's there? What's missing? How might you fill in a gap or take your research in a new direction? Start asking questions. What do you want to know? What can you learn from this object/corpus? You don't have to have a firm research question yet, but you should be wondering things. What kind of approach do you want to take? How might that dictate the form of your data? Do you need a list of people and their attributes for network analysis? Do you want to count things and visualize patterns? Do you need place names and coordinates? Consider your limitations. What is your timeframe? What is your skill level? What data is available already and what must you create yourself? Are you working alone or in a team? Do you have access to everything you need? Are you prepared to do a lot of data entry, or do you know that tedious work is not your thing? Set your scope. Then make your scope a little smaller. Trust me. You can always add more later, but when you're new to this kind of work, it's best to start small. If you want to map something, do you need to map the whole country? WOuld it be better to do a region, a state, or even a county? If you're interested in text mining 19th century literature, perhaps a single genre, author, or timespan could serve as a boundary to that work? At this point, you should be at a place where you can write a proposal for your project. You know the types of things you want to do, the questions you want to explore, even if you don't have the answers yet. If you're still stuck, try this flow chart from The Pudding on Writing a Data-Driven Story Research Questions and Data Modeling # In the data section , we covered the mechanics of data modeling. Here, let's cover it as part of the research and project process. Chances are, data modeling will go hand in hand with determining your specific research questions. You will need to iterate over this process a few times before you've arrived at something feasible. What is one potential research question that you can think of? Pay attention to how you begin your question. Are you asking how many? Who? What? How would you answer that question with some kind of data? It can be an unproven, hypothetical answer, but you should think through what kind of answers you're looking for. Now swim around in your answer a little bit. What is your answer made of? Do you need to count something to get to that answer? What do you need to count? Are you tracking something over time, and therefore need time-based data? Are you looking at relationships, so you need information about people? Make a list of all the pieces of information you might need to answer your question. This could be the beginning of your data model. Go through these steps again with a different question and potential answer. How do your data needs compare? Can you start to see a spreadsheet or a corpus forming? Do you need to do more research? How did that go? Hopefully by working through those questions, you should have an idea of where you're headed. Again, it's totally normal to adjust or go through this cycle again. You may even get to the point of visualization and realize you need to regroup. For now, let's assume you're ready to start putting together your data. You have some options. Creating data # You may determine that you need to create your data set from scratch. What does \"from scratch\" mean? Are eggs involved? Chances are you are not going to be pulling data magically from your brain. It's more likely that you will have a source or multiple sources that need to be transformed in a structured data set that can be analyzed by a piece of software. The draft data model you created will guide you in this process. Many humanities data projects begin with the kind of old books that seem to be forgotten about on library shelves. Scholars of past centuries did a lot of data-driven work, they just put all their data into a print book, and now that information needs to be transformed. For example, The Ancient Graffiti Project relies on a 19th century collection of inscriptions gathered into set of large, heavy volumes known as Corpus Inscriptionum Latinarum . Mapping the Scottish Reformation is a prosopography project that gathers data about Scottish ministers from a text known as the Fasti . While the author of the Fasti , Hew Scott, did tremendous work in compiling this text, he was loose with the facts and didn't cite his sources. In the end, both of these projects will provide a database and visualizations for users to ask their own research questions. But the process of getting there involves careful data entry and cross-referencing. The project team is not necessarily transcribing texts from beginning to end, they are extracting each data point in a way that fits their data model. They might not extract every piece of information, instead, they're letting their project goals guide them. Rest assured, both of these projects added to and refined their data model over time. If you have generated some research questions and drafted a data model, go ahead and open up a new spreadsheet. Label some fields, start filling in data, then step back and review your work. Does your source include information you forgot to put in your data model? Do these fields make sense to another person? Do you need another spreadsheet? On the flip side, are you trying to gather too much data? Do you need every single detail? If you're not sure how to answer these questions, why not skip ahead to visualization or analysis? Test out the data you've gathered so far, even if it's not complete, just to see how it performs. Again, this iterative approach is key to creating a workable project. Finding data # You may determine that your project or your constraints require that you find existing data sources. That's great! You're reusing existing information in new ways! But it can be a struggle to find the exact data set you're looking for. There is not one single catalog of data that you can use to find what you're looking for. You may have to combine pieces of multiple data sets. Alternatively, you may not have access or be legally allowed to use the data you're seeking. It's another reason to identify other scholars working on similar topic, you may be able to ask advice or find guidance. Analyzing your data # Okay, let's say you have put together a respectable data set that you think addresses your goals. What next? Throughout this coursebook, we have explored a range of analysis methods. Your research questions should help you determine the method or methods that make sense. But once you start using a method or visualizing your data, how do you know when you're getting answers? Here are some things to keep in mind: Know your material. It's up to you and your understanding of your subject to determine what matters. If you've generated a network graph, but you don't know much about the major players, you're going to struggle. Let your visualizations inspire you to dig back into research topics. Set benchmarks. Running a process once or creating one visualization isn't enough. Perform your analysis methods over and over so you have a sense of what is normal vs. extreme. You might also find errors in your data this way. If you're interested in how often the word \"love\" is used over time, you might want to try synonyms of that word. Do all words behave the same way? What are the trends over time? Is that actually a unique line on your graph? Remember your discipline. Positioning your work within a discipline is not an arbitrary decision. It gives you standards and priorities for your work. A literary scholar is going to approach a text differently than a historian. They're going to care about different questions. If you're a historian, you might be intrigued by the use of literary symbolism in a text, but know that you have to focus on the historical elements, not the style. Read the literature of the discipline and review other projects to get a sense of how their analysis works. Case study: Spanish Gallery # To pull this all together, let's walk through a case study of a project from a former student, Alice. Alice is an art history major with some experience in digital methods from other classes and her work on a DH project. In one of her classes, she learns about the Spanish Gallery . The Spanish Gallery was assembled at the Louvre by King Louis Phillipe in 1838 to showcase Spanish art during a time of French occupation of Spain. After his death, the gallery was sent to auction in London and dispersed into private hands. Alice wonders what happened to these paintings after the auction, as well as their origins in Spain. She imagines what the gallery might have looked like all assembled, similar to a project she put together on Boydell's Shakespeare Gallery . In reading the Wikipedia article, she notices a link to the digital copy of the gallery catalog from 1838. She recognizes these catalog as a potential source of data, especially when combined with her knowledge of searching auction records. The catalog is arranged in alphabetical order by artist, with a brief bio, then a list of the paintings and their dimensions. Alice sees that the digital book is available to download as plain text, which will make it easier to work with. She also notices that the text isn't quite perfect, so she will need to budget time for correction. Not to mention it's all in French! Finally, there appear to be annotations in pencil on the pages, so that may be something to look into. Alice opens Google Sheets and gets to work on this spreadsheet . The catalog has assigned each painting a number, so she uses that as an ID field. The thing that is being described in each row is the painting, with the artist, bio, size, location modifying that painting. After starting that work, she realizes that it's repetitive to include the art information multiple times, so she creates a second sheet that is just a list of the artists and their information. She cleverly uses a built-in translation feature in Google sheets to translate the French to English for the whole column, rather than doing it one by one: =GOOGLETRANSLATE(B2, \"fr\", \"en\") . Alice can't help but start researching the modern locations of these pieces of art. She finds that this work is going to be more challenging than she anticipated, so she creates another sheet to separate this work into a second phase of the project. There just aren't enough accessible records for each piece of work. Phase 1 will have to be about creating data on the gallery itself, Phase 2 will concern the auction and dispersal of the art. Similarly, finding the original locations of the art may be challenging too, since the catalog only lists where the artist was from. She knows another student who has used Google Street View and tourist websites to confirm the location of shrines in Spain. She supposes that's Phase 3! Alice's project is still in progress, but hopefully this example gave you an idea of how a project like this can change as it progresses. Resources #","title":"Process"},{"location":"process/#picking-a-topic","text":"First things first, what do you even want to do? Big question, right? Don't worry, it's normal to feel lost at the start. You're probably at the beginning of your academic career and haven't gone through the years of training like most scholars have. You might not feel committed to one discipline or subject area yet. If that's the case, this is your chance to experiment! You can try on a topic like you try on a pair of shoes, spending a little time in them to see if they're still comfortable after walking a mile to the grocery store. For most of you, this is one assignment for one class, not something that has to follow you the rest of your life, unless you want it to. Embrace the ability to travel in your own direction. Some of you may be \"idea people\" and feel like you're brimming with potential topics already. If so, that's great! There will be a section about \"scope\" for you later on. If you need some help generating ideas, it can be helpful to consider these categories: Objects - What thing do you want to learn more about? Is there a book, a moment in history, or an archival collection that has caught your interest? Are you interested in fanfiction about anime, 19th century detective novels, the 1918 pandemic, or perhaps some beautiful illustrated maps you saw in a museum exhibit? Some people are drawn to objects and collections, and work from there. There are lots of ways to source or generate data about objects. You might want to spend your time crafting a beautiful, if small, data set about a set of objects. Questions - What questions keep you up at night? What do you ponder while you run or while you're driving? What have you wondered about while reading for your philosophy class? You might be more of an abstract thinker. How can you approach these questions in a data-driven way? You might be more interested in big approaches to data - what can learn from hundreds or thousands of texts? What patterns can you find through time and space? You should consider working with existing data sets and playing with the kinds of questions and answers you can find. Skills - What do you want to get better at? Some folks are more motivated by improving their own abilities, rather than doing a deep dive into a topic. That's okay too. What skills do you want to work on? How might they transfer to another project or method? Do you want to get really good at data visualization, or maybe play with a new programming language? Or perhaps build confidence with web scraping? Go for it! You can let the software drive your decision making. We need the builders, just as much as we need the analyzers.","title":"Picking a topic"},{"location":"process/#research","text":"Once you have an inkling of what your topic might be, it's time to get researching. You want to get a sense of what is out there already about this topic. You want to find resources that might help with the context, but you also want to see what kind of work has already been done. Save yourself the frustration of getting halfway through a data collection project before you realize that someone has already done this work before. Getting a sense of existing approaches might also help steer you toward your approach. What do similar projects lack? How can you fill in the gaps? Some of you may already be flying through the internet, opening up tab after tab, making notes. But you might also find this part of the process to be paralyzing. It's a wide open world out there. Here are some tips to help you structure this important piece: Structure your research process into phases. Gathering, analyzing, refining, etc. Give yourself time to explore, without deciding whether or not an article, a tutorial, a website is useful or not. You're just trying familiarize yourself with what is out there. You can use a document or something like ZOtero to help. Once you've done your gathering, then spend some time actually reading and considering what you've found. It is useful? Does it help you think about your topic in a new way? Did the title seem great but the content was lacking? Does an article give you a new path to explore? Try to sit with what you've found and think about it before moving on. Once you have a solid sense of what is out there about your topic, it's time to refine and narrow your scope. This stage has its own challenges. It can be hard to make a decision at this stage. You feel pressure to make the right one. Don't worry, you can still pivot if you need to, but remember that there are benefits to making decisions and moving ahead now. You're probably on a deadline and need to get moving onto the next phase of the project. Consider creating a mental or physical \"parking lot\" for ideas or resources that you are intrigued by, but don't have time for right now. You may notice that these tips are conflating the idea of your topic and the research about your topic. That's because at this stage, things are still rather fluid. You may discover that the object, question, or skill that was going to be your topic is just not tenable at this time. Or you may be researching the historical background for one thing and come across an amazing data set for another thing that changes your direction. Enough pep talk! Give me some real tips: Start at the library. Whether it's your library's website or the physical location, browse the resources that your institution has provided for you. Talk to a librarian you know, they love to help. Start local. Every town has its stories. Archives, museums, historical societies, and special collections can hold original or unique materials about the people and places nearby. In this coursebook, we've worked with data about the inhabitants of the local cemetery and the school newspaper. These local stories may not have been analyzed or shared before, but they still have relevance to larger issues. Not all small institutions have the resources to make their holdings available online, so you may have to talk to someone or get creative. Consider the discipline. Don't forget that scholarly work segments itself by discipline, so you may have to navigate those boundaries. Is your topic fundamentally about literature? About history? About philosophy? What other disciplines does it touch? Check out the scholarly organization for that discipline, its major journals, or research centers.","title":"Research!"},{"location":"process/#whats-my-project","text":"Hopefully, if you've read other sections of this coursebook, you have an idea of what humanities data could look like. But what will your data look like? How do you take a topic idea and turn that into data? During the research process, identify and inventory any existing data sets related to your topic. What's there? What's missing? How might you fill in a gap or take your research in a new direction? Start asking questions. What do you want to know? What can you learn from this object/corpus? You don't have to have a firm research question yet, but you should be wondering things. What kind of approach do you want to take? How might that dictate the form of your data? Do you need a list of people and their attributes for network analysis? Do you want to count things and visualize patterns? Do you need place names and coordinates? Consider your limitations. What is your timeframe? What is your skill level? What data is available already and what must you create yourself? Are you working alone or in a team? Do you have access to everything you need? Are you prepared to do a lot of data entry, or do you know that tedious work is not your thing? Set your scope. Then make your scope a little smaller. Trust me. You can always add more later, but when you're new to this kind of work, it's best to start small. If you want to map something, do you need to map the whole country? WOuld it be better to do a region, a state, or even a county? If you're interested in text mining 19th century literature, perhaps a single genre, author, or timespan could serve as a boundary to that work? At this point, you should be at a place where you can write a proposal for your project. You know the types of things you want to do, the questions you want to explore, even if you don't have the answers yet. If you're still stuck, try this flow chart from The Pudding on Writing a Data-Driven Story","title":"What's my project?"},{"location":"process/#research-questions-and-data-modeling","text":"In the data section , we covered the mechanics of data modeling. Here, let's cover it as part of the research and project process. Chances are, data modeling will go hand in hand with determining your specific research questions. You will need to iterate over this process a few times before you've arrived at something feasible. What is one potential research question that you can think of? Pay attention to how you begin your question. Are you asking how many? Who? What? How would you answer that question with some kind of data? It can be an unproven, hypothetical answer, but you should think through what kind of answers you're looking for. Now swim around in your answer a little bit. What is your answer made of? Do you need to count something to get to that answer? What do you need to count? Are you tracking something over time, and therefore need time-based data? Are you looking at relationships, so you need information about people? Make a list of all the pieces of information you might need to answer your question. This could be the beginning of your data model. Go through these steps again with a different question and potential answer. How do your data needs compare? Can you start to see a spreadsheet or a corpus forming? Do you need to do more research? How did that go? Hopefully by working through those questions, you should have an idea of where you're headed. Again, it's totally normal to adjust or go through this cycle again. You may even get to the point of visualization and realize you need to regroup. For now, let's assume you're ready to start putting together your data. You have some options.","title":"Research Questions and Data Modeling"},{"location":"process/#creating-data","text":"You may determine that you need to create your data set from scratch. What does \"from scratch\" mean? Are eggs involved? Chances are you are not going to be pulling data magically from your brain. It's more likely that you will have a source or multiple sources that need to be transformed in a structured data set that can be analyzed by a piece of software. The draft data model you created will guide you in this process. Many humanities data projects begin with the kind of old books that seem to be forgotten about on library shelves. Scholars of past centuries did a lot of data-driven work, they just put all their data into a print book, and now that information needs to be transformed. For example, The Ancient Graffiti Project relies on a 19th century collection of inscriptions gathered into set of large, heavy volumes known as Corpus Inscriptionum Latinarum . Mapping the Scottish Reformation is a prosopography project that gathers data about Scottish ministers from a text known as the Fasti . While the author of the Fasti , Hew Scott, did tremendous work in compiling this text, he was loose with the facts and didn't cite his sources. In the end, both of these projects will provide a database and visualizations for users to ask their own research questions. But the process of getting there involves careful data entry and cross-referencing. The project team is not necessarily transcribing texts from beginning to end, they are extracting each data point in a way that fits their data model. They might not extract every piece of information, instead, they're letting their project goals guide them. Rest assured, both of these projects added to and refined their data model over time. If you have generated some research questions and drafted a data model, go ahead and open up a new spreadsheet. Label some fields, start filling in data, then step back and review your work. Does your source include information you forgot to put in your data model? Do these fields make sense to another person? Do you need another spreadsheet? On the flip side, are you trying to gather too much data? Do you need every single detail? If you're not sure how to answer these questions, why not skip ahead to visualization or analysis? Test out the data you've gathered so far, even if it's not complete, just to see how it performs. Again, this iterative approach is key to creating a workable project.","title":"Creating data"},{"location":"process/#finding-data","text":"You may determine that your project or your constraints require that you find existing data sources. That's great! You're reusing existing information in new ways! But it can be a struggle to find the exact data set you're looking for. There is not one single catalog of data that you can use to find what you're looking for. You may have to combine pieces of multiple data sets. Alternatively, you may not have access or be legally allowed to use the data you're seeking. It's another reason to identify other scholars working on similar topic, you may be able to ask advice or find guidance.","title":"Finding data"},{"location":"process/#analyzing-your-data","text":"Okay, let's say you have put together a respectable data set that you think addresses your goals. What next? Throughout this coursebook, we have explored a range of analysis methods. Your research questions should help you determine the method or methods that make sense. But once you start using a method or visualizing your data, how do you know when you're getting answers? Here are some things to keep in mind: Know your material. It's up to you and your understanding of your subject to determine what matters. If you've generated a network graph, but you don't know much about the major players, you're going to struggle. Let your visualizations inspire you to dig back into research topics. Set benchmarks. Running a process once or creating one visualization isn't enough. Perform your analysis methods over and over so you have a sense of what is normal vs. extreme. You might also find errors in your data this way. If you're interested in how often the word \"love\" is used over time, you might want to try synonyms of that word. Do all words behave the same way? What are the trends over time? Is that actually a unique line on your graph? Remember your discipline. Positioning your work within a discipline is not an arbitrary decision. It gives you standards and priorities for your work. A literary scholar is going to approach a text differently than a historian. They're going to care about different questions. If you're a historian, you might be intrigued by the use of literary symbolism in a text, but know that you have to focus on the historical elements, not the style. Read the literature of the discipline and review other projects to get a sense of how their analysis works.","title":"Analyzing your data"},{"location":"process/#case-study-spanish-gallery","text":"To pull this all together, let's walk through a case study of a project from a former student, Alice. Alice is an art history major with some experience in digital methods from other classes and her work on a DH project. In one of her classes, she learns about the Spanish Gallery . The Spanish Gallery was assembled at the Louvre by King Louis Phillipe in 1838 to showcase Spanish art during a time of French occupation of Spain. After his death, the gallery was sent to auction in London and dispersed into private hands. Alice wonders what happened to these paintings after the auction, as well as their origins in Spain. She imagines what the gallery might have looked like all assembled, similar to a project she put together on Boydell's Shakespeare Gallery . In reading the Wikipedia article, she notices a link to the digital copy of the gallery catalog from 1838. She recognizes these catalog as a potential source of data, especially when combined with her knowledge of searching auction records. The catalog is arranged in alphabetical order by artist, with a brief bio, then a list of the paintings and their dimensions. Alice sees that the digital book is available to download as plain text, which will make it easier to work with. She also notices that the text isn't quite perfect, so she will need to budget time for correction. Not to mention it's all in French! Finally, there appear to be annotations in pencil on the pages, so that may be something to look into. Alice opens Google Sheets and gets to work on this spreadsheet . The catalog has assigned each painting a number, so she uses that as an ID field. The thing that is being described in each row is the painting, with the artist, bio, size, location modifying that painting. After starting that work, she realizes that it's repetitive to include the art information multiple times, so she creates a second sheet that is just a list of the artists and their information. She cleverly uses a built-in translation feature in Google sheets to translate the French to English for the whole column, rather than doing it one by one: =GOOGLETRANSLATE(B2, \"fr\", \"en\") . Alice can't help but start researching the modern locations of these pieces of art. She finds that this work is going to be more challenging than she anticipated, so she creates another sheet to separate this work into a second phase of the project. There just aren't enough accessible records for each piece of work. Phase 1 will have to be about creating data on the gallery itself, Phase 2 will concern the auction and dispersal of the art. Similarly, finding the original locations of the art may be challenging too, since the catalog only lists where the artist was from. She knows another student who has used Google Street View and tourist websites to confirm the location of shrines in Spain. She supposes that's Phase 3! Alice's project is still in progress, but hopefully this example gave you an idea of how a project like this can change as it progresses.","title":"Case study: Spanish Gallery"},{"location":"process/#resources","text":"","title":"Resources"},{"location":"schedule/","text":"Jump to: Week 1 | Week 2 | Week 3 | Week 4 | Week 5 | Week 6 | Week 7 | Week 8 | Week 9 | Week 10 | Week 11 | Week 12 This class will meet on Zoom during its normally scheduled time: TR 4:30-6:00pm. Week 1 - How the Web Works # Tuesday, August 25, 2020 # Getting to know you survey Read What is Humanities Data by class time. Class agenda: introductions + icebreaker: earliest memory of the internet? (4:30-4:45) course overview (4:45-5:05) quick break (5:05-5:10) breakout rooms: course charter (5:10-5:30) review charter together (5:30-5:45) go over activities for Thursday (5:45-6pm) For Thursday Complete Activity 1.1 . Nothing to turn in, just spent some time exploring and thinking. Download Sublime Text Editor . Read How the Web Works by class time. We will do the activities in class, but read through them so you're ready. Read Digital_Humanities , chapter 1 Thursday, August 27, 2020 # How the Web Works! Class agenda: icebreaker: what do you wish you'd known about W&L as a first year? (4:30-4:40) activity 1.1 sharing + discussion (4:40-4:50) activities 1.2 + 1.3 walk through (4:50-5:00) activities 1.2 + 1.3 solo work time + break (5:00-5:15) activities 1.4-1.6 walk through (5:15-5:30) activities 1.4-1.6 work time (5:30-6:00) Activity log week 1 # Due Monday 8/31 at 9am Wow because of date typo, you have until Tuesday, 9/1 at 9am to submit. Complete Activities 1.2, 1.3, 1.4 . In Canvas, submit the URL webpage that you have coded and uploaded to your domain. Complete Activities 1.5-1.6 . In Canvas, submit the URL to your Wordpress site. There should be evidence that you have customized your Wordpress beyond the basic settings. Blog post #1. Respond to the following prompt in a 300-500 word blog post on your Wordpress. In Canvas, submit the URL the blog post. Prompt: What brought you to this class? What is your current relationship with technology? Where do you see data in your coursework? Where do you see data in your daily life? What humanities subjects are particularly interesting to you? What are your own goals for this course? What do you want to accomplish/learn/achieve? Week 2 - Your Computer # Tuesday, September 1, 2020 # Read Your Computer by class time. Class agenda: icebreaker: It's September today. What will you miss the most about summer? breakout groups: review websites in small groups (4:45-5:00) First, share with your partner one realization you had while learning HTML/CSS. It can be technical like \"don't forget that slash\" or conceptual \"I realized I need to read directions more closely.\" Second, visit each other's hand-coded website (not Wordpress). Share the link in chat. Right click/Ctrl + Click to view page source. Proof your partner's code. Check their syntax. Did they miss any slashes? Is everything in the right place? Work together to add the following to each of your websites: an iframe tag which will embed another website onto your own. two div tags with different background colors (hint, you'll need to use a class or id) Be sure to upload your changes to your website. break (5:00-5:05) start Activity 2.1 together (5:05-5:45) talk about Week 2 blog post For Thursday: Finish Activity 2.1 and 2.2 by class time. Thursday, September 3, 2020 # Class agenda: icebreaker: what's your favorite comfort tv show? (4:30-4:40) Introduce Final Project (4:40-5:00) Start Activity 2.3 together in breakout rooms (5:00-5:45) Questions??? (5:45-6:00) Activity log week 2 # Due Monday 9/7 at 9am Complete Activity 2.3 . In Canvas, submit a document with your answers. It is more important that you show your process, thinking, and attempts than it is that you get everything right. Blog post #2: Respond to the following prompt in a 500 word blog post on your Wordpress. In Canvas, submit the URL the blog post. Prompt: To help us conceptualize humanities data projects, let's look at a few more projects and the accompanying reviews. Scholarly articles and books are typically reviewed by other experts in the field. There are established procedures and venues for this kind of work, however DH projects do not always fit into those procedures. Reviews in DH is a new publication that \"bridges the gap\" by providing reviews of projects. For this blog post, you will explore and write about a project before reading the accompanying review, then read the review and see how your analysis lines up. First, take a look at the Review Content . Next, select a project to review. The projects on this list have been selected to fit with the content of this course. Remember, don't read the review until you've written about the project yourself! The (De)collected War of the Worlds | Project Review Borderlands Archives Cartography | Project Review Chicana/o Activism in the Southern Plains Through Time and Space | Project Review Ticha | Project Review Lansing Urban Renewal | Project Review Write 200-300 on the project, using the Reviews in DH guidelines. Remember, you are not an expert yet, so you may not be able to address all aspects of the project. Now read the accompanying review. What did you learn? How did it compare to your review? Write another 200-300 words on the differences in your reviews and the new concepts or approaches you learned. This blog is about process and learning! You are not expected to know everything already! I want to see how you are engaging with new material. Week 3 - Data # Tuesday, September 8, 2020 # Read the Data chapter by class time. Class agenda: icebreaker: favorite dog breed (4:30-4:45) Activity 3.1 (4:45-5:10) Activity 3.2 (5:10-5:40) Activity 3.3 (5:40-6:00) For Thursday: Read through Activity 3.4, including the report to prepare for our in-class activity. Read over the Process section and bring questions. Read Big? Smart? Clean? Messy? Data in the Humanities Thursday, September 10, 2020 # Class agenda: icebreaker: what is the least interesting fact about yourself? (4:30-4:40) Project Q&A (4:40-4:50) Activity 3.4 (4:50-6:00) Activity log week 3 # Due Monday 9/14 at 9am Submit your answers from Activities 3.3 via a document in Canvas. Blog post #3: Respond to the following prompt in 300-500 words. Time to start thinking about your project! You will conduct this project on your own, though we'll have plenty of opportunities for peer review and feedback. You will choose your topic, so it's time to start brainstorming. What types of humanities-based topics do you find compelling? What subjects are you interested in school and out of school? What might make a good data-driven topic? Maybe you've always wanted to see a map from a particular historical event, or analyze the word choice of a favorite author. Week 4 - Visualization # Tuesday, September 15, 2020 # Read Visualization by class time. Data + Design: A simple introduction to preparing and visualizing information , Visualization section, chapters 12-15 Go out into the wilds of the internet and find a data visualization that speaks to you to prepare for Activity 4.2. Class agenda: icebreaker: Are you excited about fall? Why? (4:30-4:45) Activity 4.1. (4:45-5:10) Activity 4.2 (5:15-6pm) For Thursday Read When the Designer Shows Up in the Design Read The Historian's Macroscope - Making Your Data Legible: A Basic Introduction to Visualizations and subsequent sections Thursday, September 17, 2020 # Class agenda: icebreaker: is there a scary thing from your childhood that still affects your behavior today? (4:30-4:40pm) Discuss readings (4:40-5:00pm) Activity 4.3 (5:00-5:45pm) Regroup (5:45-6pm) Activity log week 4 # Due Monday 9/21 at 9am Turn in the results of Activity 4.1 on Canvas. Write 100-200 words on the visualization you found, addressing the questions in Activity 4.2. Turn this in as a document on Canvas. Blog post #4: For your blog post this week, go through the same process in Activity 4.3 with the Cemetery data set. You may have to clean up fields in Open Refine or Excel to get the results you're looking for. Post several visualizations, along with a 300-400 word blog post on your results. What is your goal with these visualizations? How do they help you understand the data? How did you put these graphs together? Did you have to clean the data or look up how to create the graph? What did you struggle with? Are you happy with the results? What might you want to do with more time/skills? What have you learned about data visualization? Week 5 - Network Analysis # Tuesday, September 22, 2020 # Read Network by class time. Read Demystifying Networks Class agenda: icebreaker: favorite chore (4:30-4:40) recap week 4 + intro week 5 (4:40-5:00) Activity 5.1 in breakout rooms (5:00-5:30) Share Activity 5.1 (5:30pm) Talk about homework for this week For Thursday Prepare for Activity 5.2 . You will be asked to share in class on the project you choose! Thursday, September 24, 2020 # Read An Entry of One's Own Read the first few sections of the Process chapter again. Class agenda: icebreaker: favorite casserole? (4:30-4:40) Activity 5.2 (4:40-5:10pm) Activity 5.3 (5:15-5:45pm) Talk about 5.4 and blog post. Activity log week 5 # Due Monday 9/28 at 9am Spend some more time with the data and networks of Activity 5.4 . Upload one network visualization to your website with 150-300 words on what worked and what didn't. What is this network telling you? What is it obscuring? What else do you need to know about the people in this network? Blog post #5: Time to start your research! Prompt: Review the Research section of the Process chapter. Set a timer for 25 minutes and do some gathering of potential sources and contextual material. Remember your ideas from week 3? This is your chance to explore further. Keep a log of what you find. Take a break, or come back another day and set your timer again. This time your goal is to refine your list into useful sources. You should have at least 2 scholarly sources and at least 2 other sources. Your blog post should include 100+ words on the experience of doing this work. What worked for you? What didn't? What did you discover about the research process? List your 5 sources with 2-sentence annotations. In the first sentence, summarize the source. In the second sentence, say something about why this source is useful for your project. Week 6 - Spatial Analysis # Tuesday, September 29, 2020 # Read the Spatial Analysis by class time. Class agenda: icebreaker: if money/time/obligations/pandemic were no issue, where would go right now? (4:30-4:40) Activity 6.1 Activity 6.2 Thursday, October 1, 2020 # read What is Spatial History? and The Contested Nature of Historical GIS by class time. Class agenda: icebreaker: guilty pleasure movie (Bryan) Activity 6.3 Activity log week 6 # Due Monday 10/5 at 9am To complete Activity 6.3 , select a second mapping platform to explore (not the one you looked at in class). In a 300-500 words, answer the questions from Activity 6.3, then provide additional commentary on the differences between this platform and the one you looked at in class. Consider how to use the platform and what can be created with it. Use the Coeducation Report data set to create a map, then embed it into your post. Blog post: Now that we've had more practice reviewing projects, it's time to write your own from scratch! Using the Reviews in DH Review Content criteria, write a 500 word review of one of the following spatial projects: Civil War DC Mapping the Second Klu Klux Klan Navigating the Green Book Visualizing Emancipation DECIMA Week 7 - Text Analysis # Tuesday, October 6, 2020 # Read Text Analysis by class time. Print out this activity packet before class! Class agenda: icebreaker: favorite hike or outdoor location in Rockbridge county? 4 Ways to Read activity (4:45-6) Thursday, October 8, 2020 # Read Argument Clinic by Scott Weingart Sign up for a meeting time on Box . Class agenda: icebreaker: comfort book anyone? Presentation opportunities: Science, Society, and the Arts + Cheakapeake DH Consortium Conference Activity 7.2 Activity log week 7 # Due Monday 10/12 at 9am Activity 7.3 Draft your project proposal in preparation for next week's individual consults. This will be your week 7 blog post. It is okay if it is full of questions! If you feel yourself getting stressed out while writing this proposal, please take a break and feeling out the project feelings form ! It can be anonymous if you want and will help me prep for our meeting. Week 8 - Project Planning # We'll schedule individual meetings this week to discuss your project. Sign up for a time on Box . I recommend reading all of the Process section if you haven't already! Due Monday 10/19 at 9am Your final project proposal is due. No blog post! Week 9 - Data Modeling # Tuesday, October 20, 2020 # Class agenda icebreaker: favorite scary movie? (4:30-4:40) Project check-in (4:40-4:50) Data modeling (4:50-5:05) Data plan (5:05-5:20) Data license (5:20-5:30) Wordpress time! (5:30-5:45) Thursday, October 22, 2020 # Open office hours: drop in to ask questions, get feedback, or hang out. (4:30-5:30pm) Continue to use the project feelings form as a place to dump feelings you have about your project! Responses can be anonymous. Due Monday 10/28 at 9am The data documentation assignment should be posted to your course website with the link turned in on Canvas. Blog post week #9: In 250-300 words, let's have a project update! Address the following questions: what did you get done this week? How did it go? What could have gone better? What lessons did you learn this week? What did you learn that will be useful outside this class? What are your goals for next week? Week 10 - Data Analysis + Visualization # Tuesday, October 27, 2020 # Open office hours: drop in to ask questions, get feedback, or hang out. (4:30-5:30pm) For Thursday: Prepare 1-2 draft visualizations for peer review! Post them to your website so your classmate can view them. Thursday, October 29, 2020 # Class agenda: icebreaker: what was your best childhood Halloween costume? visualization peer review. In your groups, address the following: Take a look at your partner's visualization. Can you figure out what's going on without help? Share one positive comment and one recommendation for improvement. Ask questions of your reviewer! How can they help you improve your visualization in a way you haven't thought about yet? While you're here, any recommendations for your partner's website? Is the info organized well? Can you find each project? Are visuals/fonts/colors used effectively? Due Monday 11/2 at 9am Drafts of your data visualizations are due. Post to your website and turn in the link on Canvas. You might find that after conducting analysis and writing about your results that you need to modify your visualizations. That's fine! Final data visualizations will be due 11/9. Even though this is a draft, you still need to turn something in to receive credit for this assignment. Blog post week #10: In 250-300 words, describe your experience with peer review. What feedback did you receive? What changes will you make based on this feedback? How did looking at your partner's visualization help you think about your own? What surprised you about their feedback? How can you modify your project to ensure comprehension from other viewers? What do you need to do to improve your visualizations for next week? Week 11 - Results + Interpretation # Tuesday, November 3, 2020 # VOTE Thursday, November 5, 2020 # Week 12 - Wrapping up + Presentation # Tuesday, November 10, 2020 # Thursday, November 12, 2020 #","title":"Schedule"},{"location":"schedule/#week-1-how-the-web-works","text":"","title":"Week 1 - How the Web Works"},{"location":"schedule/#tuesday-august-25-2020","text":"Getting to know you survey Read What is Humanities Data by class time. Class agenda: introductions + icebreaker: earliest memory of the internet? (4:30-4:45) course overview (4:45-5:05) quick break (5:05-5:10) breakout rooms: course charter (5:10-5:30) review charter together (5:30-5:45) go over activities for Thursday (5:45-6pm) For Thursday Complete Activity 1.1 . Nothing to turn in, just spent some time exploring and thinking. Download Sublime Text Editor . Read How the Web Works by class time. We will do the activities in class, but read through them so you're ready. Read Digital_Humanities , chapter 1","title":"Tuesday, August 25, 2020"},{"location":"schedule/#thursday-august-27-2020","text":"How the Web Works! Class agenda: icebreaker: what do you wish you'd known about W&L as a first year? (4:30-4:40) activity 1.1 sharing + discussion (4:40-4:50) activities 1.2 + 1.3 walk through (4:50-5:00) activities 1.2 + 1.3 solo work time + break (5:00-5:15) activities 1.4-1.6 walk through (5:15-5:30) activities 1.4-1.6 work time (5:30-6:00)","title":"Thursday, August 27, 2020"},{"location":"schedule/#activity-log-week-1","text":"Due Monday 8/31 at 9am Wow because of date typo, you have until Tuesday, 9/1 at 9am to submit. Complete Activities 1.2, 1.3, 1.4 . In Canvas, submit the URL webpage that you have coded and uploaded to your domain. Complete Activities 1.5-1.6 . In Canvas, submit the URL to your Wordpress site. There should be evidence that you have customized your Wordpress beyond the basic settings. Blog post #1. Respond to the following prompt in a 300-500 word blog post on your Wordpress. In Canvas, submit the URL the blog post. Prompt: What brought you to this class? What is your current relationship with technology? Where do you see data in your coursework? Where do you see data in your daily life? What humanities subjects are particularly interesting to you? What are your own goals for this course? What do you want to accomplish/learn/achieve?","title":"Activity log week 1"},{"location":"schedule/#week-2-your-computer","text":"","title":"Week 2 - Your Computer"},{"location":"schedule/#tuesday-september-1-2020","text":"Read Your Computer by class time. Class agenda: icebreaker: It's September today. What will you miss the most about summer? breakout groups: review websites in small groups (4:45-5:00) First, share with your partner one realization you had while learning HTML/CSS. It can be technical like \"don't forget that slash\" or conceptual \"I realized I need to read directions more closely.\" Second, visit each other's hand-coded website (not Wordpress). Share the link in chat. Right click/Ctrl + Click to view page source. Proof your partner's code. Check their syntax. Did they miss any slashes? Is everything in the right place? Work together to add the following to each of your websites: an iframe tag which will embed another website onto your own. two div tags with different background colors (hint, you'll need to use a class or id) Be sure to upload your changes to your website. break (5:00-5:05) start Activity 2.1 together (5:05-5:45) talk about Week 2 blog post For Thursday: Finish Activity 2.1 and 2.2 by class time.","title":"Tuesday, September 1, 2020"},{"location":"schedule/#thursday-september-3-2020","text":"Class agenda: icebreaker: what's your favorite comfort tv show? (4:30-4:40) Introduce Final Project (4:40-5:00) Start Activity 2.3 together in breakout rooms (5:00-5:45) Questions??? (5:45-6:00)","title":"Thursday, September 3, 2020"},{"location":"schedule/#activity-log-week-2","text":"Due Monday 9/7 at 9am Complete Activity 2.3 . In Canvas, submit a document with your answers. It is more important that you show your process, thinking, and attempts than it is that you get everything right. Blog post #2: Respond to the following prompt in a 500 word blog post on your Wordpress. In Canvas, submit the URL the blog post. Prompt: To help us conceptualize humanities data projects, let's look at a few more projects and the accompanying reviews. Scholarly articles and books are typically reviewed by other experts in the field. There are established procedures and venues for this kind of work, however DH projects do not always fit into those procedures. Reviews in DH is a new publication that \"bridges the gap\" by providing reviews of projects. For this blog post, you will explore and write about a project before reading the accompanying review, then read the review and see how your analysis lines up. First, take a look at the Review Content . Next, select a project to review. The projects on this list have been selected to fit with the content of this course. Remember, don't read the review until you've written about the project yourself! The (De)collected War of the Worlds | Project Review Borderlands Archives Cartography | Project Review Chicana/o Activism in the Southern Plains Through Time and Space | Project Review Ticha | Project Review Lansing Urban Renewal | Project Review Write 200-300 on the project, using the Reviews in DH guidelines. Remember, you are not an expert yet, so you may not be able to address all aspects of the project. Now read the accompanying review. What did you learn? How did it compare to your review? Write another 200-300 words on the differences in your reviews and the new concepts or approaches you learned. This blog is about process and learning! You are not expected to know everything already! I want to see how you are engaging with new material.","title":"Activity log week 2"},{"location":"schedule/#week-3-data","text":"","title":"Week 3 - Data"},{"location":"schedule/#tuesday-september-8-2020","text":"Read the Data chapter by class time. Class agenda: icebreaker: favorite dog breed (4:30-4:45) Activity 3.1 (4:45-5:10) Activity 3.2 (5:10-5:40) Activity 3.3 (5:40-6:00) For Thursday: Read through Activity 3.4, including the report to prepare for our in-class activity. Read over the Process section and bring questions. Read Big? Smart? Clean? Messy? Data in the Humanities","title":"Tuesday, September 8, 2020"},{"location":"schedule/#thursday-september-10-2020","text":"Class agenda: icebreaker: what is the least interesting fact about yourself? (4:30-4:40) Project Q&A (4:40-4:50) Activity 3.4 (4:50-6:00)","title":"Thursday, September 10, 2020"},{"location":"schedule/#activity-log-week-3","text":"Due Monday 9/14 at 9am Submit your answers from Activities 3.3 via a document in Canvas. Blog post #3: Respond to the following prompt in 300-500 words. Time to start thinking about your project! You will conduct this project on your own, though we'll have plenty of opportunities for peer review and feedback. You will choose your topic, so it's time to start brainstorming. What types of humanities-based topics do you find compelling? What subjects are you interested in school and out of school? What might make a good data-driven topic? Maybe you've always wanted to see a map from a particular historical event, or analyze the word choice of a favorite author.","title":"Activity log week 3"},{"location":"schedule/#week-4-visualization","text":"","title":"Week 4 - Visualization"},{"location":"schedule/#tuesday-september-15-2020","text":"Read Visualization by class time. Data + Design: A simple introduction to preparing and visualizing information , Visualization section, chapters 12-15 Go out into the wilds of the internet and find a data visualization that speaks to you to prepare for Activity 4.2. Class agenda: icebreaker: Are you excited about fall? Why? (4:30-4:45) Activity 4.1. (4:45-5:10) Activity 4.2 (5:15-6pm) For Thursday Read When the Designer Shows Up in the Design Read The Historian's Macroscope - Making Your Data Legible: A Basic Introduction to Visualizations and subsequent sections","title":"Tuesday, September 15, 2020"},{"location":"schedule/#thursday-september-17-2020","text":"Class agenda: icebreaker: is there a scary thing from your childhood that still affects your behavior today? (4:30-4:40pm) Discuss readings (4:40-5:00pm) Activity 4.3 (5:00-5:45pm) Regroup (5:45-6pm)","title":"Thursday, September 17, 2020"},{"location":"schedule/#activity-log-week-4","text":"Due Monday 9/21 at 9am Turn in the results of Activity 4.1 on Canvas. Write 100-200 words on the visualization you found, addressing the questions in Activity 4.2. Turn this in as a document on Canvas. Blog post #4: For your blog post this week, go through the same process in Activity 4.3 with the Cemetery data set. You may have to clean up fields in Open Refine or Excel to get the results you're looking for. Post several visualizations, along with a 300-400 word blog post on your results. What is your goal with these visualizations? How do they help you understand the data? How did you put these graphs together? Did you have to clean the data or look up how to create the graph? What did you struggle with? Are you happy with the results? What might you want to do with more time/skills? What have you learned about data visualization?","title":"Activity log week 4"},{"location":"schedule/#week-5-network-analysis","text":"","title":"Week 5 - Network Analysis"},{"location":"schedule/#tuesday-september-22-2020","text":"Read Network by class time. Read Demystifying Networks Class agenda: icebreaker: favorite chore (4:30-4:40) recap week 4 + intro week 5 (4:40-5:00) Activity 5.1 in breakout rooms (5:00-5:30) Share Activity 5.1 (5:30pm) Talk about homework for this week For Thursday Prepare for Activity 5.2 . You will be asked to share in class on the project you choose!","title":"Tuesday, September 22, 2020"},{"location":"schedule/#thursday-september-24-2020","text":"Read An Entry of One's Own Read the first few sections of the Process chapter again. Class agenda: icebreaker: favorite casserole? (4:30-4:40) Activity 5.2 (4:40-5:10pm) Activity 5.3 (5:15-5:45pm) Talk about 5.4 and blog post.","title":"Thursday, September 24, 2020"},{"location":"schedule/#activity-log-week-5","text":"Due Monday 9/28 at 9am Spend some more time with the data and networks of Activity 5.4 . Upload one network visualization to your website with 150-300 words on what worked and what didn't. What is this network telling you? What is it obscuring? What else do you need to know about the people in this network? Blog post #5: Time to start your research! Prompt: Review the Research section of the Process chapter. Set a timer for 25 minutes and do some gathering of potential sources and contextual material. Remember your ideas from week 3? This is your chance to explore further. Keep a log of what you find. Take a break, or come back another day and set your timer again. This time your goal is to refine your list into useful sources. You should have at least 2 scholarly sources and at least 2 other sources. Your blog post should include 100+ words on the experience of doing this work. What worked for you? What didn't? What did you discover about the research process? List your 5 sources with 2-sentence annotations. In the first sentence, summarize the source. In the second sentence, say something about why this source is useful for your project.","title":"Activity log week 5"},{"location":"schedule/#week-6-spatial-analysis","text":"","title":"Week 6 - Spatial Analysis"},{"location":"schedule/#tuesday-september-29-2020","text":"Read the Spatial Analysis by class time. Class agenda: icebreaker: if money/time/obligations/pandemic were no issue, where would go right now? (4:30-4:40) Activity 6.1 Activity 6.2","title":"Tuesday, September 29, 2020"},{"location":"schedule/#thursday-october-1-2020","text":"read What is Spatial History? and The Contested Nature of Historical GIS by class time. Class agenda: icebreaker: guilty pleasure movie (Bryan) Activity 6.3","title":"Thursday, October 1, 2020"},{"location":"schedule/#activity-log-week-6","text":"Due Monday 10/5 at 9am To complete Activity 6.3 , select a second mapping platform to explore (not the one you looked at in class). In a 300-500 words, answer the questions from Activity 6.3, then provide additional commentary on the differences between this platform and the one you looked at in class. Consider how to use the platform and what can be created with it. Use the Coeducation Report data set to create a map, then embed it into your post. Blog post: Now that we've had more practice reviewing projects, it's time to write your own from scratch! Using the Reviews in DH Review Content criteria, write a 500 word review of one of the following spatial projects: Civil War DC Mapping the Second Klu Klux Klan Navigating the Green Book Visualizing Emancipation DECIMA","title":"Activity log week 6"},{"location":"schedule/#week-7-text-analysis","text":"","title":"Week 7 - Text Analysis"},{"location":"schedule/#tuesday-october-6-2020","text":"Read Text Analysis by class time. Print out this activity packet before class! Class agenda: icebreaker: favorite hike or outdoor location in Rockbridge county? 4 Ways to Read activity (4:45-6)","title":"Tuesday, October 6, 2020"},{"location":"schedule/#thursday-october-8-2020","text":"Read Argument Clinic by Scott Weingart Sign up for a meeting time on Box . Class agenda: icebreaker: comfort book anyone? Presentation opportunities: Science, Society, and the Arts + Cheakapeake DH Consortium Conference Activity 7.2","title":"Thursday, October 8, 2020"},{"location":"schedule/#activity-log-week-7","text":"Due Monday 10/12 at 9am Activity 7.3 Draft your project proposal in preparation for next week's individual consults. This will be your week 7 blog post. It is okay if it is full of questions! If you feel yourself getting stressed out while writing this proposal, please take a break and feeling out the project feelings form ! It can be anonymous if you want and will help me prep for our meeting.","title":"Activity log week 7"},{"location":"schedule/#week-8-project-planning","text":"We'll schedule individual meetings this week to discuss your project. Sign up for a time on Box . I recommend reading all of the Process section if you haven't already! Due Monday 10/19 at 9am Your final project proposal is due. No blog post!","title":"Week 8 - Project Planning"},{"location":"schedule/#week-9-data-modeling","text":"","title":"Week 9 - Data Modeling"},{"location":"schedule/#tuesday-october-20-2020","text":"Class agenda icebreaker: favorite scary movie? (4:30-4:40) Project check-in (4:40-4:50) Data modeling (4:50-5:05) Data plan (5:05-5:20) Data license (5:20-5:30) Wordpress time! (5:30-5:45)","title":"Tuesday, October 20, 2020"},{"location":"schedule/#thursday-october-22-2020","text":"Open office hours: drop in to ask questions, get feedback, or hang out. (4:30-5:30pm) Continue to use the project feelings form as a place to dump feelings you have about your project! Responses can be anonymous. Due Monday 10/28 at 9am The data documentation assignment should be posted to your course website with the link turned in on Canvas. Blog post week #9: In 250-300 words, let's have a project update! Address the following questions: what did you get done this week? How did it go? What could have gone better? What lessons did you learn this week? What did you learn that will be useful outside this class? What are your goals for next week?","title":"Thursday, October 22, 2020"},{"location":"schedule/#week-10-data-analysis-visualization","text":"","title":"Week 10 - Data Analysis + Visualization"},{"location":"schedule/#tuesday-october-27-2020","text":"Open office hours: drop in to ask questions, get feedback, or hang out. (4:30-5:30pm) For Thursday: Prepare 1-2 draft visualizations for peer review! Post them to your website so your classmate can view them.","title":"Tuesday, October 27, 2020"},{"location":"schedule/#thursday-october-29-2020","text":"Class agenda: icebreaker: what was your best childhood Halloween costume? visualization peer review. In your groups, address the following: Take a look at your partner's visualization. Can you figure out what's going on without help? Share one positive comment and one recommendation for improvement. Ask questions of your reviewer! How can they help you improve your visualization in a way you haven't thought about yet? While you're here, any recommendations for your partner's website? Is the info organized well? Can you find each project? Are visuals/fonts/colors used effectively? Due Monday 11/2 at 9am Drafts of your data visualizations are due. Post to your website and turn in the link on Canvas. You might find that after conducting analysis and writing about your results that you need to modify your visualizations. That's fine! Final data visualizations will be due 11/9. Even though this is a draft, you still need to turn something in to receive credit for this assignment. Blog post week #10: In 250-300 words, describe your experience with peer review. What feedback did you receive? What changes will you make based on this feedback? How did looking at your partner's visualization help you think about your own? What surprised you about their feedback? How can you modify your project to ensure comprehension from other viewers? What do you need to do to improve your visualizations for next week?","title":"Thursday, October 29, 2020"},{"location":"schedule/#week-11-results-interpretation","text":"","title":"Week 11 - Results + Interpretation"},{"location":"schedule/#tuesday-november-3-2020","text":"VOTE","title":"Tuesday, November 3, 2020"},{"location":"schedule/#thursday-november-5-2020","text":"","title":"Thursday, November 5, 2020"},{"location":"schedule/#week-12-wrapping-up-presentation","text":"","title":"Week 12 - Wrapping up + Presentation"},{"location":"schedule/#tuesday-november-10-2020","text":"","title":"Tuesday, November 10, 2020"},{"location":"schedule/#thursday-november-12-2020","text":"","title":"Thursday, November 12, 2020"},{"location":"spatial-analysis/","text":"In this section, we'll learn about ways of analyzing spatial data, through maps or similar visualizations. What is spatial analysis? What does it look like in the humanities? Narrative maps Data maps Beyond maps Spatial data Mapping Tools Activities Activity 6.1 Activity 6.2 Activity 6.3 Resources Readings Credits What is spatial analysis? # It's hard to deny the popularity of maps. Many of us use them on a daily basis - for wayfinding, as art on our walls, or as data visualization. Advancements in digital maps and location-based services from our cell phones make us more reliant on maps and GIS or geographic information systems, than ever before. Location-based data is now a huge commodity, not to mention a security and privacy issue. That being said, the tools of geographers can now be used by humanities scholars in a wide range of ways. Both commercial and academic mapping tools are available for studying the movement of humans through space and time. There are major Digital Humanities projects devoted to creating, organizing, and publishing data about historical locations. There are tools to layer historic maps onto modern maps, allowing the user to layer the past onto the present. But it's important to remember that all maps, historical or modern, print or browser-based, are representations of space. Google Maps is just as capable of distorting reality as a map from 1500. Humanities scholars are using maps to study the past, but they're also demonstrating that maps \"are themselves highly contingent fabrications, bending the physical reality of the world to our innate need to grasp and process, and dangerously full of altered data.\" ( Torn/Apart ) What does it look like in the humanities? # Spatial work takes many forms in the humanities. In his Spatial Humanities workshop , historian Lincoln Mullen describes two types: narrative maps and data map. We'll use these categories to explore what spatial humanities looks like. At this point, this coursebook will not cover virtual reality or three-dimensional space, but know that those are increasingly commons ways to represent space. They also generate a lot of data! Narrative maps # According to Mullen, we can define narrative maps this way: A narrative map tells a story plotted through space. The point of a narrative map is not to display data. Rather it is to provide an explicit visual counterpart to the implicit spatial underpinnings of a narrative or argument. Narrative map are a little more like infographics than data visualizations in this way. They might depict a journey, narrate the movement of an event, or layer historical maps of the same place in a creative way. Like every tool or method, it's important to know your audience and goals when deciding to use a narrative map. They can be a great way to convey information in an engaging setting, particularly one that is public-facing, such as a digital exhibit for a museum. But they don't, by-and-large, contain a lot of data, which is what we're interested in in this coursebook. Data maps # Data maps are maps created to visualize or present data about or relevant to a location. In most cases, you are uploading a data set to an application and that application generates a view of the data, rather than dropping pins by hand. Your data set could contain information about many things - people, buildings, monuments, statistics, you name it - but there should be a spatial data component. What that component looks like often depends on the tool you're using. Some programs can identify state names or zip codes, others need specific coordinates. Of course, it also depends on the rest of your data. Do you want your map to show voting patterns by county? Then you'll need spatial data that is aware of county boundaries. Perhaps you want to show individual residences, then you're likely to need individual coordinates for each location. Do you need to keep time period in mind in case boundaries have changed? Maybe you want users to be able to interact with your map, how will that look? Data driven maps can be tricky because it's not always clear what is easy to do and what is more difficult. Each tool has its own affordances or expectations. It can be a good idea to explore a new mapping tool with a sample data set to learn the features and limitations. In a later section, we'll share various mapping tools and their strengths and weaknesses. Beyond maps # It should be noted that there is more to analyzing space than just building maps. Advanced GIS applications or programming languages will provide you with methods (beyond your eyes) for analyzing patterns in your data. Most of these methods are beyond the scope of this coursebook, but know that they exist when you're ready to take your spatial analysis to the next level. Spatial data # Due to the volume of proprietary and open tools, spatial data can come in many forms. We'll get to file formats in a minute, but let's start with the basics: Coordinate system - By drawing lines around the earth (latitude and longitude), geographers have created a grid system from which we can calculate points and assign numbers to locations. The equator and the Prime Meridian are the lines where we start counting, where the latitude and longitude (respectively) is set to 0\u00b0. Map projections - As we know, the earth is round. To project the round earth onto a flat map (or plane), adjustments must be made. Imagine trying to cover a baseball with a piece of paper - it's going to have some creases right? Geographers use various projections to account for this problem. Most of us will find the Mercator projection to be quite familiar, but it is actually quite misleading . If you have the option, you want to choose a projection that fairly represents the space you're depicting. Latitude/longitude - Latitude are lines drawn horizontally around the earth. Longitude are lines drawn vertically around the earth. Lat/long coordinates can be expressed in degrees, minutes, and seconds (38\u00b0 53\u2032 23\u2033 N, 77\u00b0 00\u2032 32\u2033 W) or decimals (38.8897, -77.0089). You'll typically use decimal lat/long in spatial data projects. Vector vs. raster - You'll encounter these terms when it comes time to put stuff on your map. Vector layers are made up of shapes (including points, lines, polygons, etc) that can shift and change along with your map. Even though they look like lines, they are actually the relationship between places. Raster layers are made up of pixels. Imagine adding a historic map or a satellite image to your mp. While raster layers can stretch to fit a certain area, they might pixelate or morph in undesirable ways. Shapefiles - Shapefiles is a term for vector spatial data. Your points, lines, polygons are called shapes. As data, they are expressed with coordinates. Specifically, they are used by the company ESRI in their ArCGIS suite of software. You'll see shapefiles with the file extensions: .shp, .shx, .dbf, and .prj . GeoJSON - We've seen some of these letters before right? JSON is a serialized data format. GeoJSON is a common data format for storing geographic information. It might look something like this: { \"type\": \"Feature\", \"geometry\": { \"type\": \"Point\", \"coordinates\": [125.6, 10.1] }, Mapping Tools # The popularity and usefulness of maps means that there are a wide range of commercial and scholarly tools available. Unfortunately, many of the flashiest tools are only available at high cost. ESRI ArcGIS - ArcGIS is the industry standard in GIS and is licensed by corporations and academic institutions. ESRI provides an ecosystem of software, most of it quite expensive and desktop based. ArcGIS Online is the browser-based option, though you have to pay for full features. ESRI does provide ArcGIS StoryMaps , a great narrative map option, at no cost. QGIS - QGIS is free, open GIS software for your desktop. It allows you to view, modify, and analyze geospatial data. However, QGIS is more for crunching data, not publishing it online. Google Maps - Most folks are familiar with Google Maps. The My Maps feature allows you to create your own map, including via an uploaded dataset (though there are limits on the amount of data and features). You might see some references to maps created with Google Fusion Tables - this product no longer exists. One of the reasons to be conscious of the companies you trust with your product. Tableau - Tableau is another industry piece of software, frequently licensed by businesses and universities. The free version is Tableau Public . LeafletJS - Leaflet.js is a JavaScript library created interactive maps for the web. You will need to know a little to a lot of code to get your map to do what you want, but it's a popular option for custom projects. Palladio - Palladio is a great digital humanities tool for early exploration of your data. The mapping capabilities are limited, but it's easy to get up and running with a prototyped map. StoryMapJS - StoryMapJS is the quintessential narrative map tool. You can customize your map and add to your story through slides in their standalone interface. There is no data component here, but you can add various media. Neatline - Neatline is another narrative map tool that fits within a large digital exhibit platform called Omeka. Neatline allows you to insert text and image into your map, or even add in a timeline component. Activities # Activity 6.1 # To start thinking spatially, let's create a map from scratch. Gather some blank paper and your favorite writing/drawing/coloring instruments. Without consulting any existing maps, draw a map of campus. As a group, we'll compare our maps with each other. How are our maps similar or different? What choices did you make? How are your choices informed by your hobbies, extracurriculars, personality, abilities, etc? What was challenging? What makes our campus hard/easy to map? Now, let's look at the existing campus map as well as images from the firm redesigning some of our spaces on campus. Activity 6.2 # Let's continue building on our Coeducation Report data set so we can use it to try out some mapping tools in the next activity. Our data set needs coordinates, not just location names, for it to be easily to process by the mapping tools. Before I tell you exactly how to add coordinates, spend some time on Google. What services are out there for converting location names to coordinates? How do they work? What formats do they accept or produce? Remember, while it might be possible to convert each location one at a time, this would not be practical if our data set was any bigger. Let's look for some more automated solutions. Activity 6.3 # There are a lot of options for tools that will create maps. In this activity, we'll divide into groups and test out a platform or two. We'll use the Coeducation Report as the data source for our map. Platforms: Palladio ArcGIS Online - Note that you can create a free public account or use the StoryMaps feature. We have an institutional license if you want to use ArcGIS longer term. Google My Maps Tableau Public In your groups, answer the following questions. Record your answers in a Boxnote so that other students can consult the information. Who created this tool? Is it open source? Is it free? How do I use it? Browser or download? OS preference? How do I add data? What formats does it accept? How do I add layers? How do I add shapes or pins? Can I add a historical map? How? Where is the basemap from? (Google, OpenStreetMap, etc.) Can I change it? What type of mapping project would suit this tool? Can you find the documentation? Other tutorials? Using the Coeducation Report data, create a map. You may need to alter the data to fit the requirements of the mapping tool. To complete this activity after class, select a second platform and go through the same questions. Compare the two platforms. Resources # Spatial Humanities Workshop Readings # Anatomy of a Web Map What is Spatial History? Credits # Material for this section was derived from Lincoln Mullen's Spatial Humanities Workshop materials per the CC-BY-NC-SA 4.0 license.","title":"Spatial Analysis"},{"location":"spatial-analysis/#what-is-spatial-analysis","text":"It's hard to deny the popularity of maps. Many of us use them on a daily basis - for wayfinding, as art on our walls, or as data visualization. Advancements in digital maps and location-based services from our cell phones make us more reliant on maps and GIS or geographic information systems, than ever before. Location-based data is now a huge commodity, not to mention a security and privacy issue. That being said, the tools of geographers can now be used by humanities scholars in a wide range of ways. Both commercial and academic mapping tools are available for studying the movement of humans through space and time. There are major Digital Humanities projects devoted to creating, organizing, and publishing data about historical locations. There are tools to layer historic maps onto modern maps, allowing the user to layer the past onto the present. But it's important to remember that all maps, historical or modern, print or browser-based, are representations of space. Google Maps is just as capable of distorting reality as a map from 1500. Humanities scholars are using maps to study the past, but they're also demonstrating that maps \"are themselves highly contingent fabrications, bending the physical reality of the world to our innate need to grasp and process, and dangerously full of altered data.\" ( Torn/Apart )","title":"What is spatial analysis?"},{"location":"spatial-analysis/#what-does-it-look-like-in-the-humanities","text":"Spatial work takes many forms in the humanities. In his Spatial Humanities workshop , historian Lincoln Mullen describes two types: narrative maps and data map. We'll use these categories to explore what spatial humanities looks like. At this point, this coursebook will not cover virtual reality or three-dimensional space, but know that those are increasingly commons ways to represent space. They also generate a lot of data!","title":"What does it look like in the humanities?"},{"location":"spatial-analysis/#narrative-maps","text":"According to Mullen, we can define narrative maps this way: A narrative map tells a story plotted through space. The point of a narrative map is not to display data. Rather it is to provide an explicit visual counterpart to the implicit spatial underpinnings of a narrative or argument. Narrative map are a little more like infographics than data visualizations in this way. They might depict a journey, narrate the movement of an event, or layer historical maps of the same place in a creative way. Like every tool or method, it's important to know your audience and goals when deciding to use a narrative map. They can be a great way to convey information in an engaging setting, particularly one that is public-facing, such as a digital exhibit for a museum. But they don't, by-and-large, contain a lot of data, which is what we're interested in in this coursebook.","title":"Narrative maps"},{"location":"spatial-analysis/#data-maps","text":"Data maps are maps created to visualize or present data about or relevant to a location. In most cases, you are uploading a data set to an application and that application generates a view of the data, rather than dropping pins by hand. Your data set could contain information about many things - people, buildings, monuments, statistics, you name it - but there should be a spatial data component. What that component looks like often depends on the tool you're using. Some programs can identify state names or zip codes, others need specific coordinates. Of course, it also depends on the rest of your data. Do you want your map to show voting patterns by county? Then you'll need spatial data that is aware of county boundaries. Perhaps you want to show individual residences, then you're likely to need individual coordinates for each location. Do you need to keep time period in mind in case boundaries have changed? Maybe you want users to be able to interact with your map, how will that look? Data driven maps can be tricky because it's not always clear what is easy to do and what is more difficult. Each tool has its own affordances or expectations. It can be a good idea to explore a new mapping tool with a sample data set to learn the features and limitations. In a later section, we'll share various mapping tools and their strengths and weaknesses.","title":"Data maps"},{"location":"spatial-analysis/#beyond-maps","text":"It should be noted that there is more to analyzing space than just building maps. Advanced GIS applications or programming languages will provide you with methods (beyond your eyes) for analyzing patterns in your data. Most of these methods are beyond the scope of this coursebook, but know that they exist when you're ready to take your spatial analysis to the next level.","title":"Beyond maps"},{"location":"spatial-analysis/#spatial-data","text":"Due to the volume of proprietary and open tools, spatial data can come in many forms. We'll get to file formats in a minute, but let's start with the basics: Coordinate system - By drawing lines around the earth (latitude and longitude), geographers have created a grid system from which we can calculate points and assign numbers to locations. The equator and the Prime Meridian are the lines where we start counting, where the latitude and longitude (respectively) is set to 0\u00b0. Map projections - As we know, the earth is round. To project the round earth onto a flat map (or plane), adjustments must be made. Imagine trying to cover a baseball with a piece of paper - it's going to have some creases right? Geographers use various projections to account for this problem. Most of us will find the Mercator projection to be quite familiar, but it is actually quite misleading . If you have the option, you want to choose a projection that fairly represents the space you're depicting. Latitude/longitude - Latitude are lines drawn horizontally around the earth. Longitude are lines drawn vertically around the earth. Lat/long coordinates can be expressed in degrees, minutes, and seconds (38\u00b0 53\u2032 23\u2033 N, 77\u00b0 00\u2032 32\u2033 W) or decimals (38.8897, -77.0089). You'll typically use decimal lat/long in spatial data projects. Vector vs. raster - You'll encounter these terms when it comes time to put stuff on your map. Vector layers are made up of shapes (including points, lines, polygons, etc) that can shift and change along with your map. Even though they look like lines, they are actually the relationship between places. Raster layers are made up of pixels. Imagine adding a historic map or a satellite image to your mp. While raster layers can stretch to fit a certain area, they might pixelate or morph in undesirable ways. Shapefiles - Shapefiles is a term for vector spatial data. Your points, lines, polygons are called shapes. As data, they are expressed with coordinates. Specifically, they are used by the company ESRI in their ArCGIS suite of software. You'll see shapefiles with the file extensions: .shp, .shx, .dbf, and .prj . GeoJSON - We've seen some of these letters before right? JSON is a serialized data format. GeoJSON is a common data format for storing geographic information. It might look something like this: { \"type\": \"Feature\", \"geometry\": { \"type\": \"Point\", \"coordinates\": [125.6, 10.1] },","title":"Spatial data"},{"location":"spatial-analysis/#mapping-tools","text":"The popularity and usefulness of maps means that there are a wide range of commercial and scholarly tools available. Unfortunately, many of the flashiest tools are only available at high cost. ESRI ArcGIS - ArcGIS is the industry standard in GIS and is licensed by corporations and academic institutions. ESRI provides an ecosystem of software, most of it quite expensive and desktop based. ArcGIS Online is the browser-based option, though you have to pay for full features. ESRI does provide ArcGIS StoryMaps , a great narrative map option, at no cost. QGIS - QGIS is free, open GIS software for your desktop. It allows you to view, modify, and analyze geospatial data. However, QGIS is more for crunching data, not publishing it online. Google Maps - Most folks are familiar with Google Maps. The My Maps feature allows you to create your own map, including via an uploaded dataset (though there are limits on the amount of data and features). You might see some references to maps created with Google Fusion Tables - this product no longer exists. One of the reasons to be conscious of the companies you trust with your product. Tableau - Tableau is another industry piece of software, frequently licensed by businesses and universities. The free version is Tableau Public . LeafletJS - Leaflet.js is a JavaScript library created interactive maps for the web. You will need to know a little to a lot of code to get your map to do what you want, but it's a popular option for custom projects. Palladio - Palladio is a great digital humanities tool for early exploration of your data. The mapping capabilities are limited, but it's easy to get up and running with a prototyped map. StoryMapJS - StoryMapJS is the quintessential narrative map tool. You can customize your map and add to your story through slides in their standalone interface. There is no data component here, but you can add various media. Neatline - Neatline is another narrative map tool that fits within a large digital exhibit platform called Omeka. Neatline allows you to insert text and image into your map, or even add in a timeline component.","title":"Mapping Tools"},{"location":"spatial-analysis/#activities","text":"","title":"Activities"},{"location":"spatial-analysis/#activity-61","text":"To start thinking spatially, let's create a map from scratch. Gather some blank paper and your favorite writing/drawing/coloring instruments. Without consulting any existing maps, draw a map of campus. As a group, we'll compare our maps with each other. How are our maps similar or different? What choices did you make? How are your choices informed by your hobbies, extracurriculars, personality, abilities, etc? What was challenging? What makes our campus hard/easy to map? Now, let's look at the existing campus map as well as images from the firm redesigning some of our spaces on campus.","title":"Activity 6.1"},{"location":"spatial-analysis/#activity-62","text":"Let's continue building on our Coeducation Report data set so we can use it to try out some mapping tools in the next activity. Our data set needs coordinates, not just location names, for it to be easily to process by the mapping tools. Before I tell you exactly how to add coordinates, spend some time on Google. What services are out there for converting location names to coordinates? How do they work? What formats do they accept or produce? Remember, while it might be possible to convert each location one at a time, this would not be practical if our data set was any bigger. Let's look for some more automated solutions.","title":"Activity 6.2"},{"location":"spatial-analysis/#activity-63","text":"There are a lot of options for tools that will create maps. In this activity, we'll divide into groups and test out a platform or two. We'll use the Coeducation Report as the data source for our map. Platforms: Palladio ArcGIS Online - Note that you can create a free public account or use the StoryMaps feature. We have an institutional license if you want to use ArcGIS longer term. Google My Maps Tableau Public In your groups, answer the following questions. Record your answers in a Boxnote so that other students can consult the information. Who created this tool? Is it open source? Is it free? How do I use it? Browser or download? OS preference? How do I add data? What formats does it accept? How do I add layers? How do I add shapes or pins? Can I add a historical map? How? Where is the basemap from? (Google, OpenStreetMap, etc.) Can I change it? What type of mapping project would suit this tool? Can you find the documentation? Other tutorials? Using the Coeducation Report data, create a map. You may need to alter the data to fit the requirements of the mapping tool. To complete this activity after class, select a second platform and go through the same questions. Compare the two platforms.","title":"Activity 6.3"},{"location":"spatial-analysis/#resources","text":"Spatial Humanities Workshop","title":"Resources"},{"location":"spatial-analysis/#readings","text":"Anatomy of a Web Map What is Spatial History?","title":"Readings"},{"location":"spatial-analysis/#credits","text":"Material for this section was derived from Lincoln Mullen's Spatial Humanities Workshop materials per the CC-BY-NC-SA 4.0 license.","title":"Credits"},{"location":"text-analysis/","text":"In this section, we'll learn about ways of using the computer to read and analyze texts. Much of the material in this section was adapted and condensed (with permission and blessing) from the Text Analysis Coursebook written by Sarah Horowitz and Brandon Walsh. Check out their coursebook for the full chapters and accompanying exercises. Close Reading How Computers Read Bag of words Topic Modeling Sentiment Analysis Building your corpus Copyright Text Analysis Tools Activities Activity 7.1 - 4 Ways of Reading a Text Activity 7.2 - Voyant Activity 7.3 Projects Resources Close Reading # Text analysis is something that we all engage in, whether we realize it or not. The term is broad and capacious and encapsulates a variety of different activities. Even something as simple as slowing down when you see a stop sign is a kind of text analysis: doing so means you have parsed the meaning of the words on the sign and reacted accordingly. Indeed any of the following, related activities are forms of text analysis: Paraphrasing a text Searching for hidden meanings in a text Adapting a text and reflecting on it Examining the details in a text This last point is worth pausing over: close reading, in particular, is often proclaimed as one of the primary analytical tool of scholars and students in the humanities. To read closely means to give careful attention to the various components that make up a text, ones which cause us to think or feel a certain way about it. Close reading relies on a core principle about the text under study: Everything about the text matters, whether the author intended for it to matter or not. Consider the following thought experiment. One day you come home to find the following note from your roommate on the counter: took care of these dishes? Thanks. Next to the note: dirty dishes. Was your roommate in a hurry and actually asking you to wash dishes? Or were they sarcastically trying to give you grief for not having done your part? Lots of questions. To imagine responses to them you might employ a range of assumptions and interpretations depending on the scenario: Context: you have been growing more and more irritated with your roommate for some time now. Their actions just really get under your skin: dirty dishes, laundry everywhere, the works. They clearly meant the note as an insult. Author: your roommate is actually a great person and would never leave a passive aggressive note. In fact, they probably meant it as a joke. Text: Take a look at that question mark. And then the curt second sentence. Your roommate put those things there on purpose to be rude. The list could go on and on. We employ a similar range of skills when we read anything, be it fiction, poetry, or historical documents. Close reading might be best described as an activity in which a reader simply lets no detail of the text go unquestioned. The best way at approaching a good close reading is by asking (and attempting to answer) questions about every piece of a text. How Computers Read # We've talked about how computers and humans have their respective skills. With respect to text analysis, we can say that computers and humans have complementary skills. Computers are good at doing things that would take us a long time to do or that would be incredibly tedious. Computers can easily count and compare and will do so for pretty much as long as you tell them to do so. In contrast, humans are very good at understanding nuance and context. Thus, you wouldn\u2019t want a computer to do any close reading, or unpack the claims of a primary or secondary text; this is something you are far better at. By the same token, it\u2019s probably easier to have a computer list all the numbers between one and 45678987 than to do it yourself. Before we start analyzing texts, we need to know how computers understand text. Consider the following sentence: \u201cWe saw 8 1/2.\u201d Taken alone, the sentence doesn\u2019t tell us much. Its meaning depends a lot on the question to which we might be responding, and we can think of two possible questions with very different contexts: \u201cHow many movies did you see? \u201cWhat movie did you see?\u201d In the first case, we might be responding with the number of movies that we had seen. It was a slow weekend, and we spent it at the local movie theater hopping from film to film. It was a great time! In the second situation, we might be responding with the title of a specific film, 8 1/2 by Italian director Frederico Fellini. So one answer is a number, and one answer is a name. Since humans are good at grasping context, we would easily be able to distinguish between the two. In most situations, we would just adjust our understanding internally before moving on with the conversation. Computers cannot make inferences like these, and this fact has serious implications: numbers and words have significantly different uses. Here are two further extensions of the conversation: If you add four to how many movies you saw, what is the result? If we were talking about a number of movies, my response would clearly be, \u201cOh that\u2019s 12.5. Why are you giving me a math quiz?\u201d If we were talking about the Fellini film, we might respond, \u201cWhat? Oh, we were talking about a title, not a number. We can\u2019t add things to a title.\u201d Again, humans have the ability to respond to context, infer, and adapt. Computers aren\u2019t nearly as flexible: they need to know ahead of time, in most cases, what kind of information they are dealing with. That way they can act as you anticipated. We learned earlier about the concept of \"data types.\" Well now they matter! In the above example, we encountered: Strings: characters, the stuff of words Integers: a whole numbers The distinction between strings and integers is important for text analysis. You can perform arithmetic operations on integers while strings respond less well to such things. You can capitalize words, but not numbers. And computers generally want you to deal with similar objects: you can combine strings (words can become sentences) or add numbers, but trying to combine a string and an integer will break things. To be clear to the computer which data type we mean, we use can indicate strings with \"quote\" marks and integers without. 8 vs. \"8\" . In response, the computer wants us to to know that it doesn't know that \"8\" is related to \"Eight\" or \"Eighth\" or even \"eight\" . It might seem surprising that computers aren't smarter than that, but it brings us to one of the first steps in text analysis: tokenization, or the process by which we break apart all of the words and punctuation in a text into separate tokens. \"I love you\" becomes \"I\", \"love\", \"you\", \".\". We can break things down even further once we\u2019ve divided a text into individual words. While we often care about how many times each particular token or word occurs, we might also care about the different kinds of words. We might want to keep track, on the one hand, of all the different words in a text regardless of how often they occur. But we might also want a different kind of vocabulary list. Rather than counting all the words, we might just want to grab a single example of each token type. If we have the following document: Test test test sentence sentence We have five tokens and two types (\u2018test\u2019 and \u2018sentence\u2019). A list of types might be good for getting a sense of the kinds of language used in a text, while a raw list of tokens could be useful for figuring out what kinds of words occur in which proportions. Depending on our research questions and interests, statistics like these can help us figure out what the document discusses as well as how it is being discussed. \u201cBut wait,\u201d you say, \u201ccomputers care about capitalization. So if we tokenize a text and try to compare \u2018word\u2019 and \u2018Word\u2019 they will think they are entirely different things!\u201d Good catch! You\u2019re right, those differences in capitalization often aren\u2019t meaningful. It is a fairly common practice to lowercase all the words after you tokenize them. This process is often called normalizing the data, since we are smoothing out inconsistencies that might get in the way of our research questions. This whole collection of processes of segmentation, tokenization, and normalization has a name of its own as well: preprocessing, all those things you do to data before you work with it. Depending on your interests, you might include other steps, such as tagging tokens for parts of speech or filtering out particular types of words. Note that preprocessing can change your list of types. A computer would not recognize \u201cSaid\u201d and \u201csaid\u201d as being of the same type, but, if you normalize capitalization so that every token is lowercased, a computer would see them as the same word. So you often need to decide what pieces you care about at the beginning of the process. Bag of words # When we read, our eyes move in sequence across the page and take in phrase after phrase in the order in which they were intended. This sense of chronology is integral to how we, as human readers, understand texts. But it is possible to imagine other ways of reading. Have you ever skimmed over a page backwards looking at every other word? You probably still got the gist of the text even though you didn\u2019t read it in order and even though you missed many of the words. If we take the words in a text as being indicative of its underlying topics, we actually don\u2019t need to worry about word order so much. The sequence of words, sometimes called the syntagmatic axis , only matters for certain kinds of reading. But we can find out interesting things about texts if we are a little more flexible and think about them not as things that unfold over time but rather as pure token counts, as bags of words. In a bag of words model , word order becomes irrelevant. All we care about is what words occur in a text and how often they do so. You can use a bag of words approach to determine how different or similar whole books or authors are from each other. If we have lists of words for each text as well as for the corpus (or set of documents) as a whole, we can actually work backwards to get a sense of the underlying topics that we were talking about a moment ago. Instead of skimming a paragraph to determine its basic topic, we could scan full texts \u2013 and scan lots of them. And rather than trying to get a sense of 1-3 topics, we could break our text apart into 15-20 different topics. Now we are cooking with gas, and we\u2019re talking about topic modeling. Topic Modeling # Given enough time and energy, we can imagine a tool that would infer topics for us without us having to read all of our documents first. The approach that we will take is a technique called topic modeling , a computational method that allows you to discover the topics that construct a text. Topic modeling does so by exercising a variety of statistical protocols over and over again on a text. Topic modeling involves some complex statistics, so the following description might seem a bit hand-wavey. Topic modeling software looks for words that tend to occur next to each in statistically significant ways. The sum total of these words that occur next to each other becomes legible to a reader as a topic. Unlike the human brain, topic modeling software can process hundreds of thousands of texts, over and over again, refining its sense of how all the pieces fit together. It can give us a sense of the themes and discourses that run beneath an entire corpus. In the following example, you can see a list of 10 topics from a corpus of 42 narratives by enslaved women, taken from the North American Slave Narratives collection and generated using the Topic Modeling Tool . 0. aunt dice thy mos master sam riverside john ye soul eyes phillis er river mistress thee evelyn song trevor thine 1. sojourner truth god people good isabella de mother years lord mind slavery son meeting washington time colored church master jesus 2. time life made day house great man long mr home found children give woman place work make friend called heard 3. mrs lincoln harriet mr mother de people president time war years woman white south letter received dress york room millie 4. mother master peter children mr free told slave good man slaves young day morning time don people mistress night house 5. miss de mr ann jane good young face master peterkin henry eyes don ll amy white polly room poor yer 6. negro colored years women men race war country people children free miss john slaves mrs white made slavery south negroes 7. good god years poor mind toussaint master heart love received jesus great thy read people saviour year thou lived man 8. lord god time people good church told day thought brother meeting home night mrs work sister man mother morning pray 9. church school work teacher people year time years conference good philadelphia members number great class colored god young early st So when you run topic modeling software, it looks for words that occur near each other in texts in meaningful ways over the course of the corpus. In most cases, it looks for words that occur in documents together. Remember, these words are not dependent on their location within the document. Topic modeling works on a bag of words model that only cares about whether or not the words occur within the text, not their position within it. But you might occasionally chunk larger documents into a series of paragraphs so that the software thinks about them each as separate documents for finer granularity. There are a number of similar tricks for refining your processes. Until now, we have stressed approaching text analysis with a clear sense of your interests and the research questions that drive them. Topic modeling works a little differently: it is more useful for exploratory work. We call topic modeling unsupervised classification because we are asking the computer to analyze and mark a text without giving it any clear directions. We just say, \u201chere is some text. Do your thing and tell me what you find.\u201d A supervised classifier would take information from us to help it make decisions. We might say, \u201cread this text. If it has more than fifty uses of the word \u2018crime\u2019 mark it as \u2018detective fiction.\u2019 If it has fifty uses of the word \u2018love,\u2019 mark it as \u2018romance\u2019\u201d). Unsupervised classifiers like topic modeling instead know very little about the underlying texts that they are examining. Instead, they process them based on an underlying model. In the last section we called the bag of words model an epistemology of texts, a way of understanding documents that might be different from what you were familiar with. In the case of the kind of topic modeling we have been discussing, that model could further be called Latent Dirichlet Allocation (LDA). We won\u2019t go into any detail about the specifics of LDA, but it is important to know that this is the model you are working with and that LDA assumes that a text is constructed from a small number of topics. Sentiment Analysis # As we learn more complex ways to analyze texts, you might find yourself wondering: Is a particular text happy or sad? For that matter, is a sentence? A word? This type of analysis that tries to capture the emotional resonance of a text is called sentiment analysis . You\u2019ve probably engaged with this kind of work without realizing it. If you\u2019ve ever been to Rotten Tomatoes to see what score a movie has gotten, you are looking at an aggregated number of reviews that have been marked as positive or negative. Businesses have a stake in such things as well. If you tweet about your recent flight, the airline would probably want to know whether you hated it or loved it. The former might result in you being directed to customer service, while the latter could result in a benign response like \u201cthanks for flying with us!\u201d Sentiment analysis can also offer interesting opportunities for textual analysis. t would be fascinating to have a computer that could easily mark the sentiments in texts for you. If you have been following dutifuly along, however, you should know that computers can\u2019t do much of anything without being explicitly told how. They can do very little in the way of understanding data without a human to guide them. Trying to extract complicated information like the sentimental arc of a text, how we are meant to feel about a sentence, or how an author intended us to feel are all complicated tasks that computers have a difficult time with. In fact, they can be hard for two different people to agree on. Try to guess whether these two sentences would be classified as good or bad: \u201cI am very happy.\u201d \u201cShe is so sad.\u201d Those were easy ones: good and bad. Hot and cold. How about this one: \u201cIt was the best of times, it was the worst of times\u2026\u201d This sentence is from Charles Dickens\u2019s Tale of Two Cities and is probably a bit hard to parse in such a binary way. If it is both good and bad, it probably comes out as neutral, right? But Dickens was talking about the era of the French Revolution here; his whole point was that this was an extraordinary time, hardly a \u201cneutral\u201d situation. In fact, he is interested in juxtaposing different things - best/worst, London/Paris, etc. - not in resolving them. We would probably need some system for determining what to do in such situations. Try this sentence, by Jane Austen, which complicates matters even further: \u201cIt is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\u201d An avid reader of Austen would know that her texts come loaded with satire. It is unlikely she actually means her words to be taken at face value. Virtually no truths actually are universally acknowledged to be true, so the sentence winks at the reader and should not be taken in full seriousness. In fact, much of her work is meant as a scathing criticism of the culture and people around her. These opinions are largely indirect, couched in irony and satire that asks the reader to read against what the text says on the surface. All of these things are difficult to convey to readers, let alone computers. Sentiment analysis through technology is tricky, but that doesn\u2019t mean that researchers don\u2019t try. The process is difficult and riddled with error, but also intellectually interesting in a number of ways. How do we map complicated abstract ideas like emotion in a way that computers could understand them? What can sentiment analysis like this tell us about the objects that we study? Building your corpus # In text analysis lingo, corpus is another word for your data set. A corpus is a collection or body of texts with certain boundaries, usually determined by the researcher. A corpus may be the entire run of a newspaper or literary works by African-American authors . But where does the text come from? There are a lot of existing places to search for existing corpora (the plural of corpus, a lovely word). Libraries, archives, and museums have been sharing their collections online for a long time, but have only recently started making it easy for researchers to use those collections in a computational way. Some institutions build tools for you to use, or offer their data as a download . You might also be able to contact an institution to see what they could make available. If you are not finding a pre-packaged corpus available, you have options for creating your own. If you're working from print material, you will need to scan the pages and extract the text through a process known as Optical Character Recognition . Be warned, this could be a lot of work! If you want to work with something that is already digital, but not well organized, there are tools for scraping information from the Web. In most cases, your corpus will consist of a collection of text files in a folder or series of folders. Since we've established that the computer doesn't really care about styling when it comes to processing documents, using the .txt. format is best. In most cases, you will want some identifying metadata in your file names so you can identify the file in a graph or other visualization. You'll want to generate a file naming convention that highlights what is unique about each document (date, name, etc). In the Data section, we talked about how computers use file names to sort content. If you want your text files to appear in chronological order, you should begin your file names with a year. Copyright # One final note on copyright. Creative works published after 1925 (as of 2020) are within copyright, meaning you do not have the right to do anything you want with them. The principles of fair use allow you to use those works for academic purposes, but that does not usually extend to republishing them in full. If you wanted to perform text analysis on a recently published novel, you might have to generate that text yourself, and then only share your results, not the corpus itself. Fortunately, materials published before 1925 are available for use! You can find texts in places like Project Gutenburg or the Public Domain Review . Text Analysis Tools # Voyant - A robust and popular tool for conducting many types of text analysis. FYI, you can run Voyant locally on your own computer. This is useful for large datasets or times when Voyant is slow. Lexos - This tool from Wheaton College performs some powerful pre-processing of your corpus for text analysis. AntConc - Software package for linguistic analysis of texts, particularly concordance work. Google Books Ngram Viewer - A popular tool for visualizing n-grams over time in the Google Books data set. Prof. Brandon Walsh delves into the nuances of the Ngram Viewer over on his text analysis workbook . You can apply the same technology to your own corpus with a took called Bookworm . MALLET - Stands for MAchine Learning for LanguagE Toolkit. It's a powerful command line tool for topic modeling. Scott Enderle's Topic Modeling Tool - is an option for topic modeling if you don't want to use the command line. It creates nice HTML pages for interacting with the data. NLTK - Python library containing a number of text analysis tools. If you feel comfortable with the command line or have programming experience, you might want to explore this option. Check out the Steinheil Affair project on GitHub for an example of what can be with NLTK. You might also be interested in A Humanist's Cookbook for Natural Language Processing with Python . Activities # Activity 7.1 - 4 Ways of Reading a Text # In this activity, we'll work through Prof. Brandon Walsh's \"4 Ways of Reading a Text\" lecture. To prepare, please print one copy of this handout packet . Activity 7.2 - Voyant # Let's spend some time getting comfortable with Voyant Tools . Visit Project Gutenberg and find a text you know well. Maybe it's one you read in a class or one that's just a personal favorite. Download the plain text version of your selected book. In Voyant Tools , upload the text file you just downloaded. Use the little window icon to change the tool being displayed in each window. Answer the following questions. For each question, identify the visualization tool you used to find the answer. Sometimes there are more than one way to visualize the results. Which methods work bets? What are the most common words in your text? The least common? Search for a meaningful word in the Trends tool. How is that word used over the course of your text? What are the topics of your text? Which tool did you use to find the topics? Which tools help you find connections between words? What is the weirdest visualization you can find? Which visualization seems totally bizarre and not helpful? Why? How do you export your results? What types of questions can you form about your text with these tools? Activity 7.3 # Now that you're comfortable with Voyant , let's return to the Ring-tum Phi and compare it to another corpus: the Alumni Magazine . Select similar portions of time from both the Ring-tum Phi and the Alumni Magazine. You probably don't want to start with the entire decade - it's often best to start with a small bit of data and move up from there. Assemble your corpus. Now that you're familiar with Voyant, what makes the most sense for each file? How many years should go in each file aka what is the \"granularity?\" How do you need to structure the corpus to balance the granularity of each file? IE: if one text file contains an entire year and another file contains just one month, how will that affect your graph? Upload your corpus to Voyant. To add a multiple files, find the Documents tab in the lower left corner. Press the Modify button, then Add, then Upload, to add more text files to Voyant. Make sure your files are named in a way so that they will be easy to distinguish from one another. Try out the various visualizations in Voyant. Which work best for a comparison between these two corpora? What can you learn about how these two publications cover the same topics? How might you use text analysis methods in your own project? Select three of the most meaningful visualizations and include them in a post along with 400-500 on the experience of text analysis, the meaning in your visualizations, and the answers to the questions in step 4. Projects # Mining the Dispatch The Proceedings of the Old Bailey Quantifying Kissinger Dash Amerikan Robots Reading Vogue A Topic Model of Literary Studies Journals America's Public Bible Resources # Corpus Analysis with AntConc","title":"Text Analysis"},{"location":"text-analysis/#close-reading","text":"Text analysis is something that we all engage in, whether we realize it or not. The term is broad and capacious and encapsulates a variety of different activities. Even something as simple as slowing down when you see a stop sign is a kind of text analysis: doing so means you have parsed the meaning of the words on the sign and reacted accordingly. Indeed any of the following, related activities are forms of text analysis: Paraphrasing a text Searching for hidden meanings in a text Adapting a text and reflecting on it Examining the details in a text This last point is worth pausing over: close reading, in particular, is often proclaimed as one of the primary analytical tool of scholars and students in the humanities. To read closely means to give careful attention to the various components that make up a text, ones which cause us to think or feel a certain way about it. Close reading relies on a core principle about the text under study: Everything about the text matters, whether the author intended for it to matter or not. Consider the following thought experiment. One day you come home to find the following note from your roommate on the counter: took care of these dishes? Thanks. Next to the note: dirty dishes. Was your roommate in a hurry and actually asking you to wash dishes? Or were they sarcastically trying to give you grief for not having done your part? Lots of questions. To imagine responses to them you might employ a range of assumptions and interpretations depending on the scenario: Context: you have been growing more and more irritated with your roommate for some time now. Their actions just really get under your skin: dirty dishes, laundry everywhere, the works. They clearly meant the note as an insult. Author: your roommate is actually a great person and would never leave a passive aggressive note. In fact, they probably meant it as a joke. Text: Take a look at that question mark. And then the curt second sentence. Your roommate put those things there on purpose to be rude. The list could go on and on. We employ a similar range of skills when we read anything, be it fiction, poetry, or historical documents. Close reading might be best described as an activity in which a reader simply lets no detail of the text go unquestioned. The best way at approaching a good close reading is by asking (and attempting to answer) questions about every piece of a text.","title":"Close Reading"},{"location":"text-analysis/#how-computers-read","text":"We've talked about how computers and humans have their respective skills. With respect to text analysis, we can say that computers and humans have complementary skills. Computers are good at doing things that would take us a long time to do or that would be incredibly tedious. Computers can easily count and compare and will do so for pretty much as long as you tell them to do so. In contrast, humans are very good at understanding nuance and context. Thus, you wouldn\u2019t want a computer to do any close reading, or unpack the claims of a primary or secondary text; this is something you are far better at. By the same token, it\u2019s probably easier to have a computer list all the numbers between one and 45678987 than to do it yourself. Before we start analyzing texts, we need to know how computers understand text. Consider the following sentence: \u201cWe saw 8 1/2.\u201d Taken alone, the sentence doesn\u2019t tell us much. Its meaning depends a lot on the question to which we might be responding, and we can think of two possible questions with very different contexts: \u201cHow many movies did you see? \u201cWhat movie did you see?\u201d In the first case, we might be responding with the number of movies that we had seen. It was a slow weekend, and we spent it at the local movie theater hopping from film to film. It was a great time! In the second situation, we might be responding with the title of a specific film, 8 1/2 by Italian director Frederico Fellini. So one answer is a number, and one answer is a name. Since humans are good at grasping context, we would easily be able to distinguish between the two. In most situations, we would just adjust our understanding internally before moving on with the conversation. Computers cannot make inferences like these, and this fact has serious implications: numbers and words have significantly different uses. Here are two further extensions of the conversation: If you add four to how many movies you saw, what is the result? If we were talking about a number of movies, my response would clearly be, \u201cOh that\u2019s 12.5. Why are you giving me a math quiz?\u201d If we were talking about the Fellini film, we might respond, \u201cWhat? Oh, we were talking about a title, not a number. We can\u2019t add things to a title.\u201d Again, humans have the ability to respond to context, infer, and adapt. Computers aren\u2019t nearly as flexible: they need to know ahead of time, in most cases, what kind of information they are dealing with. That way they can act as you anticipated. We learned earlier about the concept of \"data types.\" Well now they matter! In the above example, we encountered: Strings: characters, the stuff of words Integers: a whole numbers The distinction between strings and integers is important for text analysis. You can perform arithmetic operations on integers while strings respond less well to such things. You can capitalize words, but not numbers. And computers generally want you to deal with similar objects: you can combine strings (words can become sentences) or add numbers, but trying to combine a string and an integer will break things. To be clear to the computer which data type we mean, we use can indicate strings with \"quote\" marks and integers without. 8 vs. \"8\" . In response, the computer wants us to to know that it doesn't know that \"8\" is related to \"Eight\" or \"Eighth\" or even \"eight\" . It might seem surprising that computers aren't smarter than that, but it brings us to one of the first steps in text analysis: tokenization, or the process by which we break apart all of the words and punctuation in a text into separate tokens. \"I love you\" becomes \"I\", \"love\", \"you\", \".\". We can break things down even further once we\u2019ve divided a text into individual words. While we often care about how many times each particular token or word occurs, we might also care about the different kinds of words. We might want to keep track, on the one hand, of all the different words in a text regardless of how often they occur. But we might also want a different kind of vocabulary list. Rather than counting all the words, we might just want to grab a single example of each token type. If we have the following document: Test test test sentence sentence We have five tokens and two types (\u2018test\u2019 and \u2018sentence\u2019). A list of types might be good for getting a sense of the kinds of language used in a text, while a raw list of tokens could be useful for figuring out what kinds of words occur in which proportions. Depending on our research questions and interests, statistics like these can help us figure out what the document discusses as well as how it is being discussed. \u201cBut wait,\u201d you say, \u201ccomputers care about capitalization. So if we tokenize a text and try to compare \u2018word\u2019 and \u2018Word\u2019 they will think they are entirely different things!\u201d Good catch! You\u2019re right, those differences in capitalization often aren\u2019t meaningful. It is a fairly common practice to lowercase all the words after you tokenize them. This process is often called normalizing the data, since we are smoothing out inconsistencies that might get in the way of our research questions. This whole collection of processes of segmentation, tokenization, and normalization has a name of its own as well: preprocessing, all those things you do to data before you work with it. Depending on your interests, you might include other steps, such as tagging tokens for parts of speech or filtering out particular types of words. Note that preprocessing can change your list of types. A computer would not recognize \u201cSaid\u201d and \u201csaid\u201d as being of the same type, but, if you normalize capitalization so that every token is lowercased, a computer would see them as the same word. So you often need to decide what pieces you care about at the beginning of the process.","title":"How Computers Read"},{"location":"text-analysis/#bag-of-words","text":"When we read, our eyes move in sequence across the page and take in phrase after phrase in the order in which they were intended. This sense of chronology is integral to how we, as human readers, understand texts. But it is possible to imagine other ways of reading. Have you ever skimmed over a page backwards looking at every other word? You probably still got the gist of the text even though you didn\u2019t read it in order and even though you missed many of the words. If we take the words in a text as being indicative of its underlying topics, we actually don\u2019t need to worry about word order so much. The sequence of words, sometimes called the syntagmatic axis , only matters for certain kinds of reading. But we can find out interesting things about texts if we are a little more flexible and think about them not as things that unfold over time but rather as pure token counts, as bags of words. In a bag of words model , word order becomes irrelevant. All we care about is what words occur in a text and how often they do so. You can use a bag of words approach to determine how different or similar whole books or authors are from each other. If we have lists of words for each text as well as for the corpus (or set of documents) as a whole, we can actually work backwards to get a sense of the underlying topics that we were talking about a moment ago. Instead of skimming a paragraph to determine its basic topic, we could scan full texts \u2013 and scan lots of them. And rather than trying to get a sense of 1-3 topics, we could break our text apart into 15-20 different topics. Now we are cooking with gas, and we\u2019re talking about topic modeling.","title":"Bag of words"},{"location":"text-analysis/#topic-modeling","text":"Given enough time and energy, we can imagine a tool that would infer topics for us without us having to read all of our documents first. The approach that we will take is a technique called topic modeling , a computational method that allows you to discover the topics that construct a text. Topic modeling does so by exercising a variety of statistical protocols over and over again on a text. Topic modeling involves some complex statistics, so the following description might seem a bit hand-wavey. Topic modeling software looks for words that tend to occur next to each in statistically significant ways. The sum total of these words that occur next to each other becomes legible to a reader as a topic. Unlike the human brain, topic modeling software can process hundreds of thousands of texts, over and over again, refining its sense of how all the pieces fit together. It can give us a sense of the themes and discourses that run beneath an entire corpus. In the following example, you can see a list of 10 topics from a corpus of 42 narratives by enslaved women, taken from the North American Slave Narratives collection and generated using the Topic Modeling Tool . 0. aunt dice thy mos master sam riverside john ye soul eyes phillis er river mistress thee evelyn song trevor thine 1. sojourner truth god people good isabella de mother years lord mind slavery son meeting washington time colored church master jesus 2. time life made day house great man long mr home found children give woman place work make friend called heard 3. mrs lincoln harriet mr mother de people president time war years woman white south letter received dress york room millie 4. mother master peter children mr free told slave good man slaves young day morning time don people mistress night house 5. miss de mr ann jane good young face master peterkin henry eyes don ll amy white polly room poor yer 6. negro colored years women men race war country people children free miss john slaves mrs white made slavery south negroes 7. good god years poor mind toussaint master heart love received jesus great thy read people saviour year thou lived man 8. lord god time people good church told day thought brother meeting home night mrs work sister man mother morning pray 9. church school work teacher people year time years conference good philadelphia members number great class colored god young early st So when you run topic modeling software, it looks for words that occur near each other in texts in meaningful ways over the course of the corpus. In most cases, it looks for words that occur in documents together. Remember, these words are not dependent on their location within the document. Topic modeling works on a bag of words model that only cares about whether or not the words occur within the text, not their position within it. But you might occasionally chunk larger documents into a series of paragraphs so that the software thinks about them each as separate documents for finer granularity. There are a number of similar tricks for refining your processes. Until now, we have stressed approaching text analysis with a clear sense of your interests and the research questions that drive them. Topic modeling works a little differently: it is more useful for exploratory work. We call topic modeling unsupervised classification because we are asking the computer to analyze and mark a text without giving it any clear directions. We just say, \u201chere is some text. Do your thing and tell me what you find.\u201d A supervised classifier would take information from us to help it make decisions. We might say, \u201cread this text. If it has more than fifty uses of the word \u2018crime\u2019 mark it as \u2018detective fiction.\u2019 If it has fifty uses of the word \u2018love,\u2019 mark it as \u2018romance\u2019\u201d). Unsupervised classifiers like topic modeling instead know very little about the underlying texts that they are examining. Instead, they process them based on an underlying model. In the last section we called the bag of words model an epistemology of texts, a way of understanding documents that might be different from what you were familiar with. In the case of the kind of topic modeling we have been discussing, that model could further be called Latent Dirichlet Allocation (LDA). We won\u2019t go into any detail about the specifics of LDA, but it is important to know that this is the model you are working with and that LDA assumes that a text is constructed from a small number of topics.","title":"Topic Modeling"},{"location":"text-analysis/#sentiment-analysis","text":"As we learn more complex ways to analyze texts, you might find yourself wondering: Is a particular text happy or sad? For that matter, is a sentence? A word? This type of analysis that tries to capture the emotional resonance of a text is called sentiment analysis . You\u2019ve probably engaged with this kind of work without realizing it. If you\u2019ve ever been to Rotten Tomatoes to see what score a movie has gotten, you are looking at an aggregated number of reviews that have been marked as positive or negative. Businesses have a stake in such things as well. If you tweet about your recent flight, the airline would probably want to know whether you hated it or loved it. The former might result in you being directed to customer service, while the latter could result in a benign response like \u201cthanks for flying with us!\u201d Sentiment analysis can also offer interesting opportunities for textual analysis. t would be fascinating to have a computer that could easily mark the sentiments in texts for you. If you have been following dutifuly along, however, you should know that computers can\u2019t do much of anything without being explicitly told how. They can do very little in the way of understanding data without a human to guide them. Trying to extract complicated information like the sentimental arc of a text, how we are meant to feel about a sentence, or how an author intended us to feel are all complicated tasks that computers have a difficult time with. In fact, they can be hard for two different people to agree on. Try to guess whether these two sentences would be classified as good or bad: \u201cI am very happy.\u201d \u201cShe is so sad.\u201d Those were easy ones: good and bad. Hot and cold. How about this one: \u201cIt was the best of times, it was the worst of times\u2026\u201d This sentence is from Charles Dickens\u2019s Tale of Two Cities and is probably a bit hard to parse in such a binary way. If it is both good and bad, it probably comes out as neutral, right? But Dickens was talking about the era of the French Revolution here; his whole point was that this was an extraordinary time, hardly a \u201cneutral\u201d situation. In fact, he is interested in juxtaposing different things - best/worst, London/Paris, etc. - not in resolving them. We would probably need some system for determining what to do in such situations. Try this sentence, by Jane Austen, which complicates matters even further: \u201cIt is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\u201d An avid reader of Austen would know that her texts come loaded with satire. It is unlikely she actually means her words to be taken at face value. Virtually no truths actually are universally acknowledged to be true, so the sentence winks at the reader and should not be taken in full seriousness. In fact, much of her work is meant as a scathing criticism of the culture and people around her. These opinions are largely indirect, couched in irony and satire that asks the reader to read against what the text says on the surface. All of these things are difficult to convey to readers, let alone computers. Sentiment analysis through technology is tricky, but that doesn\u2019t mean that researchers don\u2019t try. The process is difficult and riddled with error, but also intellectually interesting in a number of ways. How do we map complicated abstract ideas like emotion in a way that computers could understand them? What can sentiment analysis like this tell us about the objects that we study?","title":"Sentiment Analysis"},{"location":"text-analysis/#building-your-corpus","text":"In text analysis lingo, corpus is another word for your data set. A corpus is a collection or body of texts with certain boundaries, usually determined by the researcher. A corpus may be the entire run of a newspaper or literary works by African-American authors . But where does the text come from? There are a lot of existing places to search for existing corpora (the plural of corpus, a lovely word). Libraries, archives, and museums have been sharing their collections online for a long time, but have only recently started making it easy for researchers to use those collections in a computational way. Some institutions build tools for you to use, or offer their data as a download . You might also be able to contact an institution to see what they could make available. If you are not finding a pre-packaged corpus available, you have options for creating your own. If you're working from print material, you will need to scan the pages and extract the text through a process known as Optical Character Recognition . Be warned, this could be a lot of work! If you want to work with something that is already digital, but not well organized, there are tools for scraping information from the Web. In most cases, your corpus will consist of a collection of text files in a folder or series of folders. Since we've established that the computer doesn't really care about styling when it comes to processing documents, using the .txt. format is best. In most cases, you will want some identifying metadata in your file names so you can identify the file in a graph or other visualization. You'll want to generate a file naming convention that highlights what is unique about each document (date, name, etc). In the Data section, we talked about how computers use file names to sort content. If you want your text files to appear in chronological order, you should begin your file names with a year.","title":"Building your corpus"},{"location":"text-analysis/#copyright","text":"One final note on copyright. Creative works published after 1925 (as of 2020) are within copyright, meaning you do not have the right to do anything you want with them. The principles of fair use allow you to use those works for academic purposes, but that does not usually extend to republishing them in full. If you wanted to perform text analysis on a recently published novel, you might have to generate that text yourself, and then only share your results, not the corpus itself. Fortunately, materials published before 1925 are available for use! You can find texts in places like Project Gutenburg or the Public Domain Review .","title":"Copyright"},{"location":"text-analysis/#text-analysis-tools","text":"Voyant - A robust and popular tool for conducting many types of text analysis. FYI, you can run Voyant locally on your own computer. This is useful for large datasets or times when Voyant is slow. Lexos - This tool from Wheaton College performs some powerful pre-processing of your corpus for text analysis. AntConc - Software package for linguistic analysis of texts, particularly concordance work. Google Books Ngram Viewer - A popular tool for visualizing n-grams over time in the Google Books data set. Prof. Brandon Walsh delves into the nuances of the Ngram Viewer over on his text analysis workbook . You can apply the same technology to your own corpus with a took called Bookworm . MALLET - Stands for MAchine Learning for LanguagE Toolkit. It's a powerful command line tool for topic modeling. Scott Enderle's Topic Modeling Tool - is an option for topic modeling if you don't want to use the command line. It creates nice HTML pages for interacting with the data. NLTK - Python library containing a number of text analysis tools. If you feel comfortable with the command line or have programming experience, you might want to explore this option. Check out the Steinheil Affair project on GitHub for an example of what can be with NLTK. You might also be interested in A Humanist's Cookbook for Natural Language Processing with Python .","title":"Text Analysis Tools"},{"location":"text-analysis/#activities","text":"","title":"Activities"},{"location":"text-analysis/#activity-71-4-ways-of-reading-a-text","text":"In this activity, we'll work through Prof. Brandon Walsh's \"4 Ways of Reading a Text\" lecture. To prepare, please print one copy of this handout packet .","title":"Activity 7.1 - 4 Ways of Reading a Text"},{"location":"text-analysis/#activity-72-voyant","text":"Let's spend some time getting comfortable with Voyant Tools . Visit Project Gutenberg and find a text you know well. Maybe it's one you read in a class or one that's just a personal favorite. Download the plain text version of your selected book. In Voyant Tools , upload the text file you just downloaded. Use the little window icon to change the tool being displayed in each window. Answer the following questions. For each question, identify the visualization tool you used to find the answer. Sometimes there are more than one way to visualize the results. Which methods work bets? What are the most common words in your text? The least common? Search for a meaningful word in the Trends tool. How is that word used over the course of your text? What are the topics of your text? Which tool did you use to find the topics? Which tools help you find connections between words? What is the weirdest visualization you can find? Which visualization seems totally bizarre and not helpful? Why? How do you export your results? What types of questions can you form about your text with these tools?","title":"Activity 7.2 - Voyant"},{"location":"text-analysis/#activity-73","text":"Now that you're comfortable with Voyant , let's return to the Ring-tum Phi and compare it to another corpus: the Alumni Magazine . Select similar portions of time from both the Ring-tum Phi and the Alumni Magazine. You probably don't want to start with the entire decade - it's often best to start with a small bit of data and move up from there. Assemble your corpus. Now that you're familiar with Voyant, what makes the most sense for each file? How many years should go in each file aka what is the \"granularity?\" How do you need to structure the corpus to balance the granularity of each file? IE: if one text file contains an entire year and another file contains just one month, how will that affect your graph? Upload your corpus to Voyant. To add a multiple files, find the Documents tab in the lower left corner. Press the Modify button, then Add, then Upload, to add more text files to Voyant. Make sure your files are named in a way so that they will be easy to distinguish from one another. Try out the various visualizations in Voyant. Which work best for a comparison between these two corpora? What can you learn about how these two publications cover the same topics? How might you use text analysis methods in your own project? Select three of the most meaningful visualizations and include them in a post along with 400-500 on the experience of text analysis, the meaning in your visualizations, and the answers to the questions in step 4.","title":"Activity 7.3"},{"location":"text-analysis/#projects","text":"Mining the Dispatch The Proceedings of the Old Bailey Quantifying Kissinger Dash Amerikan Robots Reading Vogue A Topic Model of Literary Studies Journals America's Public Bible","title":"Projects"},{"location":"text-analysis/#resources","text":"Corpus Analysis with AntConc","title":"Resources"},{"location":"visualization/","text":"In this section, we'll learn the basics of data visualization. What is data visualization? Why visualize? What makes a good visualization? Types of visualizations How do I do it? Activities Activity 4.1 Activity 4.2 Activity 4.3 Excel + Pivot Tables Resources Readings What is data visualization? # Data visualization is everywhere. You can hardly turn on the tv, open a book or a new tab without seeing data being visualized. Infographics are prominent examples of data visualization. Using images to represent data can help people translate the information into something they better understand. Commons forms of data visualizations are charts and graphs but maps are also data visualization. Infact, geographic data is so specialized that we will spend an entire unit focusing just on mapping. Can you think of a favorite example of good data visualization? What about a terrible example? What made the visualization work? How did you feel when you realized the visualization was flawed? The Historian's Macroscope defines data visualization like this: \"A method of deforming, compressing, or otherwise manipulating data in order to see it in new and enlightening ways. A good visualization can turn hours of careful study into a flash of insight, or can convey a complex narrative in a single moment... Visualizations can also lie, confuse, or otherwise misrepresent if used poorly.\" - The Historian's Macroscope Visualization has been happening in the humanities for a long time, well before computers. Charles Minard's 1869 chart showing the number of men in Napoleon\u2019s 1812 Russian campaign army is often cited as an early example. Charles Joseph Minard If you're interested in the history of information visualization, look up Edward Tufte and his work, The Visual Display of Quantitative Information . Why visualize? # It is easy to fall into a trap of visualizing data because you can. Data visualization is new and shiny and looks great on a Powerpoint slide in front of an audience. Before you fill up that slidedeck, take some time to evaluate why you're visualizing. Are there aspects of the data that are confusing or difficult to describe? Does the visualization reinforce your point? Does it show something in a new way? Have you selected the right visualization for what you're trying to share? Visualizations can be powerful, just make sure you're using them for good. Before you get to the point of communicating your findings to an audience, data visualization can play a key role in helping you understand and assess your data. Visualization tools can test your assumptions about your data or draw attention to errors you might have missed. If you're dealing with a 19th century topic, but the first entry in time is in the 17th century, you may have a 6 instead of a 9 somewhere in your data. You should not hesitate to visualize your data before it's done being collected or cleaned. It's very likely that the visualization will offer up a new way of thinking about your methods. What makes a good visualization? # Think back to my question about a favorite example of good visualization? What made it good? How did you know it was effective? Maybe it's easier to think about a bad visualization and reverse engineer. This is a laughably bad visualization. But why? Take a second to make a list. Mine looks like this: distracting bananas 3D columns, they obscure each other, not to mention I can't tell the quantity they represent. years are already on an axis, do I need the colors? And the legend? country names are hard to read. colors are very similar From this list, I could speculate that principles of good visualization might be: minimal decoration. flat, not three-dimensional. clear, legible labels. color choices that take into account the prevalence of color blindness . appropriate chart type. balanced scale and axises. meaningful legend and explanatory title and text. Good visualizations are built with accessibility issues in mind and improve the value of the visualization for everyone. Read accessible data viz is better data viz and consider the principles it suggests when you are evaluating and creating visualizations. I walk you through this exercise in hopes that you can use it on other visualizations you encounter. It's easy to look at a list of best practices, it's a little more challenging to apply those practices to your own work or the work of others. Types of visualizations # There are many types of charts and options for visualizing data. Here is a quick list of the most common, along with links to more thorough lists. Bar or column chart - useful for comparison of values across categories. Heatmap - uses color variance to show difference in values. Often seen in maps. Infographics - can include data visualizations, but mostly just present info in a visual way. Line graph - shows quantitative values over time or other interval. Pie graph - Show proportions of a whole. Scatterplot - Data points are arranged by relationship to two variables. Tree map - shows hierarchy and size of each value. Data Visualization Catalogue is a great resource for learning more about chart types. It allows you to select by chart type of type of data being visualized. How do I do it? # There is no one perfect tool for data visualization. Spreadsheet software like Excel and Google Sheets, as well as programming languages or analysis tool, all have their own methods for creating visualizations. You may also find standalone tools that take your data and produce visualizations. The nature of your project, your data, and the goals of your visualizations will dictate the tools you use. If you're just starting out, use Excel or Google Sheets to get comfortable. If you're ready for the next level, try RAW Graphs . Tableau Public is the free version of what is quickly becoming industry standard software. If you feel ready for advanced work, check out D3.js , a JavaScript library for creating data-driven documents. Activities # Activity 4.1 # Before you start making your own visualizations, let's practice improving on existing visualizations. Visit viz.wtf or HelpmeViz . Browse the examples, then pick 1-2 visualizations to analyze as a group. Make a list of what's wrong with the visualization. Be specific! What was the goal of the visualization? How has it failed to meet that goal? Record your results in a Boxnote in our shared Box folder . How can this visualization be improved? Be specific! Activity 4.2 # Let's also make sure you can think critically about professional visualizations that are designed to persuade. Go out into the wilds of the internet and find a visualization that speaks to you. It's election season! You shouldn't have any trouble. What is this visualization trying to convey? Is it successful? Why or why not? Does the style suit the data? Who is the audience and what is the context? What is source of the data of this visualization? Can you find the original data set? What format is it in? Can you attempt to recreate the visualization? Or make a better version? Activity 4.3 # Now it's time to try your hand at creating your own visualization. Let's start with the Coeducation Report data set. You can use this assembled data set or work on the one we assembled in Google Sheets . Don't forget to download it to your own computer, so you don't alter the data for anyone else. Brainstorm some potential visualizations. What do you want to communicate about this data set? What's interesting about it? How can a visualization help show an intriguing or important part of this data? Narrow your list down to one or two ideas. What type of visualizations are you interested in using and how will they help you get at the information you want to convey? It might help to draw your viz on paper first so you have an idea of what you want to create. Use Excel (see next section for a short tutorial) or Google Sheets to create a basic graph. Are you happy with it? Why or why not? How do different types of graphs change your understanding of the data? Try creating the same graph in both programs to see if there are any differences. If you're still not happy with your graph, try Raw Graphs . There are some wild options, is your graph still legible? For your blog post this week, go through this same process with the Cemetery data set. You may have to clean up fields in Open Refine or Excel to get the results you're looking for. Post several visualizations, along with a 300-400 word blog post on your results. What is your goal with these visualizations? How do they help you understand the data? How did you put these graphs together? Did you have to clean the data or look up how to create the graph? What did you struggle with? Are you happy with the results? What might you want to do with more time/skills? Excel + Pivot Tables # To create a graph in Excel, select all the columns in your data set. From the Insert tab, select PivotChart. You can accept the defaults to open a new tab in your spreadsheet. The new tab will have a window called PivotTable fields. It contains a list of fields, then four windows: Filters, Legend (Series), Axis (Categories), and Values. Dragging fields into these categories will create your graph. You can drag the same field into multiple sections or drag multiple fields into different sections to alter the graph. For example, I can create a very basic bar graph that shows how many people support/opppose/don't care about coeducation by dragging the \"opinion\" field into the Values box (so it counts the number of entries) and the Axis box (so it shows the different answers). What happens when I drag opinions into the Legend box instead? Resources # ColorBrewer - tool for creating distinct color palettes Data Viz Catalogue Raw Graphs Readings # Data + Design: A simple introduction to preparing and visualizing information The Historian's Macroscope - Making Your Data Legible: A Basic Introduction to Visualizations and subsequent sections The Pudding - data-driven visual essays","title":"Visualization"},{"location":"visualization/#what-is-data-visualization","text":"Data visualization is everywhere. You can hardly turn on the tv, open a book or a new tab without seeing data being visualized. Infographics are prominent examples of data visualization. Using images to represent data can help people translate the information into something they better understand. Commons forms of data visualizations are charts and graphs but maps are also data visualization. Infact, geographic data is so specialized that we will spend an entire unit focusing just on mapping. Can you think of a favorite example of good data visualization? What about a terrible example? What made the visualization work? How did you feel when you realized the visualization was flawed? The Historian's Macroscope defines data visualization like this: \"A method of deforming, compressing, or otherwise manipulating data in order to see it in new and enlightening ways. A good visualization can turn hours of careful study into a flash of insight, or can convey a complex narrative in a single moment... Visualizations can also lie, confuse, or otherwise misrepresent if used poorly.\" - The Historian's Macroscope Visualization has been happening in the humanities for a long time, well before computers. Charles Minard's 1869 chart showing the number of men in Napoleon\u2019s 1812 Russian campaign army is often cited as an early example. Charles Joseph Minard If you're interested in the history of information visualization, look up Edward Tufte and his work, The Visual Display of Quantitative Information .","title":"What is data visualization?"},{"location":"visualization/#why-visualize","text":"It is easy to fall into a trap of visualizing data because you can. Data visualization is new and shiny and looks great on a Powerpoint slide in front of an audience. Before you fill up that slidedeck, take some time to evaluate why you're visualizing. Are there aspects of the data that are confusing or difficult to describe? Does the visualization reinforce your point? Does it show something in a new way? Have you selected the right visualization for what you're trying to share? Visualizations can be powerful, just make sure you're using them for good. Before you get to the point of communicating your findings to an audience, data visualization can play a key role in helping you understand and assess your data. Visualization tools can test your assumptions about your data or draw attention to errors you might have missed. If you're dealing with a 19th century topic, but the first entry in time is in the 17th century, you may have a 6 instead of a 9 somewhere in your data. You should not hesitate to visualize your data before it's done being collected or cleaned. It's very likely that the visualization will offer up a new way of thinking about your methods.","title":"Why visualize?"},{"location":"visualization/#what-makes-a-good-visualization","text":"Think back to my question about a favorite example of good visualization? What made it good? How did you know it was effective? Maybe it's easier to think about a bad visualization and reverse engineer. This is a laughably bad visualization. But why? Take a second to make a list. Mine looks like this: distracting bananas 3D columns, they obscure each other, not to mention I can't tell the quantity they represent. years are already on an axis, do I need the colors? And the legend? country names are hard to read. colors are very similar From this list, I could speculate that principles of good visualization might be: minimal decoration. flat, not three-dimensional. clear, legible labels. color choices that take into account the prevalence of color blindness . appropriate chart type. balanced scale and axises. meaningful legend and explanatory title and text. Good visualizations are built with accessibility issues in mind and improve the value of the visualization for everyone. Read accessible data viz is better data viz and consider the principles it suggests when you are evaluating and creating visualizations. I walk you through this exercise in hopes that you can use it on other visualizations you encounter. It's easy to look at a list of best practices, it's a little more challenging to apply those practices to your own work or the work of others.","title":"What makes a good visualization?"},{"location":"visualization/#types-of-visualizations","text":"There are many types of charts and options for visualizing data. Here is a quick list of the most common, along with links to more thorough lists. Bar or column chart - useful for comparison of values across categories. Heatmap - uses color variance to show difference in values. Often seen in maps. Infographics - can include data visualizations, but mostly just present info in a visual way. Line graph - shows quantitative values over time or other interval. Pie graph - Show proportions of a whole. Scatterplot - Data points are arranged by relationship to two variables. Tree map - shows hierarchy and size of each value. Data Visualization Catalogue is a great resource for learning more about chart types. It allows you to select by chart type of type of data being visualized.","title":"Types of visualizations"},{"location":"visualization/#how-do-i-do-it","text":"There is no one perfect tool for data visualization. Spreadsheet software like Excel and Google Sheets, as well as programming languages or analysis tool, all have their own methods for creating visualizations. You may also find standalone tools that take your data and produce visualizations. The nature of your project, your data, and the goals of your visualizations will dictate the tools you use. If you're just starting out, use Excel or Google Sheets to get comfortable. If you're ready for the next level, try RAW Graphs . Tableau Public is the free version of what is quickly becoming industry standard software. If you feel ready for advanced work, check out D3.js , a JavaScript library for creating data-driven documents.","title":"How do I do it?"},{"location":"visualization/#activities","text":"","title":"Activities"},{"location":"visualization/#activity-41","text":"Before you start making your own visualizations, let's practice improving on existing visualizations. Visit viz.wtf or HelpmeViz . Browse the examples, then pick 1-2 visualizations to analyze as a group. Make a list of what's wrong with the visualization. Be specific! What was the goal of the visualization? How has it failed to meet that goal? Record your results in a Boxnote in our shared Box folder . How can this visualization be improved? Be specific!","title":"Activity 4.1"},{"location":"visualization/#activity-42","text":"Let's also make sure you can think critically about professional visualizations that are designed to persuade. Go out into the wilds of the internet and find a visualization that speaks to you. It's election season! You shouldn't have any trouble. What is this visualization trying to convey? Is it successful? Why or why not? Does the style suit the data? Who is the audience and what is the context? What is source of the data of this visualization? Can you find the original data set? What format is it in? Can you attempt to recreate the visualization? Or make a better version?","title":"Activity 4.2"},{"location":"visualization/#activity-43","text":"Now it's time to try your hand at creating your own visualization. Let's start with the Coeducation Report data set. You can use this assembled data set or work on the one we assembled in Google Sheets . Don't forget to download it to your own computer, so you don't alter the data for anyone else. Brainstorm some potential visualizations. What do you want to communicate about this data set? What's interesting about it? How can a visualization help show an intriguing or important part of this data? Narrow your list down to one or two ideas. What type of visualizations are you interested in using and how will they help you get at the information you want to convey? It might help to draw your viz on paper first so you have an idea of what you want to create. Use Excel (see next section for a short tutorial) or Google Sheets to create a basic graph. Are you happy with it? Why or why not? How do different types of graphs change your understanding of the data? Try creating the same graph in both programs to see if there are any differences. If you're still not happy with your graph, try Raw Graphs . There are some wild options, is your graph still legible? For your blog post this week, go through this same process with the Cemetery data set. You may have to clean up fields in Open Refine or Excel to get the results you're looking for. Post several visualizations, along with a 300-400 word blog post on your results. What is your goal with these visualizations? How do they help you understand the data? How did you put these graphs together? Did you have to clean the data or look up how to create the graph? What did you struggle with? Are you happy with the results? What might you want to do with more time/skills?","title":"Activity 4.3"},{"location":"visualization/#excel-pivot-tables","text":"To create a graph in Excel, select all the columns in your data set. From the Insert tab, select PivotChart. You can accept the defaults to open a new tab in your spreadsheet. The new tab will have a window called PivotTable fields. It contains a list of fields, then four windows: Filters, Legend (Series), Axis (Categories), and Values. Dragging fields into these categories will create your graph. You can drag the same field into multiple sections or drag multiple fields into different sections to alter the graph. For example, I can create a very basic bar graph that shows how many people support/opppose/don't care about coeducation by dragging the \"opinion\" field into the Values box (so it counts the number of entries) and the Axis box (so it shows the different answers). What happens when I drag opinions into the Legend box instead?","title":"Excel + Pivot Tables"},{"location":"visualization/#resources","text":"ColorBrewer - tool for creating distinct color palettes Data Viz Catalogue Raw Graphs","title":"Resources"},{"location":"visualization/#readings","text":"Data + Design: A simple introduction to preparing and visualizing information The Historian's Macroscope - Making Your Data Legible: A Basic Introduction to Visualizations and subsequent sections The Pudding - data-driven visual essays","title":"Readings"},{"location":"what-is-humanities-data/","text":"In this section, we'll lay the groundwork for what we mean when we talk about \"humanities data.\" What is data? What are the humanities? What is humanities data? Why humanities data? What about digital humanities (DH)? Activities Activity 1.1 Resources Readings What is data? # What do you see in you mind's eye when someone says \"data?\" Data is one of those ubiquitous words that we use and see everywhere, but when it comes time to define it, we hesitate. It is used by so many people in so many contexts, it can be difficult to narrow down. If asked to share the first thing that comes to mind, you might say \"information,\" \"numbers,\" or \"facts.\" Often, data is defined by the verbs associated with it, such as this definition from Wikpedia: \"Data is measured, collected and reported, and analyzed, whereupon it can be visualized using graphs, images or other analysis tools. Data as a general concept refers to the fact that some existing information or knowledge is represented or coded in some form suitable for better usage or processing.\" - Wikipedia Data is understood as something we do something with. It is an object in motion, under manipulation. It is used to say something, to prove something, or to disprove something else. Data is supposed to hold the truth. \"What does the data say?\" is a common refrain, as if data on its own can hold the answers. There is a lot more to say about the role and rhetoric of data in our society, but that does not bring us closer to a definition. If we're looking for a simple way to understand data, try this: \"Data is a value assigned to a thing.\" - School of Data . A value assigned to a thing. Value is a word we encounter a lot when working with technology and math. It is often a number, but it does not have to be. If you're filling out a form that asks for your name, email address, and phone number, the values are pieces of information that you contribute. The things are the categories, the labels that remain constant no matter how many people fill out the form. Data can be expressed in many different ways, something we'll talk about in another section. But for now, if you need a visualization, think of a simple table. Thing1 Thing2 Value 1 Value 2 What are the humanities? # The humanities might be a little easier to define than data. Wikipedia says that the humanities \"academic disciplines that study human culture.\" How does your own university define the humanities? At my school, the humanities course designation is defined this way: \"Courses in a variety of disciplines focus on aspects of human experience and on methods of addressing the basic questions of meaning in humanistic study. Courses in history, philosophy, religion, or other departments or interdepartmental programs may fulfill this requirement. W&L Registrar Now might be a good time to do some research about the history of universities and how we ended up with the academic disciplines we have. Take a look at the first few pages of Chapter 1 in Digital_Humanities . Did anything surprise you? What questions might you have about your own discipline, if you have one? But the registrar's definition offers something else: humanities courses focus on \"methods of addressing the basic questions of meaning in humanistic study.\" You might also see this phrased as \"humanistic inquiry.\" Does \"humanistic\" just mean that it is coming from a humanities discipline? Or is there more to it? If we were to borrow a computing term, using the term in its own definition is a bit recursive . Let's turn to some other potential definitions: \"The spectrum of humanistic thought, like that of scientific investigation, encompasses the gamut of beliefs regarding the nature of knowledge, the world, and the human ability to establish understanding with various degrees of certainty. D_H \"Humanistic inquiry acknowledges the situated, partial, and constitutive character of knowledge production, the recognition that knowledge is constructed, taken, not simply given as a natural representation of pre-existing fact.\" Joanna Drucker Both of these definitions give us an idea of what the humanities might be after: knowledge. The nature, meaning, and construction of knowledge. And right away, we should notice that knowledge is not a certain, natural thing. We might say that each of the humanities disciplines takes its own approach to finding meaning and constructing knowledge about the human experience. How does Philosophy do this? English? History? What is humanities data? # Miriam Poser, well-known DH scholar at UCLA and someone you'll see repeatedly referenced in this coursebook, calls humanities data a \"necessary contradiction.\" She describes the humanities scholar's resistance to seeing their sources/material/texts as data. You might have a professor who thinks this way. Many humanities scholars engage with the objects of their study in a way that does require spreadsheets, databases, or powerful computers. They might read their print books, visit archives to read printed documents, or view artwork in person. That being said, many humanities scholars do, and have for a long time, used technology to help them in their work. Scholars were using computers in the 1950s to create concordances and indices . Today, scholars are reading e-books, annotating on their iPads, organizing and tagging digital images , or searching scholarly databases. In doing so, they're relying on tools and processes built by librarians, who have been organizing information for a long time. As technology expands into every corner of our lives, humanities scholars find themselves wanting and needing to address their questions, aka their lines of humanistic inquiry, in new ways. In some cases, humanities scholars have led the way in building new tools and methods for analyzing data. Examples. But what humanities scholars have found is that their \"data\" does not always make good data. We'll learn later on about tidy, well-structured data sets, but humanities sources do not fit the bill. Humanities-based research objects could be: a single book, a set of unique, hand-written manuscripts stored in four different libraries, the art on the walls of a whole city, ancient graffiti etched into crumbling plaster, or audiovisual material so fragile that it degrades with every viewing. Humanities scholars might have questions that start with, \"how many\" or \"what percentage,\" but they see years of work ahead in order to answer those questions. It's true that there are some humanities data projects that could go on for years and years. But before we get going, we have to figure out where we're going and how we're going to get there. This is a crucial piece called data modeling and it actually takes up a large portion of the work of a humanities data project. Before we start typing into our Google doc, we need to create a model of exactly what we're collecting, how it is going to be formatted, and the most difficult: what information we don't care about. For some, this is an excruciating process. Every detail is interesting, worth a whole day of rabbit trails and research. For others, it brings immense satisfaction to organize their material into neat rows and columns. Regardless it's necessary to bring your goals, data, and analysis methods in sync with one another. Fortunately, you don't have to do it alone. Humanities scholars have a reputation for working in isolation. Thinking, reading, and writing are solo activities. But data-driven humanities projects often require a team of people with a range of skillsets. Humanities scholars partner with librarians, technologists, amongst others in order to build giant databases or interactive applications. And importantly, they collaborate to extend and share their data. As just one example, the Pelagios Network lists dozens of partners in their expansive goal to \"link and explore the history of places.\" Why humanities data? # Why is this coursebook about data in the humanities and not just data in general? What about the social sciences? Or journalism? Aren't some of these methods used in the sciences as well? What if I'm not going to be a professor, why should I care? Good questions! First, there are a lot of existing resources for folks in other fields looking to learn about working with data. Examples. There are not as many resources available for folks, especially students, looking to learn about working with humanities data. This coursebook aims to fill this gap, and to do so from an multi-disciplinary perspective. But the better answer is that there are valuable lessons and transferable skills to be learned from working with humanities data. The skills, things like data modeling, cleaning, visualization, and analysis can be used in all sort of other ways. Data and databases are present in virtually every industry. Beyond technical skills, working with humanities data show you how to apply that humanistic inquiry to technology. It helps you see how the complexities of our world may have been sliced or squished to fit into a database. Understanding the principles of design will help recognize a misleading data visualization. What about digital humanities (DH)? # As you work through this coursebook, you will find references to \"Digital Humanities\" or DH. A lot of energy has been put into coming up with definitions for the Digital Humanities, but the short version is: it's the intersection of the humanities and technology. It's an umbrella term created to help humanists understand what their disciplines look like in a technological world. It's a community of practice that values process, openness, and experimentation. Humanities data projects certainly fall within Digital Humanities, but not every DH project is a humanities project. At its worst, it's a gatekeeping term, used to weaken the confidence of those who aren't sure their work is DH enough. Some people believe that in the future, DH will just be the humanities, but until then, we have this label to organize around. Activities # Activity 1.1 # Let's take a look at two humanities data projects to get a sense what we're talking about. Spend a few minutes exploring each project, then come back and see if you can answer the following questions. It's okay if you're not quite sure what each question means, but give it your best shot. Projects: Project 1: Photogrammar Project 2: Robots Reading Vogue Questions: What is the goal of this project? Are there guiding research questions? Who are authors? What are their affiliations and roles? Are students involved? How was this project funded? What is the source of the data? How has the data been processed or modified for this project? What do the visualizations show? Are they interactive? What tools or technologies were used to build this project? What was interesting about this project? What was confusing? Resources # Data Carpentry The Digital Humanities Literacy Guidebook Digital Humanities Now The Historian's Macroscope: Big Digital History The Programming Historian Propublica Data Institute School of Data Online Courses Readings # A Companion to Digital Humanities Defining Data for Humanists: Text, Artifact, Information or Evidence? Digital_Humanities , chapter 1 Humanities Aproaches to Graphical Display Humanities Data: A Necessary Contradiction Technology Is Taking Over English Departments: The false promise of the digital humanities What is data?","title":"What is Humanities Data?"},{"location":"what-is-humanities-data/#what-is-data","text":"What do you see in you mind's eye when someone says \"data?\" Data is one of those ubiquitous words that we use and see everywhere, but when it comes time to define it, we hesitate. It is used by so many people in so many contexts, it can be difficult to narrow down. If asked to share the first thing that comes to mind, you might say \"information,\" \"numbers,\" or \"facts.\" Often, data is defined by the verbs associated with it, such as this definition from Wikpedia: \"Data is measured, collected and reported, and analyzed, whereupon it can be visualized using graphs, images or other analysis tools. Data as a general concept refers to the fact that some existing information or knowledge is represented or coded in some form suitable for better usage or processing.\" - Wikipedia Data is understood as something we do something with. It is an object in motion, under manipulation. It is used to say something, to prove something, or to disprove something else. Data is supposed to hold the truth. \"What does the data say?\" is a common refrain, as if data on its own can hold the answers. There is a lot more to say about the role and rhetoric of data in our society, but that does not bring us closer to a definition. If we're looking for a simple way to understand data, try this: \"Data is a value assigned to a thing.\" - School of Data . A value assigned to a thing. Value is a word we encounter a lot when working with technology and math. It is often a number, but it does not have to be. If you're filling out a form that asks for your name, email address, and phone number, the values are pieces of information that you contribute. The things are the categories, the labels that remain constant no matter how many people fill out the form. Data can be expressed in many different ways, something we'll talk about in another section. But for now, if you need a visualization, think of a simple table. Thing1 Thing2 Value 1 Value 2","title":"What is data?"},{"location":"what-is-humanities-data/#what-are-the-humanities","text":"The humanities might be a little easier to define than data. Wikipedia says that the humanities \"academic disciplines that study human culture.\" How does your own university define the humanities? At my school, the humanities course designation is defined this way: \"Courses in a variety of disciplines focus on aspects of human experience and on methods of addressing the basic questions of meaning in humanistic study. Courses in history, philosophy, religion, or other departments or interdepartmental programs may fulfill this requirement. W&L Registrar Now might be a good time to do some research about the history of universities and how we ended up with the academic disciplines we have. Take a look at the first few pages of Chapter 1 in Digital_Humanities . Did anything surprise you? What questions might you have about your own discipline, if you have one? But the registrar's definition offers something else: humanities courses focus on \"methods of addressing the basic questions of meaning in humanistic study.\" You might also see this phrased as \"humanistic inquiry.\" Does \"humanistic\" just mean that it is coming from a humanities discipline? Or is there more to it? If we were to borrow a computing term, using the term in its own definition is a bit recursive . Let's turn to some other potential definitions: \"The spectrum of humanistic thought, like that of scientific investigation, encompasses the gamut of beliefs regarding the nature of knowledge, the world, and the human ability to establish understanding with various degrees of certainty. D_H \"Humanistic inquiry acknowledges the situated, partial, and constitutive character of knowledge production, the recognition that knowledge is constructed, taken, not simply given as a natural representation of pre-existing fact.\" Joanna Drucker Both of these definitions give us an idea of what the humanities might be after: knowledge. The nature, meaning, and construction of knowledge. And right away, we should notice that knowledge is not a certain, natural thing. We might say that each of the humanities disciplines takes its own approach to finding meaning and constructing knowledge about the human experience. How does Philosophy do this? English? History?","title":"What are the humanities?"},{"location":"what-is-humanities-data/#what-is-humanities-data","text":"Miriam Poser, well-known DH scholar at UCLA and someone you'll see repeatedly referenced in this coursebook, calls humanities data a \"necessary contradiction.\" She describes the humanities scholar's resistance to seeing their sources/material/texts as data. You might have a professor who thinks this way. Many humanities scholars engage with the objects of their study in a way that does require spreadsheets, databases, or powerful computers. They might read their print books, visit archives to read printed documents, or view artwork in person. That being said, many humanities scholars do, and have for a long time, used technology to help them in their work. Scholars were using computers in the 1950s to create concordances and indices . Today, scholars are reading e-books, annotating on their iPads, organizing and tagging digital images , or searching scholarly databases. In doing so, they're relying on tools and processes built by librarians, who have been organizing information for a long time. As technology expands into every corner of our lives, humanities scholars find themselves wanting and needing to address their questions, aka their lines of humanistic inquiry, in new ways. In some cases, humanities scholars have led the way in building new tools and methods for analyzing data. Examples. But what humanities scholars have found is that their \"data\" does not always make good data. We'll learn later on about tidy, well-structured data sets, but humanities sources do not fit the bill. Humanities-based research objects could be: a single book, a set of unique, hand-written manuscripts stored in four different libraries, the art on the walls of a whole city, ancient graffiti etched into crumbling plaster, or audiovisual material so fragile that it degrades with every viewing. Humanities scholars might have questions that start with, \"how many\" or \"what percentage,\" but they see years of work ahead in order to answer those questions. It's true that there are some humanities data projects that could go on for years and years. But before we get going, we have to figure out where we're going and how we're going to get there. This is a crucial piece called data modeling and it actually takes up a large portion of the work of a humanities data project. Before we start typing into our Google doc, we need to create a model of exactly what we're collecting, how it is going to be formatted, and the most difficult: what information we don't care about. For some, this is an excruciating process. Every detail is interesting, worth a whole day of rabbit trails and research. For others, it brings immense satisfaction to organize their material into neat rows and columns. Regardless it's necessary to bring your goals, data, and analysis methods in sync with one another. Fortunately, you don't have to do it alone. Humanities scholars have a reputation for working in isolation. Thinking, reading, and writing are solo activities. But data-driven humanities projects often require a team of people with a range of skillsets. Humanities scholars partner with librarians, technologists, amongst others in order to build giant databases or interactive applications. And importantly, they collaborate to extend and share their data. As just one example, the Pelagios Network lists dozens of partners in their expansive goal to \"link and explore the history of places.\"","title":"What is humanities data?"},{"location":"what-is-humanities-data/#why-humanities-data","text":"Why is this coursebook about data in the humanities and not just data in general? What about the social sciences? Or journalism? Aren't some of these methods used in the sciences as well? What if I'm not going to be a professor, why should I care? Good questions! First, there are a lot of existing resources for folks in other fields looking to learn about working with data. Examples. There are not as many resources available for folks, especially students, looking to learn about working with humanities data. This coursebook aims to fill this gap, and to do so from an multi-disciplinary perspective. But the better answer is that there are valuable lessons and transferable skills to be learned from working with humanities data. The skills, things like data modeling, cleaning, visualization, and analysis can be used in all sort of other ways. Data and databases are present in virtually every industry. Beyond technical skills, working with humanities data show you how to apply that humanistic inquiry to technology. It helps you see how the complexities of our world may have been sliced or squished to fit into a database. Understanding the principles of design will help recognize a misleading data visualization.","title":"Why humanities data?"},{"location":"what-is-humanities-data/#what-about-digital-humanities-dh","text":"As you work through this coursebook, you will find references to \"Digital Humanities\" or DH. A lot of energy has been put into coming up with definitions for the Digital Humanities, but the short version is: it's the intersection of the humanities and technology. It's an umbrella term created to help humanists understand what their disciplines look like in a technological world. It's a community of practice that values process, openness, and experimentation. Humanities data projects certainly fall within Digital Humanities, but not every DH project is a humanities project. At its worst, it's a gatekeeping term, used to weaken the confidence of those who aren't sure their work is DH enough. Some people believe that in the future, DH will just be the humanities, but until then, we have this label to organize around.","title":"What about digital humanities (DH)?"},{"location":"what-is-humanities-data/#activities","text":"","title":"Activities"},{"location":"what-is-humanities-data/#activity-11","text":"Let's take a look at two humanities data projects to get a sense what we're talking about. Spend a few minutes exploring each project, then come back and see if you can answer the following questions. It's okay if you're not quite sure what each question means, but give it your best shot. Projects: Project 1: Photogrammar Project 2: Robots Reading Vogue Questions: What is the goal of this project? Are there guiding research questions? Who are authors? What are their affiliations and roles? Are students involved? How was this project funded? What is the source of the data? How has the data been processed or modified for this project? What do the visualizations show? Are they interactive? What tools or technologies were used to build this project? What was interesting about this project? What was confusing?","title":"Activity 1.1"},{"location":"what-is-humanities-data/#resources","text":"Data Carpentry The Digital Humanities Literacy Guidebook Digital Humanities Now The Historian's Macroscope: Big Digital History The Programming Historian Propublica Data Institute School of Data Online Courses","title":"Resources"},{"location":"what-is-humanities-data/#readings","text":"A Companion to Digital Humanities Defining Data for Humanists: Text, Artifact, Information or Evidence? Digital_Humanities , chapter 1 Humanities Aproaches to Graphical Display Humanities Data: A Necessary Contradiction Technology Is Taking Over English Departments: The false promise of the digital humanities What is data?","title":"Readings"},{"location":"your-computer/","text":"In this section, we'll learn a little bit about how our computer works using a tool called the command line. Do you really know your computer? Command Line How it works Syntax File Structures Activities Activity 2.1 Activity 2.2 Activity 2.3 Resources https://xkcd.com/934/ Do you really know your computer? # For many of us, computers are a means to an end, a tool for a job. We use our computers to write papers, send email, talk to our friends and family, watch Netflix, or maybe play a game. Maybe you use your computer to make art, do science, or analyze statistics. Most likely, you have a preference for a PC or a Mac, even if the only reason you can articulate is the way it looks. But how well do you really know your computer? Do you know what operating system it runs? Which version of that operating system? When was the last time you updated it? Have you ever upgraded your hardware? Do you even know which pieces of your computer are the hardware? As an exercise, see if you can find the name and version of your operating system. What other information can you find out? For a device as ubiquitous as the computer, most of us don't know a lot about how they work. On the one hand, that's great! Why should you have to learn the intricacies of bits, bytes, circuits, and microprocessors to do something basic like write an email? On the other hand, what if you want to use the power of your computer to count words, make a map, or analyze a network? When it comes to working with data, having a deeper understanding of what your computer can do can be to your advantage. Not everything can be done in the browser. After all, computers started their life as counting machines. If you want to work with large sets of data in an efficient manner, ask a computer to do what it is best at: count things, find patterns, or carry out a series of operations. That is not to say computers can do all the work! Humans have their own part to play. Someone has to give the instructions, write the programs, make connections, or see the larger context of the computer's work. In this section, we'll learn about one specific way to give instructions to your computer: the command line. Command Line # Most of us interact with our computers or phones through highly visual interfaces. We know what button to press because of the stylized image representing it. We understand what it means when a website has a blue \"f\" or when a friend sends a thumbs-up emoji. Your computer, regardless of operating system, wants to interact with you via visual cues. We call these Graphical User Interfaces aka GUI (pronounced gooey). But there's another way. You can interact with your computer entirely via text commands through something called a Command Line Interface or CLI. When you see a hacker typing green text into a black box in a movie, they're using the command line. But the command line isn't just for hacking. Not only can you perform a number of different actions on your computer, many pieces of software can only be used through the command line. You can use the command line to create, move, or copy files. You can convert images or find and replace. You can run complex programming languages, or just write a simple script to automate a repetitive task. There are many tools for working with data that require you to use the command line. Just like HTML and CSS, you do not have to memorize every command. There are plenty of sites to remind you of the commands or that provide existing scripts to help with a task. How it works # For Mac users: Search and open ( Command + Space ) for an app called Terminal. A white box should appear. You should see some information about you computer, then your name, then a dollar sign. It could look like this 38371-Lib-Brooks:~ mackenzie$ . Typing in this program is a little different than typing in a document. You cannot use your mouse to click into text and change what is written. Pretend you're working at an old school computer where there is no mouse. If you want to change what you've written, use the arrow keys or backspace to navigate. Type in the letters pwd . No spaces, no capitalization. Hit enter. This command stands for \"print working directory\" and tell us where we are in the file structure. Confused yet? Scroll down to the next section for more context. For Windows users: Search/open a program called cmd.exe or Powershell. If you're using a Chromebook, or another device, you may not be able to participate in the command line activities, since the nature of your device limits access to your computer. Syntax # Just as we learned the unique syntaxes for HTML and CSS, issuing commands in the command line requires a new syntax. Here's the basic format: $ command parameters -flag The $ is not something you type. It appears at the command prompt as a way to indicate that you should begin typing. Most tutorials you see will start their commands with the $ to show that this is the beginning of a command. Think of it like a period at the end of a sentence. It is there to visually prompt you. The next part of the syntax is the command. Usually this is a single word or series of letters, like pwd ls or cd . Go ahead and type one of these commands into the command prompt, then press enter. Next, we have parameters, or some kind of direction for our command. The command cd stands for \"change directory.\" Often, we need to tell the command prompt where we want to change into, so we need to give it a parameter. The command cd Desktop would change our directory (more about directories in a moment) to the Desktop. So in this case, Desktop = parameter. Finally, we have a flag, usually expressed as a dash + letter -f . Flags are options that you might turn on to change the nature of your command. Enter ls -l into the command prompt and press enter. ls stands for list directory contents, and in this case, the -l flag means list the contents in the long version. Now try just ls without the flag. Notice the difference? File Structures # Before we go any further, there is a key concept you must understand about how your computer stores information. All of the stuff on your computer, the documents, music, PDFs, videos, is organized into folders (also known as directories). Modern operating systems, especially on a Mac, as well as cloud-based storage like Google Drive or Box, obscure this fact. But it's essential to understanding the command line, working with data, and publishing content to the web. When you are issuing a command, or referencing a file, in any type of coding context, that action is being done with an awareness of the file structure, even if you personally don't realize it. For example, when you wanted to insert a photo of a cute kitten into your webpage, the only way to make it work was if the image of the kitten was in the same folder as your HTML document. When you wrote <img src=\"kittens.jpg\"> you were giving the browser both the name of that file, as well as the path. If both the image and the HTML document were in the same folder, then the path was unnecessary. But say you created a folder for all your images, called \"images,\" within the same folder as your HTML document. Now, your HTML might need to look like this <img src=\"/images/kittens.jpg\"> . As humans, we rely on search functions frequently to deliver the files we're looking for. But code, as expressed through commands or HTML, doesn't have a built-in search function. And even if it did, it still might not know exactly which file you meant. You have to help the computer find its way to the file you are referring to. If you recall from the previous section, websites are just made up of files and folders too. When you upload something to Web, it is being added to a folder on someone else's server. Have you ever looked at a URL up close? Some browsers, like Safari, will try to hide it from you. But if look closely, you should see the directory structure at work. In this URL, \"faculty-and-staff\" is a subdirectory of \"english-department\" https://my.wlu.edu/english-department/faculty-and-staff . If you're looking real close, you might also notice that there are dashes in the folder/directory names. This is another important lesson! Browsers and computers have a hard time with spaces. It doesn't necessary behave the same way a character like a letter or a number does. Browsers will often replace a space with the characters %20 . It's best practice to avoid using spaces in your file names. I know your computer lets you name things with spaces, but it will save you a lot of headaches if you avoid them. Use dashes, underscores, or something called camelCase where the second word is capitalized to set it apart. Activities # There are many existing tutorials for learning the command line. The activities in this section will rely on those tutorials so you get a sense of the different types of materials out there for learning a new technology skill. Feel free to seek out videos on YouTube, LinkedIn Learning, or Code Academy if that's your style. Activity 2.1 # Work through The Command Line Crash Course . This tutorial accommodates both Windows and Mac people. This tutorial is about brute force repetition, so just lean into that method of learning for right now. Activity 2.2 # The Programming Historian is a wonderful website offering tutorials (in multiple languages!) on digital research methods in the humanities. They have two tutorials on the command line that are slightly more advanced than Activity 2.1. Work through the relevant tutorial for your operating system. Mac users: Intro to the Bash Command Line Windows users: Intro to Powershell (This one gets pretty advanced, don't worry if you can't get all the way through.) Activity 2.3 # In this activity, we'll apply our new command line skills to working with a corpus of textual data. While we're not quite ready to learn about text analysis, when we get there, we will need to have our data ready. You might notice I just threw some new words at you - \"corpus\" and \"textual.\" Don't freak out, these are just specific words to refer a collection (corpus) of textual (not numbers) data. Instead of a spreadsheet with rows and columns, we'll be working with individual documents groups together into folders. I am purposely giving you a kind of messy, but very much real, dataset to explore. It's okay if you get confused, but try to use these new commands to navigate through the dataset, rather than pointing and clicking. Download this zip file and save it to your Desktop. Unzip the file by double clicking or using an Extract All option. A zip file is a compression method for bundling up a lot of folders into one so it's easy to share. While that's unzipping, take some time to explore the Ring-tum Phi in the Digital Archive . Don't just look at the content, but think about the experience of browsing this digitized newspaper. Open the command line shell, and navigate to the folder you just downloaded. Use pwd cd and ls to navigate through the RTP-1980s folder. Can you mirror these action using your mouse and the Finder/Windows Explorer window? What are the differences? In a Word document, answer the following questions: What is the basic file structure of the data set as you have received it? What about the file names? * What are the patterns? Where (or when) do the patterns change? What is the granularity (of the text files? Does each file contain one page? Issue? Reel? Volume? Year? Use cat to read a file. Find the manual for cat. What else can you do with this command? What happens when you type ls *.2.txt ? Can you figure out how to list all the file names in RTP-1980s and send them to a text file? Last step: type history and paste your command history into the document. Resources # ExplainShell . Paste in a command and receive a definition. Sourcecaster by Thomas Padilla and James Baker. This site has example scripts for performing common tasks with files. Learn the Command Line from CodeAcademy.","title":"Your Computer"},{"location":"your-computer/#do-you-really-know-your-computer","text":"For many of us, computers are a means to an end, a tool for a job. We use our computers to write papers, send email, talk to our friends and family, watch Netflix, or maybe play a game. Maybe you use your computer to make art, do science, or analyze statistics. Most likely, you have a preference for a PC or a Mac, even if the only reason you can articulate is the way it looks. But how well do you really know your computer? Do you know what operating system it runs? Which version of that operating system? When was the last time you updated it? Have you ever upgraded your hardware? Do you even know which pieces of your computer are the hardware? As an exercise, see if you can find the name and version of your operating system. What other information can you find out? For a device as ubiquitous as the computer, most of us don't know a lot about how they work. On the one hand, that's great! Why should you have to learn the intricacies of bits, bytes, circuits, and microprocessors to do something basic like write an email? On the other hand, what if you want to use the power of your computer to count words, make a map, or analyze a network? When it comes to working with data, having a deeper understanding of what your computer can do can be to your advantage. Not everything can be done in the browser. After all, computers started their life as counting machines. If you want to work with large sets of data in an efficient manner, ask a computer to do what it is best at: count things, find patterns, or carry out a series of operations. That is not to say computers can do all the work! Humans have their own part to play. Someone has to give the instructions, write the programs, make connections, or see the larger context of the computer's work. In this section, we'll learn about one specific way to give instructions to your computer: the command line.","title":"Do you really know your computer?"},{"location":"your-computer/#command-line","text":"Most of us interact with our computers or phones through highly visual interfaces. We know what button to press because of the stylized image representing it. We understand what it means when a website has a blue \"f\" or when a friend sends a thumbs-up emoji. Your computer, regardless of operating system, wants to interact with you via visual cues. We call these Graphical User Interfaces aka GUI (pronounced gooey). But there's another way. You can interact with your computer entirely via text commands through something called a Command Line Interface or CLI. When you see a hacker typing green text into a black box in a movie, they're using the command line. But the command line isn't just for hacking. Not only can you perform a number of different actions on your computer, many pieces of software can only be used through the command line. You can use the command line to create, move, or copy files. You can convert images or find and replace. You can run complex programming languages, or just write a simple script to automate a repetitive task. There are many tools for working with data that require you to use the command line. Just like HTML and CSS, you do not have to memorize every command. There are plenty of sites to remind you of the commands or that provide existing scripts to help with a task.","title":"Command Line"},{"location":"your-computer/#how-it-works","text":"For Mac users: Search and open ( Command + Space ) for an app called Terminal. A white box should appear. You should see some information about you computer, then your name, then a dollar sign. It could look like this 38371-Lib-Brooks:~ mackenzie$ . Typing in this program is a little different than typing in a document. You cannot use your mouse to click into text and change what is written. Pretend you're working at an old school computer where there is no mouse. If you want to change what you've written, use the arrow keys or backspace to navigate. Type in the letters pwd . No spaces, no capitalization. Hit enter. This command stands for \"print working directory\" and tell us where we are in the file structure. Confused yet? Scroll down to the next section for more context. For Windows users: Search/open a program called cmd.exe or Powershell. If you're using a Chromebook, or another device, you may not be able to participate in the command line activities, since the nature of your device limits access to your computer.","title":"How it works"},{"location":"your-computer/#syntax","text":"Just as we learned the unique syntaxes for HTML and CSS, issuing commands in the command line requires a new syntax. Here's the basic format: $ command parameters -flag The $ is not something you type. It appears at the command prompt as a way to indicate that you should begin typing. Most tutorials you see will start their commands with the $ to show that this is the beginning of a command. Think of it like a period at the end of a sentence. It is there to visually prompt you. The next part of the syntax is the command. Usually this is a single word or series of letters, like pwd ls or cd . Go ahead and type one of these commands into the command prompt, then press enter. Next, we have parameters, or some kind of direction for our command. The command cd stands for \"change directory.\" Often, we need to tell the command prompt where we want to change into, so we need to give it a parameter. The command cd Desktop would change our directory (more about directories in a moment) to the Desktop. So in this case, Desktop = parameter. Finally, we have a flag, usually expressed as a dash + letter -f . Flags are options that you might turn on to change the nature of your command. Enter ls -l into the command prompt and press enter. ls stands for list directory contents, and in this case, the -l flag means list the contents in the long version. Now try just ls without the flag. Notice the difference?","title":"Syntax"},{"location":"your-computer/#file-structures","text":"Before we go any further, there is a key concept you must understand about how your computer stores information. All of the stuff on your computer, the documents, music, PDFs, videos, is organized into folders (also known as directories). Modern operating systems, especially on a Mac, as well as cloud-based storage like Google Drive or Box, obscure this fact. But it's essential to understanding the command line, working with data, and publishing content to the web. When you are issuing a command, or referencing a file, in any type of coding context, that action is being done with an awareness of the file structure, even if you personally don't realize it. For example, when you wanted to insert a photo of a cute kitten into your webpage, the only way to make it work was if the image of the kitten was in the same folder as your HTML document. When you wrote <img src=\"kittens.jpg\"> you were giving the browser both the name of that file, as well as the path. If both the image and the HTML document were in the same folder, then the path was unnecessary. But say you created a folder for all your images, called \"images,\" within the same folder as your HTML document. Now, your HTML might need to look like this <img src=\"/images/kittens.jpg\"> . As humans, we rely on search functions frequently to deliver the files we're looking for. But code, as expressed through commands or HTML, doesn't have a built-in search function. And even if it did, it still might not know exactly which file you meant. You have to help the computer find its way to the file you are referring to. If you recall from the previous section, websites are just made up of files and folders too. When you upload something to Web, it is being added to a folder on someone else's server. Have you ever looked at a URL up close? Some browsers, like Safari, will try to hide it from you. But if look closely, you should see the directory structure at work. In this URL, \"faculty-and-staff\" is a subdirectory of \"english-department\" https://my.wlu.edu/english-department/faculty-and-staff . If you're looking real close, you might also notice that there are dashes in the folder/directory names. This is another important lesson! Browsers and computers have a hard time with spaces. It doesn't necessary behave the same way a character like a letter or a number does. Browsers will often replace a space with the characters %20 . It's best practice to avoid using spaces in your file names. I know your computer lets you name things with spaces, but it will save you a lot of headaches if you avoid them. Use dashes, underscores, or something called camelCase where the second word is capitalized to set it apart.","title":"File Structures"},{"location":"your-computer/#activities","text":"There are many existing tutorials for learning the command line. The activities in this section will rely on those tutorials so you get a sense of the different types of materials out there for learning a new technology skill. Feel free to seek out videos on YouTube, LinkedIn Learning, or Code Academy if that's your style.","title":"Activities"},{"location":"your-computer/#activity-21","text":"Work through The Command Line Crash Course . This tutorial accommodates both Windows and Mac people. This tutorial is about brute force repetition, so just lean into that method of learning for right now.","title":"Activity 2.1"},{"location":"your-computer/#activity-22","text":"The Programming Historian is a wonderful website offering tutorials (in multiple languages!) on digital research methods in the humanities. They have two tutorials on the command line that are slightly more advanced than Activity 2.1. Work through the relevant tutorial for your operating system. Mac users: Intro to the Bash Command Line Windows users: Intro to Powershell (This one gets pretty advanced, don't worry if you can't get all the way through.)","title":"Activity 2.2"},{"location":"your-computer/#activity-23","text":"In this activity, we'll apply our new command line skills to working with a corpus of textual data. While we're not quite ready to learn about text analysis, when we get there, we will need to have our data ready. You might notice I just threw some new words at you - \"corpus\" and \"textual.\" Don't freak out, these are just specific words to refer a collection (corpus) of textual (not numbers) data. Instead of a spreadsheet with rows and columns, we'll be working with individual documents groups together into folders. I am purposely giving you a kind of messy, but very much real, dataset to explore. It's okay if you get confused, but try to use these new commands to navigate through the dataset, rather than pointing and clicking. Download this zip file and save it to your Desktop. Unzip the file by double clicking or using an Extract All option. A zip file is a compression method for bundling up a lot of folders into one so it's easy to share. While that's unzipping, take some time to explore the Ring-tum Phi in the Digital Archive . Don't just look at the content, but think about the experience of browsing this digitized newspaper. Open the command line shell, and navigate to the folder you just downloaded. Use pwd cd and ls to navigate through the RTP-1980s folder. Can you mirror these action using your mouse and the Finder/Windows Explorer window? What are the differences? In a Word document, answer the following questions: What is the basic file structure of the data set as you have received it? What about the file names? * What are the patterns? Where (or when) do the patterns change? What is the granularity (of the text files? Does each file contain one page? Issue? Reel? Volume? Year? Use cat to read a file. Find the manual for cat. What else can you do with this command? What happens when you type ls *.2.txt ? Can you figure out how to list all the file names in RTP-1980s and send them to a text file? Last step: type history and paste your command history into the document.","title":"Activity 2.3"},{"location":"your-computer/#resources","text":"ExplainShell . Paste in a command and receive a definition. Sourcecaster by Thomas Padilla and James Baker. This site has example scripts for performing common tasks with files. Learn the Command Line from CodeAcademy.","title":"Resources"}]}